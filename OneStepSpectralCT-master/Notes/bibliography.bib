
@misc{center_for_history_and_new_media_zotero_,
  title = {Zotero {{Quick Start Guide}}},
  howpublished = {http://zotero.org/support/quick\_start\_guide},
  author = {{Center for History and New Media}}
}

@article{candes_robust_2006,
  title = {Robust Uncertainty Principles: Exact Signal Reconstruction from Highly Incomplete Frequency Information},
  volume = {52},
  issn = {0018-9448},
  shorttitle = {Robust Uncertainty Principles},
  doi = {10.1109/TIT.2005.862083},
  abstract = {This paper considers the model problem of reconstructing an object from incomplete frequency samples. Consider a discrete-time signal f isin;C\textsuperscript{N} and a randomly chosen set of frequencies Omega;. Is it possible to reconstruct f from the partial knowledge of its Fourier coefficients on the set Omega;? A typical result of this paper is as follows. Suppose that f is a superposition of |T| spikes f(t)= sigma;\textsubscript{ tau; isin;T}f( tau;) delta;(t- tau;) obeying |T| le;C\textsubscript{M} middot;(log N)\textsuperscript{-1} middot; | Omega;| for some constant C\textsubscript{M}$>$0. We do not know the locations of the spikes nor their amplitudes. Then with probability at least 1-O(N\textsuperscript{-M}), f can be reconstructed exactly as the solution to the \#8467;\textsubscript{1} minimization problem. In short, exact recovery may be obtained by solving a convex optimization problem. We give numerical values for C\textsubscript{M} which depend on the desired probability of success. Our result may be interpreted as a novel kind of nonlinear sampling theorem. In effect, it says that any signal made out of |T| spikes may be recovered by convex programming from almost every set of frequencies of size O(|T| middot;logN). Moreover, this is nearly optimal in the sense that any method succeeding with probability 1-O(N\textsuperscript{-M}) would in general require a number of frequency samples at least proportional to |T| middot;logN. The methodology extends to a variety of other situations and higher dimensions. For example, we show how one can reconstruct a piecewise constant (one- or two-dimensional) object from incomplete frequency samples - provided that the number of jumps (discontinuities) obeys the condition above - by minimizing other convex functionals such as the total variation of f.},
  number = {2},
  journal = {Information Theory, IEEE Transactions on},
  author = {Candes, E.J. and Romberg, J. and Tao, T.},
  month = feb,
  year = {2006},
  keywords = {Fourier analysis,Fourier coefficient,Image reconstruction,convex optimization,convex programming,discrete-time signal,image sampling,incomplete frequency information,indeterminancy,linear programming,minimisation,minimization problem,nonlinear sampling theorem,piecewise constant object,piecewise constant techniques,probability,probability value,robust uncertainty principle,signal reconstruction,signal sampling,sparse matrices,sparse random matrix,trigonometric expansion},
  pages = {489 -- 509},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/VRGMBSF4/Candes et al. - 2006 - Robust uncertainty principles exact signal recons.pdf}
}

@article{han_algorithm-enabled_2015,
  title = {Algorithm-Enabled Exploration of Image-Quality Potential of Cone-Beam {{CT}} in Image-Guided Radiation Therapy},
  volume = {60},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/60/12/4601},
  abstract = {Kilo-voltage (KV) cone-beam computed tomography (CBCT) unit mounted onto a linear accelerator treatment system, often referred to as on-board imager (OBI), plays an increasingly important role in image-guided radiation therapy. While the FDK algorithm is currently used for reconstructing images from clinical OBI data, optimization-based reconstruction has also been investigated for OBI CBCT. An optimization-based reconstruction involves numerous parameters, which can significantly impact reconstruction properties (or utility). The success of an optimization-based reconstruction for a particular class of practical applications thus relies strongly on appropriate selection of parameter values. In the work, we focus on tailoring the constrained-TV-minimization-based reconstruction, an optimization-based reconstruction previously shown of some potential for CBCT imaging conditions of practical interest, to OBI imaging through appropriate selection of parameter values. In particular, for given real data of phantoms and patient collected with OBI CBCT, we first devise utility metrics specific to OBI-quality-assurance tasks and then apply them to guiding the selection of parameter values in constrained-TV-minimization-based reconstruction. The study results show that the reconstructions are with improvement, relative to clinical FDK reconstruction, in both visualization and quantitative assessments in terms of the devised utility metrics.},
  language = {en},
  number = {12},
  journal = {Physics in Medicine and Biology},
  author = {Han, Xiao and Pearson, Erik and Pelizzari, Charles and Al-Hallaq, Hania and Sidky, Emil Y. and Bian, Junguo and Pan, Xiaochuan},
  month = jun,
  year = {2015},
  pages = {4601},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/BPQGURWJ/Han et al. - 2015 - Algorithm-enabled exploration of image-quality pot.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/F3I9FV3W/4601.html}
}

@article{wang_ordered-subset_2004,
  title = {Ordered-Subset Simultaneous Algebraic Reconstruction Techniques ({{OS}}-{{SART}})},
  volume = {12},
  abstract = {In this paper, we first demonstrate that two ordered-subset simultaneous algebraic reconstruction techniques (OS-SART) can be heuristically derived from the perspective of data rectification. Then, we study the convergence in the framework of our recent work on the OS version of the Landweber scheme. The first OS-SART is the same as the BSSART formula, which is a special case of the OS version of the Landweber scheme. Hence, it converges in the weighted least square sense even in the case of inconsistent data. Both the OS-SART formulas are tested for reconstruction of CT images from practical data.},
  number = {3},
  journal = {Journal of X-Ray Science and Technology},
  author = {Wang, Ge and Jiang, Ming},
  month = jan,
  year = {2004},
  pages = {169--177},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/WNCI8XA8/Wang and Jiang - 2004 - Ordered-subset simultaneous algebraic reconstructi.pdf}
}

@article{qi_extraction_2011,
  title = {Extraction of Tumor Motion Trajectories Using {{PICCS}}-{{4DCBCT}}: A Validation Study},
  volume = {38},
  issn = {0094-2405},
  shorttitle = {Extraction of Tumor Motion Trajectories Using {{PICCS}}-{{4DCBCT}}},
  doi = {10.1118/1.3637501},
  abstract = {PURPOSE: As a counterpart of 4DCT in the treatment planning stage of radiotherapy treatment, 4D cone beam computed tomography (4DCBCT) method has been proposed to verify tumor motion trajectories before radiation therapy treatment delivery. Besides 4DCBCT acquisition using slower gantry rotation speed or multiple rotations, a new method using the prior image constrained compressed sensing (PICCS) image reconstruction method and the standard 1-min data acquisition were proposed. In this paper, the PICCS-4DCBCT method was combined with deformable registration to validate its capability in motion trajectory extraction using physical phantom data, simulated human subject data from 4DCT and in vivo human subject data.
METHODS: Two methods were used to validate PICCS-4DCBCT for the purpose of respiratory motion delineation. The standard 1-min gantry rotation Cone Beam CT acquisition was used for both methods. In the first method, 4DCBCT projection data of a physical motion phantom were acquired using an on-board CBCT acquisition system (Varian Medical Systems, Palo Alto, CA). Using a deformable registration method, the object motion trajectories were extracted from both FBP and PICCS reconstructed 4DCBCT images, and compared against the programmed motion trajectories. In the second method, using a clinical 4DCT dataset, Cone Beam CT projections were simulated by forward projection. Using a deformable registration method, the tumor motion trajectories were extracted from the reconstructed 4DCT and PICCS-4DCBCT images. The performance of PICCS-4DCBCT is assessed against the 4DCT ground truth. The breathing period was varied in the simulation to study its effect on motion extraction. For both validation methods, the root mean square error (RMSE) and the maximum of the errors (MaxE) were used to quantify the accuracy of the extracted motion trajectories. After the validation, a clinical dataset was used to demonstrate the motion delineation capability of PICCS-4DCBCT for human subjects.
RESULTS: In both validation studies, the RMSEs of the extracted motion trajectories from PICCS-4DCBCT images are less than 0.7 mm, and their MaxEs are less than 1 mm, for all three directions. In comparison, FBP-4DCBCT shows considerably larger RMSEs in the physical phantom based validation. PICCS-4DCBCT also shows insensitivity to the breathing period in the 4DCT based validation. For the in vivo human subject study, high quality 3D motion trajectory of the tumor was obtained from PICCS-4DCBCT images and showed consistency with visual observation.
CONCLUSIONS: These results demonstrate accurate delineation of tumor motion trajectory can be achieved using PICCS-4DCBCT and the standard 1-min data acquisition.},
  language = {eng},
  number = {10},
  journal = {Medical physics},
  author = {Qi, Zhihua and Chen, Guang-Hong},
  month = oct,
  year = {2011},
  keywords = {Algorithms,Computer Simulation,Cone-Beam Computed Tomography,Four-Dimensional Computed Tomography,Humans,Imaging; Three-Dimensional,Models; Statistical,Motion,Neoplasms,Phantoms; Imaging,Radiographic Image Interpretation; Computer-Assisted,Radiotherapy Planning; Computer-Assisted,Reproducibility of Results,Respiration,Time Factors},
  pages = {5530--5538},
  pmid = {21992371},
  pmcid = {PMC3195374}
}

@article{chen_time-resolved_2012,
  title = {Time-{{Resolved Interventional Cardiac C}}-Arm {{Cone}}-{{Beam CT}}: {{An Application}} of the {{PICCS Algorithm}}},
  volume = {31},
  issn = {0278-0062},
  shorttitle = {Time-{{Resolved Interventional Cardiac C}}-Arm {{Cone}}-{{Beam CT}}},
  doi = {10.1109/TMI.2011.2172951},
  abstract = {Time-resolved cardiac imaging is particularly interesting in the interventional setting since it would provide both image guidance for accurate procedural planning and cardiac functional evaluations directly in the operating room. Imaging the heart in vivo using a slowly rotating C-arm system is extremely challenging due to the limitations of the data acquisition system and the high temporal resolution required to avoid motion artifacts. In this paper, a data acquisition scheme and an image reconstruction method are proposed to achieve time-resolved cardiac cone-beam computed tomography imaging with isotropic spatial resolution and high temporal resolution using a slowly rotating C-arm system. The data are acquired within 14 s using a single gantry rotation with a short scan angular range. The enabling image reconstruction method is the prior image constrained compressed sensing (PICCS) algorithm. The prior image is reconstructed from data acquired over all cardiac phases. Each cardiac phase is then reconstructed from the retrospectively gated cardiac data using the PICCS algorithm. To validate the method, several studies were performed. Both numerical simulations using a hybrid motion phantom with static background anatomy as well as physical phantom studies have been used to demonstrate that the proposed method enables accurate reconstruction of image objects with a high isotropic spatial resolution. A canine animal model scanned in vivo was used to further validate the method.},
  number = {4},
  journal = {Medical Imaging, IEEE Transactions on},
  author = {Chen, G.-H. and Theriault-Lauzier, P. and Tang, J. and Nett, B. and Leng, S. and Zambelli, J. and Qi, Z. and Bevins, N. and Raval, A. and Reeder, S. and Rowley, H.},
  month = apr,
  year = {2012},
  pages = {907 --923},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/KPE5Z665/Chen et al. - 2012 - Time-Resolved Interventional Cardiac C-arm Cone-Be.pdf}
}

@article{hajlaoui_satellite_2010,
  title = {Satellite Image Restoration in the Context of a Spatially Varying Point Spread Function},
  volume = {27},
  doi = {10.1364/JOSAA.27.001473},
  abstract = {In this paper, we consider a deconvolution problem where the point spread function (PSF) of the optical imaging system varies between different spatial locations, thus leading to a spatially varying blur. This problem arises, for example, in synthetic aperture instruments and in wide-field optical systems. Unlike the classical deconvolution context where the PSF is assumed to be spatially invariant, the problem cannot be easily solved in the Fourier domain. We propose here an iterative algorithm based on convex optimization techniques and a wavelet frame regularization. This approach allows restoration of the image, taking into account the properties of the blur operator, the latter being known.},
  number = {6},
  journal = {Journal of the Optical Society of America A},
  author = {Hajlaoui, Nasreddine and Chaux, Caroline and Perrin, Guillaume and Falzon, Fr\dbend{}d\dbend{}ric and Benazza-Benyahia, Amel},
  month = jun,
  year = {2010},
  keywords = {Image reconstruction-restoration,Inverse problems},
  pages = {1473--1481},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/P6G8XQ47/abstract.html}
}

@misc{_algorithm_,
  title = {An Algorithm for Constrained One-Step Inversion of Spectral {{CT}} Data - {{IOPscience}}},
  howpublished = {http://iopscience.iop.org/article/10.1088/0031-9155/61/10/3784},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/JJQTA5W6/3784.html}
}

@article{nam_cosparse_2013,
  title = {The Cosparse Analysis Model and Algorithms},
  volume = {34},
  issn = {1063-5203},
  doi = {10.1016/j.acha.2012.03.006},
  abstract = {After a decade of extensive study of the sparse representation synthesis model, we can safely say that this is a mature and stable field, with clear theoretical foundations, and appealing applications. Alongside this approach, there is an analysis counterpart model, which, despite its similarity to the synthesis alternative, is markedly different. Surprisingly, the analysis model did not get a similar attention, and its understanding today is shallow and partial.

In this paper we take a closer look at the analysis approach, better define it as a generative model for signals, and contrast it with the synthesis one. This work proposes effective pursuit methods that aim to solve inverse problems regularized with the analysis-model prior, accompanied by a preliminary theoretical study of their performance. We demonstrate the effectiveness of the analysis model in several experiments, and provide a detailed study of the model associated with the 2D finite difference analysis operator, a close cousin of the TV norm.},
  number = {1},
  journal = {Applied and Computational Harmonic Analysis},
  author = {Nam, S. and Davies, M. E. and Elad, M. and Gribonval, R.},
  month = jan,
  year = {2013},
  keywords = {Analysis,Compressed-sensing,Greedy algorithms,Pursuit algorithms,Sparse representations,Synthesis,Union of subspaces},
  pages = {30--56},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/56PNTC3F/Nam et al. - 2013 - The cosparse analysis model and algorithms.pdf}
}

@article{babacan_reference-guided_2011,
  title = {Reference-Guided Sparsifying Transform Design for Compressive Sensing {{MRI}}},
  volume = {2011},
  issn = {1557-170X},
  doi = {10.1109/IEMBS.2011.6091384},
  abstract = {Compressive sensing (CS) MRI aims to accurately reconstruct images from undersampled k-space data. Most CS methods employ analytical sparsifying transforms such as total-variation and wavelets to model the unknown image and constrain the solution space during reconstruction. Recently, nonparametric dictionary-based methods for CS-MRI reconstruction have shown significant improvements over the classical methods. These existing techniques focus on learning the representation basis for the unknown image for a synthesis-based reconstruction. In this paper, we present a new framework for analysis-based reconstruction, where the sparsifying transform is learnt from a reference image to capture the anatomical structure of unknown image, and is used to guide the reconstruction process. We demonstrate with experimental data the high performance of the proposed approach over traditional methods.},
  journal = {Conference Proceedings: ... Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Conference},
  author = {Babacan, S Derin and Peng, Xi and Wang, Xian-Pei and Do, Minh N and Liang, Zhi-Pei},
  year = {2011},
  pages = {5718--5721},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/IPEW4AFE/Babacan et al. - 2011 - Reference-guided sparsifying transform design for .pdf},
  pmid = {22255638}
}

@inproceedings{hansis_four-dimensional_2009,
  title = {Four-Dimensional Cardiac Reconstruction from Rotational x-Ray Sequences: First Results for {{4D}} Coronary Angiography\&lt;/Title\&gt;},
  shorttitle = {\&lt;Title\&gt;{{Four}}-Dimensional Cardiac Reconstruction from Rotational x-Ray Sequences},
  doi = {10.1117/12.811104},
  author = {Hansis, Eberhard and Schomberg, Hermann and Erhard, Klaus and D{\"o}ssel, Olaf and Grass, Michael},
  editor = {Samei, Ehsan and Hsieh, Jiang},
  month = feb,
  year = {2009},
  pages = {72580B--72580B--11},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/AQ2KVGGC/Hansis et al. - 2009 - Four-dimensional cardiac reconstruction from rotat.pdf}
}

@article{vandemeulebroucke_spatiotemporal_2011,
  title = {Spatiotemporal Motion Estimation for Respiratory-Correlated Imaging of the Lungs},
  volume = {38},
  issn = {0094-2405},
  abstract = {PURPOSE

Four-dimensional computed tomography (4D CT) can provide patient-specific motion information for radiotherapy planning and delivery. Motion estimation in 4D CT is challenging due to the reduced image quality and the presence of artifacts. We aim to improve the robustness of deformable registration applied to respiratory-correlated imaging of the lungs, by using a global problem formulation and pursuing a restrictive parametrization for the spatiotemporal deformation model.

METHODS

A spatial transformation based on free-form deformations was extended to the temporal domain, by explicitly modeling the trajectory using a cyclic temporal model based on B-splines. A global registration criterion allowed to consider the entire image sequence simultaneously and enforce the temporal coherence of the deformation throughout the respiratory cycle. To ensure a parametrization capable of capturing the dynamics of respiratory motion, a prestudy was performed on the temporal dimension separately. The temporal parameters were tuned by fitting them to diaphragm motion data acquired for a large patient group. Suitable properties were retained and applied to spatiotemporal registration of 4D CT data. Registration results were validated using large sets of landmarks and compared to consecutive spatial registrations. To illustrate the benefit of the spatiotemporal approach, we also assessed the performance in the presence of motion-induced artifacts.

RESULTS

Cubic B-splines gave better or similar fitting results as lower orders and were selected because of their inherently stronger regularization. The fitting and registration errors increased gradually with the temporal control point spacing, representing a trade-off between achievable accuracy and sensitivity to noise and artifacts. A piecewise smooth trajectory model, allowing for a discontinuous change of speed at end-inhale, was found most suitable to account for the sudden changes of motion at this breathing phase. The spatiotemporal modeling allowed a reduction of the number of parameters of 45\%, while maintaining registration accuracy within 0.1 mm. The approach reduced the sensitivity to artifacts.

CONCLUSIONS

Spatiotemporal registration can provide accurate motion estimation for 4D CT and improves the robustness to artifacts.},
  number = {1},
  journal = {Medical physics},
  author = {Vandemeulebroucke, Jef and Rit, Simon and Kybic, Jan and Clarysse, Patrick and Sarrut, David},
  month = jan,
  year = {2011},
  keywords = {Artifacts,Diaphragm,Four-Dimensional Computed Tomography,Image Processing; Computer-Assisted,Lung,Models; Biological,Movement,Respiration,Time Factors},
  pages = {166--178},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/8Q4K8IPN/Vandemeulebroucke et al. - 2011 - Spatiotemporal motion estimation for respiratory-c.pdf},
  pmid = {21361185}
}

@article{bandi_automated_2011,
  title = {Automated Patient Couch Removal Algorithm on {{CT}} Images},
  volume = {2011},
  issn = {1557-170X},
  doi = {10.1109/IEMBS.2011.6091918},
  abstract = {The current paper proposes a novel automated patient couch removal method on Computed Tomography (CT) images. Patient couch is often considered to be an unnecessary artifact especially when 3D rendered techniques are used. The method is based on measuring similarity between selected axial slices and the assumption that the bed object is constant on different slices. Due to the weight of the patient the couch could bend which is identifiable as sagittal movement on consecutive axial slices. Therefore the method focuses on finding this movement after an initial segmentation. According to initial validation performed on real medical data, our method is an effective tool to remove patient couch without user interaction.},
  language = {eng},
  journal = {Conference proceedings: ... Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual Conference},
  author = {Bandi, Peter and Zsoter, Norbert and Seres, Laszlo and Toth, Zoltan and Papp, Laszlo},
  year = {2011},
  keywords = {Algorithms,Artifacts,Automation,Female,Humans,Radiographic Image Enhancement,Reproducibility of Results,Tomography; X-Ray Computed},
  pages = {7783--7786},
  pmid = {22256143}
}

@article{ritschl_iterative_2012,
  title = {Iterative {{4D}} Cardiac Micro-{{CT}} Image Reconstruction Using an Adaptive Spatio-Temporal Sparsity Prior},
  volume = {57},
  issn = {1361-6560},
  doi = {10.1088/0031-9155/57/6/1517},
  abstract = {Temporal-correlated image reconstruction, also known as 4D CT image reconstruction, is a big challenge in computed tomography. The reasons for incorporating the temporal domain into the reconstruction are motions of the scanned object, which would otherwise lead to motion artifacts. The standard method for 4D CT image reconstruction is extracting single motion phases and reconstructing them separately. These reconstructions can suffer from undersampling artifacts due to the low number of used projections in each phase. There are different iterative methods which try to incorporate some a priori knowledge to compensate for these artifacts. In this paper we want to follow this strategy. The cost function we use is a higher dimensional cost function which accounts for the sparseness of the measured signal in the spatial and temporal directions. This leads to the definition of a higher dimensional total variation. The method is validated using in vivo cardiac micro-CT mouse data. Additionally, we compare the results to phase-correlated reconstructions using the FDK algorithm and a total variation constrained reconstruction, where the total variation term is only defined in the spatial domain. The reconstructed datasets show strong improvements in terms of artifact reduction and low-contrast resolution compared to other methods. Thereby the temporal resolution of the reconstructed signal is not affected.},
  language = {eng},
  number = {6},
  journal = {Physics in medicine and biology},
  author = {Ritschl, Ludwig and Sawall, Stefan and Knaup, Michael and Hess, Andreas and Kachelriess, Marc},
  month = mar,
  year = {2012},
  keywords = {Algorithms,Animals,Four-Dimensional Computed Tomography,Image Processing; Computer-Assisted,Mice,Models; Statistical,X-Ray Microtomography},
  pages = {1517--1525},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/9JMKM6T2/Ritschl et al. - 2012 - Iterative 4D cardiac micro-CT image reconstruction.pdf},
  pmid = {22391045}
}

@article{gordon_algebraic_1970,
  title = {Algebraic {{Reconstruction Techniques}} ({{ART}}) for Three-Dimensional Electron Microscopy and {{X}}-Ray Photography},
  volume = {29},
  issn = {0022-5193},
  doi = {10.1016/0022-5193(70)90109-8},
  abstract = {We give a new method for direct reconstruction of three-dimensional objects from a few electron micrographs taken at angles which need not exceed a range of 60 degrees. The method works for totally asymmetric objects, and requires little computer time or storage. It is also applicable to X-ray photography, and may greatly reduce the exposure compared to current methods of body-section radiography.},
  number = {3},
  journal = {Journal of Theoretical Biology},
  author = {Gordon, Richard and Bender, Robert and Herman, Gabor T.},
  month = dec,
  year = {1970},
  pages = {471--481},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/25B6HNW2/0022519370901098.html}
}

@article{knecht_computed_2008,
  title = {Computed Tomography\textendash{}fluoroscopy Overlay Evaluation during Catheter Ablation of Left Atrial Arrhythmia},
  volume = {10},
  copyright = {Published on behalf of the European Society of Cardiology. All rights reserved. \textcopyright{} The Author 2008. For permissions please email: journals.permissions@oxfordjournals.org},
  issn = {1099-5129, 1532-2092},
  doi = {10.1093/europace/eun145},
  abstract = {Aims Proper visualization of left atrial (LA) and pulmonary vein (PV) anatomy is of crucial importance during atrial fibrillation (AF) ablation. This two-centre study evaluated a new automatic computed tomography (CT)\textendash{}fluoroscopy overlay system (EP navigator\textregistered, Philips Medical Systems, Best, The Netherlands) and the accuracy of different registration methods.Methods and results Fifty-six consecutive patients (age: 56 $\pm$ 14) with symptomatic AF underwent contrast CT of the LA/PV prior to ablation. Three registration methods were evaluated and validated by comparison with LA angiography: (i) catheter registration: the placement of catheters in identifiable anatomical structures; (ii) heart contour: based on aligning the fluoroscopy heart contours and the 3D-rendered CT volume; and (iii) spine registration: based on automatically aligning the segmented CT spine on fluoroscopy. Computed tomography segmentation was achieved in all but one patient due to motion artefacts. The mean duration of segmentation was 10 min and average registration lasted 7 min. Catheter and heart contour registration were highly accurate (discrepancy of 1.3 $\pm$ 0.6 and 0.3 $\pm$ 0.5 mm, respectively) when compared with spine registration (17 $\pm$ 9 mm, P $<$ 0.05). The EP navigator was helpful during trans-septal puncture, gave an internal view of the atria and allowed tracking of ablation lesions.Conclusion The EP navigator enabled accurate live integration of CT images and real-time fluoroscopy. Registration utilizing catheter placement or heart contours was stable and reliable.},
  language = {en},
  number = {8},
  journal = {Europace},
  author = {Knecht, S{\'e}bastien and Skali, Hicham and O'Neill, Mark D. and Wright, Matthew and Matsuo, Seiichiro and Chaudhry, Ghulam Muqtada and Haffajee, Charles I. and Nault, Isabelle and Gijsbers, Geert H. M. and Sacher, Frederic and Laurent, Francois and Montaudon, Michel and Corneloup, Olivier and Hocini, M{\'e}l{\`e}ze and Ha{\"\i}ssaguerre, Michel and Orlov, Michael V. and Ja{\"\i}s, Pierre},
  month = aug,
  year = {2008},
  pages = {931--938},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/AXJBU37B/Knecht et al. - 2008 - Computed tomographyâ€“fluoroscopy overlay evaluation.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/8FZ46RKZ/931.html},
  pmid = {18511437}
}

@article{nelder_simplex_1965,
  title = {A {{Simplex Method}} for {{Function Minimization}}},
  volume = {7},
  issn = {0010-4620, 1460-2067},
  doi = {10.1093/comjnl/7.4.308},
  abstract = {A method is described for the minimization of a function of n variables, which depends on the comparison of function values at the (n + 1) vertices of a general simplex, followed by the replacement of the vertex with the highest value by another point. The simplex adapts itself to the local landscape, and contracts on to the final minimum. The method is shown to be effective and computationally compact. A procedure is given for the estimation of the Hessian matrix in the neighbourhood of the minimum, needed in statistical estimation problems.},
  language = {en},
  number = {4},
  journal = {The Computer Journal},
  author = {Nelder, J. A. and Mead, R.},
  month = jan,
  year = {1965},
  pages = {308--313},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/SBUPG3NW/Nelder and Mead - 1965 - A Simplex Method for Function Minimization.pdf}
}

@article{tang_temporal_2010,
  title = {Temporal Resolution Improvement in Cardiac {{CT}} Using {{PICCS}} ({{TRI}}-{{PICCS}}): Performance Studies},
  volume = {37},
  issn = {0094-2405},
  shorttitle = {Temporal Resolution Improvement in Cardiac {{CT}} Using {{PICCS}} ({{TRI}}-{{PICCS}})},
  abstract = {PURPOSE

The recently proposed prior image constrained compressed sensing (PICCS) method has been applied in cardiac MDCT to improve the temporal resolution by approximately a factor of 2, by using projection data acquired from half of the standard short-scan angular range to reconstruct images with improved temporal resolution. The method was referred to as temporal resolution improvement using PICCS (TRI-PICCS). The primary purpose of this article is to study (1) the relationship between the performance of the TRI-PICCS algorithm and the angular range of projection data used in image reconstruction; (2) the relationship between the performance of the TRI-PICCS algorithm and the motion orientations and motion patterns of moving objects; and (3) the relationship between the performance of the TRI-PICCS algorithm and various heart rates.

METHODS

A hybrid phantom consisting of realistic cardiac anatomy and eight moving objects with known motion profiles to simulate coronary arteries was constructed by superimposing the analytical projection data of eight simulated moving vessels to the in vivo projection data from a cardiac MDCT scan. The motion profiles of the moving objects may independently change orientations, period, and amplitude. A prior image was reconstructed using a short-scan filtered backprojection method from a gated short-scan data set for each given motion profile. The TRI-PICCS method was applied to improve temporal resolution for each configuration of given motion profiles of moving objects and given active angular range specified by the target temporal resolution. To quantitatively study the performance, figures of merit were introduced to quantify signal intensity deficit, image distortion, and residual motion artifacts, respectively.

RESULTS

The performance of the TRI-PICCS method is the same when the projection data are taken from 100 degrees to 120 degrees. The performance of the TRI-PICCS method is independent of location and motion orientations. The performance of the TRI-PICCS method does not significantly degrade for heart rates up to 100 bpm with a gantry rotation speed of 350 ms per rotation.

CONCLUSIONS

The TRI-PICCS method can be used to systematically improve temporal resolution for MDCT cardiac imaging by a factor of 2-2.3 and the performance of the TRI-PICCS method is insensitive to motion locations and motion orientations. The TRI-PICCS method enables a single-source MDCT scanner with 350 ms or faster gantry speed to scan patients with heart rates as high as 100 bpm.},
  number = {8},
  journal = {Medical physics},
  author = {Tang, Jie and Hsieh, Jiang and Chen, Guang-Hong},
  month = aug,
  year = {2010},
  keywords = {Algorithms,Coronary Angiography,Radiographic Image Enhancement,Radiographic Image Interpretation; Computer-Assisted,Reproducibility of Results,Sensitivity and Specificity,Tomography; X-Ray Computed},
  pages = {4377--4388},
  pmid = {20879597}
}

@article{guerrero_dynamic_2006,
  title = {Dynamic Ventilation Imaging from Four-Dimensional Computed Tomography},
  volume = {51},
  issn = {0031-9155, 1361-6560},
  doi = {10.1088/0031-9155/51/4/002},
  number = {4},
  journal = {Physics in Medicine and Biology},
  author = {Guerrero, Thomas and Sanders, Kevin and Castillo, Edward and Zhang, Yin and Bidaut, Luc and Pan, Tinsu and Komaki, Ritsuko},
  month = feb,
  year = {2006},
  pages = {777--791},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/366H5RRW/guerrero2006.pdf}
}

@article{roessl_sensitivity_2011,
  title = {Sensitivity of {{Photon}}-{{Counting Based K}}-{{Edge Imaging}} in {{X}}-Ray {{Computed Tomography}}},
  volume = {30},
  issn = {0278-0062, 1558-254X},
  doi = {10.1109/TMI.2011.2142188},
  number = {9},
  journal = {IEEE Transactions on Medical Imaging},
  author = {Roessl, E. and Brendel, B. and Engel, Klaus-J{\"u}rgen and Schlomka, Jens-Peter and Thran, A. and Proksa, R.},
  month = sep,
  year = {2011},
  pages = {1678--1690},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/68WZI4JM/Roessl et al. - 2011 - Sensitivity of Photon-Counting Based K-Edge Imagin.pdf}
}

@book{bracewell_fourier_2003,
  title = {Fourier Analysis and Imaging},
  author = {Bracewell, R. N.},
  year = {2003},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/WTKZ5XRE/books.html}
}

@article{isola_motion-compensated_2008,
  title = {Motion-Compensated Iterative Cone-Beam {{CT}} Image Reconstruction with Adapted Blobs as Basis Functions},
  volume = {53},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/53/23/009},
  abstract = {This paper presents a three-dimensional method to reconstruct moving objects from cone-beam X-ray projections using an iterative reconstruction algorithm and a given motion vector field. For the image representation, adapted blobs are used, which can be implemented efficiently as basis functions. Iterative reconstruction requires the calculation of line integrals (forward projections) through the image volume, which are compared with the actual measurements to update the image volume. In the existence of a divergent motion vector field, a change in the volumes of the blobs has to be taken into account in the forward and backprojections. An efficient method to calculate the line integral through the adapted blobs is proposed. It solves the problem, how to compensate for the divergence in the motion vector field on a grid of basis functions. The method is evaluated on two phantoms, which are subject to three different known motions. Moreover, a motion-compensated filtered back-projection reconstruction method is used, and the reconstructed images are compared. Using the correct motion vector field with the iterative motion-compensated reconstruction, sharp images are obtained, with a quality that is significantly better than gated reconstructions.},
  number = {23},
  journal = {Physics in Medicine and Biology},
  author = {Isola, A A and Ziegler, A and Koehler, T and Niessen, W J and Grass, M},
  month = dec,
  year = {2008},
  keywords = {Algorithms,Computer Simulation,Cone-Beam Computed Tomography,Humans,Image Processing; Computer-Assisted,Imaging; Three-Dimensional,Phantoms; Imaging,Radiographic Image Interpretation; Computer-Assisted,Software},
  pages = {6777--6797},
  pmid = {18997267}
}

@article{tang_fully_2012,
  title = {A Fully Four-Dimensional, Iterative Motion Estimation and Compensation Method for Cardiac {{CT}}},
  volume = {39},
  issn = {0094-2405},
  doi = {10.1118/1.4725754},
  abstract = {Purpose: To develop a new fully four-dimensional (4D), iterative image reconstruction algorithm for cardiac CT that alternates the following two methods: estimation of a time-dependent motion vector field (MVF) of the heart from image data and reconstruction of images using the estimated MVF and projection data., Methods: Volumetric image data at different cardiac phase points were obtained using electrocardiogram-gated CT. Motion estimation (ME) and motion-compensated image reconstruction (MCR) were performed alternately until convergence was achieved. The ME method estimated the cardiac MVF using 4D nonrigid image registration between a cardiac reference phase and all the other phases. The nonrigid deformation of the heart was modeled using cubic B-splines. The cost function consisted of a sum of squared weighted differences and spatial and temporal regularization terms. A nested conjugate gradient optimization algorithm was applied to minimize the cost function and estimate the MVFs. Cardiac images were reconstructed using a motion-tracking algorithm that utilized the MVFs estimated by the ME method. The reconstructed images supplied the input to the ME of the next iteration. The performance of the proposed method was evaluated using four patient data sets acquired with a 64-slice CT scanner. The heart rates of the patients ranged from 52 to 71 beats/min., Results: Motion artifacts were significantly reduced, and the image quality increased with the number of iterations. Without MCR, the right coronary artery (RCA) was deformed into an arc in axial images of rapid phases. With the proposed method the RCA appeared sharper and was reconstructed similar in shape to the reconstruction at the quiescent phase at mid-diastole. The boundary between the interventricular septum and the right ventricle was also clearer and sharper using the proposed algorithm. The steepness of the transition range at a rapid phase (35\% R-R) was increased from 6.8 HU/pixel to 11.5 HU/pixel. The ME-MCR algorithm converged in just four iterations., Conclusion: We developed a fully 4D image reconstruction method that alternates ME and MCR algorithms in an iterative fashion. Performance tests using clinical patient data resulted in reduced motion artifacts.},
  number = {7},
  journal = {Medical Physics},
  author = {Tang, Qiulin and Cammin, Jochen and Srivastava, Somesh and Taguchi, Katsuyuki},
  month = jul,
  year = {2012},
  pages = {4291--4305},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/5QJ8XNSU/Tang et al. - 2012 - A fully four-dimensional, iterative motion estimat.pdf},
  pmid = {22830763},
  pmcid = {PMC3396707}
}

@article{boussel_photon_2014,
  title = {Photon Counting Spectral {{CT}} Component Analysis of Coronary Artery Atherosclerotic Plaque Samples},
  volume = {87},
  issn = {1748-880X},
  doi = {10.1259/bjr.20130798},
  abstract = {OBJECTIVE: To evaluate the capabilities of photon counting spectral CT to differentiate components of coronary atherosclerotic plaque based on differences in spectral attenuation and iodine-based contrast agent concentration.
METHODS: 10 calcified and 13 lipid-rich non-calcified histologically demonstrated atheromatous plaques from post-mortem human coronary arteries were scanned with a photon counting spectral CT scanner. Individual photons were counted and classified in one of six energy bins from 25 to 70 keV. Based on a maximum likelihood approach, maps of photoelectric absorption (PA), Compton scattering (CS) and iodine concentration (IC) were reconstructed. Intensity measurements were performed on each map in the vessel wall, the surrounding perivascular fat and the lipid-rich and the calcified plaques. PA and CS values are expressed relative to pure water values. A comparison between these different elements was performed using Kruskal-Wallis tests with pairwise post hoc Mann-Whitney U-tests and Sidak p-value adjustments.
RESULTS: RESULTS for vessel wall, surrounding perivascular fat and lipid-rich and calcified plaques were, respectively, 1.19 $\pm$ 0.09, 0.73 $\pm$ 0.05, 1.08 $\pm$ 0.14 and 17.79 $\pm$ 6.70 for PA; 0.96 $\pm$ 0.02, 0.83 $\pm$ 0.02, 0.91 $\pm$ 0.03 and 2.53 $\pm$ 0.63 for CS; and 83.3 $\pm$ 10.1, 37.6 $\pm$ 8.1, 55.2 $\pm$ 14.0 and 4.9 $\pm$ 20.0 mmol l(-1) for IC, with a significant difference between all tissues for PA, CS and IC (p $<$ 0.012).
CONCLUSION: This study demonstrates the capability of energy-sensitive photon counting spectral CT to differentiate between calcifications and iodine-infused regions of human coronary artery atherosclerotic plaque samples by analysing differences in spectral attenuation and iodine-based contrast agent concentration.
ADVANCES IN KNOWLEDGE: Photon counting spectral CT is a promising technique to identify plaque components by analysing differences in iodine-based contrast agent concentration, photoelectric attenuation and Compton scattering.},
  language = {ENG},
  number = {1040},
  journal = {The British Journal of Radiology},
  author = {Boussel, L. and Coulon, P. and Thran, A. and Roessl, E. and Martens, G. and Sigovan, M. and Douek, P.},
  month = aug,
  year = {2014},
  keywords = {Autopsy,Contrast Media,Coronary Artery Disease,Humans,Photons,Plaque; Atherosclerotic,Radiographic Image Interpretation; Computer-Assisted,Scattering; Radiation,Tomography; X-Ray Computed},
  pages = {20130798},
  pmid = {24874766},
  pmcid = {PMC4112393}
}

@article{zhu_improved_2013,
  title = {Improved {{Compressed Sensing}}-{{Based Algorithm}} for {{Sparse}}-{{View CT Image Reconstruction}}},
  volume = {2013},
  issn = {1748-670X},
  doi = {10.1155/2013/185750},
  abstract = {In computed tomography (CT), there are many situations where reconstruction has to be performed with sparse-view data. In sparse-view CT imaging, strong streak artifacts may appear in conventionally reconstructed images due to limited sampling rate that compromises image quality. Compressed sensing (CS) algorithm has shown potential to accurately recover images from highly undersampled data. In the past few years, total-variation-(TV-) based compressed sensing algorithms have been proposed to suppress the streak artifact in CT image reconstruction. In this paper, we propose an efficient compressed sensing-based algorithm for CT image reconstruction from few-view data where we simultaneously minimize three parameters: the norm, total variation, and a least squares measure. The main feature of our algorithm is the use of two sparsity transforms\&\#x2014;discrete wavelet transform and discrete gradient transform. Experiments have been conducted using simulated phantoms and clinical data to evaluate the performance of the proposed algorithm. The results using the proposed scheme show much smaller streaking artifacts and reconstruction errors than other conventional methods.},
  language = {en},
  journal = {Computational and Mathematical Methods in Medicine},
  author = {Zhu, Zangen and Wahid, Khan and Babyn, Paul and Cooper, David and Pratt, Isaac and Carter, Yasmin},
  month = mar,
  year = {2013},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/82MER8VR/Zhu et al. - 2013 - Improved Compressed Sensing-Based Algorithm for Sp.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/BBUUGA3F/185750.html}
}

@article{low_method_2003,
  title = {A Method for the Reconstruction of Four-Dimensional Synchronized {{CT}} Scans Acquired during Free Breathing},
  volume = {30},
  issn = {0094-2405},
  abstract = {Breathing motion is a significant source of error in radiotherapy treatment planning for the thorax and upper abdomen. Accounting for breathing motion has a profound effect on the size of conformal radiation portals employed in these sites. Breathing motion also causes artifacts and distortions in treatment planning computed tomography (CT) scans acquired during free breathing and also causes a breakdown of the assumption of the superposition of radiation portals in intensity-modulated radiation therapy, possibly leading to significant dose delivery errors. Proposed voluntary and involuntary breath-hold techniques have the potential for reducing or eliminating the effects of breathing motion, however, they are limited in practice, by the fact that many lung cancer patients cannot tolerate holding their breath. We present an alternative solution to accounting for breathing motion in radiotherapy treatment planning, where multislice CT scans are collected simultaneously with digital spirometry over many free breathing cycles to create a four-dimensional (4-D) image set, where tidal lung volume is the additional dimension. An analysis of this 4-D data leads to methods for digital-spirometry, based elimination or accounting of breathing motion artifacts in radiotherapy treatment planning for free breathing patients. The 4-D image set is generated by sorting free-breathing multislice CT scans according to user-defined tidal-volume bins. A multislice CT scanner is operated in the cin{\'e} mode, acquiring 15 scans per couch position, while the patient undergoes simultaneous digital-spirometry measurements. The spirometry is used to retrospectively sort the CT scans by their correlated tidal lung volume within the patient's normal breathing cycle. This method has been prototyped using data from three lung cancer patients. The actual tidal lung volumes agreed with the specified bin volumes within standard deviations ranging between 22 and 33 cm3. An analysis of sagittal and coronal images demonstrated relatively small ($<$1 cm) motion artifacts along the diaphragm, even for tidal volumes where the rate of breathing motion is greatest. While still under development, this technology has the potential for revolutionizing the radiotherapy treatment planning for the thorax and upper abdomen.},
  language = {eng},
  number = {6},
  journal = {Medical physics},
  author = {Low, Daniel A and Nystrom, Michelle and Kalinin, Eugene and Parikh, Parag and Dempsey, James F and Bradley, Jeffrey D and Mutic, Sasa and Wahab, Sasha H and Islam, Tareque and Christensen, Gary and Politte, David G and Whiting, Bruce R},
  month = jun,
  year = {2003},
  keywords = {Adult,Aged,Aged; 80 and over,Artifacts,Feedback,Female,Humans,Imaging; Three-Dimensional,Lung,Lung Neoplasms,Male,Movement,Posture,Quality Control,Radiographic Image Interpretation; Computer-Assisted,Radiometry,Radiotherapy Dosage,Radiotherapy Planning; Computer-Assisted,Reproducibility of Results,Respiration,Sensitivity and Specificity,Spirometry,Subtraction Technique},
  pages = {1254--1263},
  pmid = {12852551}
}

@article{blocklet_maximum-likelihood_1999,
  title = {Maximum-Likelihood Reconstruction with Ordered Subsets in Bone {{SPECT}}},
  volume = {40},
  issn = {0161-5505},
  abstract = {This study was aimed at determining whether the ordered-subset expectation maximum (OSEM) is more effective than filtered backprojection (FBP) for bone SPECT in the routine clinical context.
METHODS: Fifty-seven consecutive bone SPECT studies were analyzed. They included pelvic and lumbar spine, thoracolumbar spine, head and neck, feet and shoulders. A 64-projection SPECT study was acquired over 360 degrees by single-head cameras 2-3 h after the injection of 750 MBq 99mTc-methylene diphosphonate. Three observers compared the OSEM and FBP reconstructed images.
RESULTS: Streak artifacts, always present with FBP, were rarely generated with the OSEM. When present (n = 24), artifacts associated with negative values near hyperactivities in FBP were not generated with the OSEM in 67\% of the cases (n = 16), permitting a satisfactory interpretation of these regions. In half of the other cases (17\%, n = 4/24), interpretation was precluded. In only one case did the three observers agree that more hyperactivities were seen with the OSEM. Ninety-six percent of the OSEM pictures were superior or equal to FBP for anatomic resolution and were clearly better in 12\% of the cases. The extent of the lesion with the OSEM seemed better or equally defined in 96\% and clearly better in 14\% of the cases. The low-activity regions were better or equally visualized in all cases and were clearly better seen in 23\% of the cases. The quality of the pictures was found to be better or superior with the OSEM in 98\% of the cases and definitely better in 65\% of the cases.
CONCLUSION: Replacement of FBP by the OSEM in bone SPECT would be beneficial to clinical practice.},
  language = {eng},
  number = {12},
  journal = {Journal of Nuclear Medicine: Official Publication, Society of Nuclear Medicine},
  author = {Blocklet, D. and Seret, A. and Popa, N. and Schoutens, A.},
  month = dec,
  year = {1999},
  keywords = {Artifacts,Bone Diseases,Bone and Bones,Humans,Image Processing; Computer-Assisted,Likelihood Functions,Radiopharmaceuticals,Technetium Tc 99m Medronate,Tomography; Emission-Computed; Single-Photon},
  pages = {1978--1984},
  pmid = {10616874}
}

@inproceedings{tang_dose_2011,
  title = {Dose Reduction Using Prior Image Constrained Compressed Sensing ({{DR}}-{{PICCS}})},
  doi = {10.1117/12.878200},
  author = {Tang, Jie and Theriault Lauzier, Pascal and Chen, Guang-Hong},
  year = {2011},
  pages = {79612K--79612K--8},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/MNT2KMWU/2011SPIE.7961E..html}
}

@article{roessl_k-edge_2007,
  title = {K-Edge Imaging in x-Ray Computed Tomography Using Multi-Bin Photon Counting Detectors},
  volume = {52},
  issn = {0031-9155, 1361-6560},
  doi = {10.1088/0031-9155/52/15/020},
  number = {15},
  journal = {Physics in Medicine and Biology},
  author = {Roessl, E and Proksa, R},
  month = aug,
  year = {2007},
  pages = {4679--4696},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/H4T4GCHG/roessl2007.pdf}
}

@inproceedings{mory_4d_2015-1,
  address = {Lyon},
  title = {{{4D Tomography}}: An {{Application}} of {{Incremental Constraint Projection Methods}} for {{Variational Inequalities}}},
  author = {Mory, Cyril and Rit, Simon and Sixou, Bruno},
  month = sep,
  year = {2015},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/KV4K8SZA/Mory et al. - 2015 - 4D Tomography an Application of Incremental Const.pdf}
}

@article{garduno_reconstruction_2011,
  title = {Reconstruction from a {{Few Projections}} by $\mathscr{l}$(1)-{{Minimization}} of the {{Haar Transform}}},
  volume = {27},
  issn = {0266-5611},
  doi = {10.1088/0266-5611/27/5/055006},
  abstract = {Much recent activity is aimed at reconstructing images from a few projections. Images in any application area are not random samples of all possible images, but have some common attributes. If these attributes are reflected in the smallness of an objective function, then the aim of satisfying the projections can be complemented with the aim of having a small objective value. One widely investigated objective function is total variation (TV), it leads to quite good reconstructions from a few mathematically ideal projections. However, when applied to measured projections that only approximate the mathematical ideal, TV-based reconstructions from a few projections may fail to recover important features in the original images. It has been suggested that this may be due to TV not being the appropriate objective function and that one should use the $\mathscr{l}$(1)-norm of the Haar transform instead. The investigation reported in this paper contradicts this. In experiments simulating computerized tomography (CT) data collection of the head, reconstructions whose Haar transform has a small $\mathscr{l}$(1)-norm are not more efficacious than reconstructions that have a small TV value. The search for an objective function that provides diagnostically efficacious reconstructions from a few CT projections remains open.},
  number = {5},
  journal = {Inverse problems},
  author = {Gardu{\~n}o, E and Herman, G T and Davidi, R},
  month = may,
  year = {2011},
  pmid = {21909175}
}

@article{courdurier_solving_2008,
  title = {Solving the Interior Problem of Computed Tomography Using {\emph{a Priori}} Knowledge},
  volume = {24},
  issn = {0266-5611, 1361-6420},
  doi = {10.1088/0266-5611/24/6/065001},
  number = {6},
  journal = {Inverse Problems},
  author = {Courdurier, M and Noo, F and Defrise, M and Kudo, H},
  month = dec,
  year = {2008},
  pages = {065001},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/JTT97SF9/nihms77371.pdf}
}

@article{chambolle_algorithm_2004,
  title = {An {{Algorithm}} for {{Total Variation Minimization}} and {{Applications}}},
  volume = {20},
  issn = {0924-9907},
  doi = {10.1023/B:JMIV.0000011325.36760.1e},
  abstract = {We propose an algorithm for minimizing the total variation of an image, and provide a proof of convergence. We show applications to image denoising, zooming, and the computation of the mean curvature motion of interfaces.},
  number = {1-2},
  journal = {J. Math. Imaging Vis.},
  author = {Chambolle, Antonin},
  month = jan,
  year = {2004},
  keywords = {Denoising,Image reconstruction,Total variation,mean curvature motion,zooming},
  pages = {89--97},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/R72RJIAH/Chambolle - 2004 - An Algorithm for Total Variation Minimization and .pdf}
}

@article{xu_sparsity-regularized_2014,
  title = {Sparsity-Regularized Image Reconstruction of Decomposed {{K}}-Edge Data in Spectral {{CT}}},
  volume = {59},
  issn = {0031-9155, 1361-6560},
  doi = {10.1088/0031-9155/59/10/N65},
  number = {10},
  journal = {Physics in Medicine and Biology},
  author = {Xu, Qiaofeng and Sawatzky, Alex and Anastasio, Mark A and Schirra, Carsten O},
  month = may,
  year = {2014},
  pages = {N65--N79},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/RB5PXNM2/Xu et al. - 2014 - Sparsity-regularized image reconstruction of decom.pdf}
}

@article{schwartz_potential_2011,
  title = {Potential Role of Three-Dimensional Rotational Angiography and {{C}}-Arm {{CT}} for Valvular Repair and Implantation},
  volume = {27},
  issn = {1875-8312},
  doi = {10.1007/s10554-011-9839-9},
  abstract = {Imaging modalities utilized in the interventional cardiology suite have seen an impressive evolution and expansion recently, particularly with regard to the recent interest in three-dimensional (3D) imaging. Despite this, the backbone of visualization in the catheterization laboratory remains two-dimensional (2D) X-ray fluoroscopy and cine-angiography. New imaging techniques under development, referred to as three-dimensional rotational angiography (RA) and C-arm CT, hold great promise for improving current device implantation and understanding of cardiovascular anatomy. This paper reviews the evolution of rotational angiography and advanced 3D X-ray imaging applications to interventional cardiology.},
  number = {8},
  journal = {The international journal of cardiovascular imaging},
  author = {Schwartz, Jonathan G and Neubauer, Anne M and Fagan, Thomas E and Noordhoek, Niels J and Grass, Michael and Carroll, John D},
  month = dec,
  year = {2011},
  keywords = {Coronary Angiography,Heart Catheterization,Heart Valve Diseases,Heart Valve Prosthesis Implantation,Humans,Imaging; Three-Dimensional,Predictive Value of Tests,Radiographic Image Interpretation; Computer-Assisted,Tomography; X-Ray Computed,Treatment Outcome},
  pages = {1205--1222},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/7AWXVZQ7/Schwartz et al. - 2011 - Potential role of three-dimensional rotational ang.pdf},
  pmid = {21394614}
}

@incollection{hutchison_combining_2014,
  address = {Cham},
  title = {Combining {{Image Registration}}, {{Respiratory Motion Modelling}}, and {{Motion Compensated Image Reconstruction}}},
  volume = {8545},
  isbn = {978-3-319-08553-1 978-3-319-08554-8},
  booktitle = {Biomedical {{Image Registration}}},
  publisher = {{Springer International Publishing}},
  author = {McClelland, Jamie R. and Champion, Benjamin A. S. and Hawkes, David J.},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Kobsa, Alfred and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Terzopoulos, Demetri and Tygar, Doug and Weikum, Gerhard and Ourselin, S{\'e}bastien and Modat, Marc},
  year = {2014},
  pages = {103--113},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/3MM7N92T/McClelland et al. - 2014 - Combining Image Registration, Respiratory Motion M.pdf}
}

@article{saint-felix_vivo_1994,
  title = {In Vivo Evaluation of a New System for {{3D}} Computerized Angiography},
  volume = {39},
  issn = {0031-9155},
  abstract = {A new system has been designed and built to validate the concept of 3D computerized angiography (CA). This system can acquire a set of 2D digital subtracted angiography images while rotating around a patient and then, using these images, reconstruct a 3D representation of the opacified vasculature. The design principles and main characteristics of the system are described, with special attention paid to data processing aspects. An initial in vivo evaluation of this system performed on anaesthetized animals and human volunteers is presented. The influence on the quality of the 3D reconstruction of different factors such as volume resolution, estimation method, source trajectory and number of projections is discussed.},
  language = {eng},
  number = {3},
  journal = {Physics in medicine and biology},
  author = {Saint-F{\'e}lix, D and Trousset, Y and Picard, C and Ponchut, C and Rom{\'e}as, R and Roug{\'e}e, A},
  month = mar,
  year = {1994},
  keywords = {Angiography,Animals,Aorta,Calibration,Carotid Arteries,Dogs,Evaluation Studies as Topic,Humans,Image Processing; Computer-Assisted,Imaging; Three-Dimensional,Software,Time Factors},
  pages = {583--595},
  pmid = {15551600}
}

@article{reich_weak_1979,
  title = {Weak Convergence Theorems for Nonexpansive Mappings in {{Banach}} Spaces},
  volume = {67},
  issn = {0022-247X},
  doi = {10.1016/0022-247X(79)90024-6},
  number = {2},
  journal = {Journal of Mathematical Analysis and Applications},
  author = {Reich, Simeon},
  month = feb,
  year = {1979},
  pages = {274--276},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/NXWEBKVH/Reich - 1979 - Weak convergence theorems for nonexpansive mapping.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/W8GPDKHS/0022247X79900246.html}
}

@article{zeng_new_2010,
  title = {A New Iterative Reconstruction Algorithm for {{2D}} Exterior Fan-Beam {{CT}}},
  volume = {18},
  doi = {10.3233/XST-2010-0259},
  abstract = {The exterior computed tomography (CT) problem is one kind of truncation problem. It is very ill-posed, so that accurate reconstruction of the attenuation function is hardly possible from real data. Based on projection onto convex sets (POCS) algorithm, total variation minimization (TVM) methods, and C-V model, we develop and investigate a new iterative reconstruction algorithm, which is referred to as subregion-averaged-TVM-POCS (SA-TVM-POCS). Numerical simulations are presented to illustrate the efficiency of the algorithm. The results of this paper can be easily applied to other x-ray CT reconstruction problems.},
  number = {3},
  journal = {Journal of X-Ray Science and Technology},
  author = {Zeng, Li and Liu, Baodong and Liu, Linghui and Xiang, Caibing},
  month = jan,
  year = {2010},
  pages = {267--277},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/9H8NFBPQ/Zeng et al. - 2010 - A new iterative reconstruction algorithm for 2D ex.pdf}
}

@article{santin_gate_2003,
  title = {{{GATE}}: A {{Geant4}}-Based Simulation Platform for {{PET}} and {{SPECT}} Integrating Movement and Time Management},
  volume = {50},
  issn = {0018-9499},
  shorttitle = {{{GATE}}},
  doi = {10.1109/TNS.2003.817974},
  abstract = {GATE, the Geant4 application for tomographic emission, is a simulation platform developed for PET and SPECT. It combines a powerful simulation core, the Geant4 toolkit, with newly developed software components dedicated to nuclear medicine. In particular, it models the passing of time during real acquisitions, allowing it to handle dynamic systems such as decaying source distributions or moving detectors. We present several series of results that illustrate the possibilities of this new platform. The simulation of decaying sources is illustrated on a dual-isotope acquisition with multiple time-frames. Count rate curves taking into account random coincidences and dead-time are shown for a dual-crystal setup and for a small-animal PET scanner configuration. Simulated resolution curves and reconstructed images are shown for rotating PET scanners. Lastly, we present first comparisons of simulated point-spread functions and spectra with experimental results obtained from a small-animal gamma camera prototype.},
  number = {5},
  journal = {IEEE Transactions on Nuclear Science},
  author = {Santin, G. and Strul, D. and Lazaro, D. and Simon, L. and Krieguer, M. and Martins, M. V. and Breton, V. and Morel, C.},
  month = oct,
  year = {2003},
  keywords = {Application software,Cameras,Detectors,GATE,Geant4,Image reconstruction,Image resolution,Medical simulation,Monte Carlo methods,PET,Positron emission tomography,Power system modeling,SPECT,Software tools,dead-time,nuclear medicine,point-spread functions,random coincidences,single photon emission computed tomography},
  pages = {1516--1521},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/JHJEHRR9/Santin et al. - 2003 - GATE a Geant4-based simulation platform for PET a.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/I8QJRI62/1236960.html}
}

@article{prummer_cardiac_2009-1,
  title = {Cardiac {{C}}-Arm {{CT}}: A Unified Framework for Motion Estimation and Dynamic {{CT}}},
  volume = {28},
  issn = {1558-0062},
  shorttitle = {Cardiac {{C}}-Arm {{CT}}},
  doi = {10.1109/TMI.2009.2025499},
  abstract = {Generating 3-D images of the heart during interventional procedures is a significant challenge. In addition to real time fluoroscopy, angiographic C-arm systems can also now be used to generate 3-D/4-D CT images on the same system. One protocol for cardiac CT uses ECG triggered multisweep scans. A 3-D volume of the heart at a particular cardiac phase is then reconstructed by applying Feldkamp (FDK) reconstruction to the projection images with retrospective ECG gating. In this work we introduce a unified framework for heart motion estimation and dynamic cone-beam reconstruction using motion corrections. The benefits of motion correction are 1) increased temporal and spatial resolution by removing cardiac motion which may still exist in the ECG gated data sets and 2) increased signal-to-noise ratio (SNR) by using more projection data than is used in standard ECG gated methods. Three signal-enhanced reconstruction methods are introduced that make use of all of the acquired projection data to generate a 3-D reconstruction of the desired cardiac phase. The first averages all motion corrected back-projections; the second and third perform a weighted averaging according to 1) intensity variations and 2) temporal distance relative to a time resolved and motion corrected reference FDK reconstruction. In a comparison study seven methods are compared: nongated FDK, ECG-gated FDK, ECG-gated, and motion corrected FDK, the three signal-enhanced approaches, and temporally aligned and averaged ECG-gated FDK reconstructions. The quality measures used for comparison are spatial resolution and SNR. Evaluation is performed using phantom data and animal models. We show that data driven and subject-specific motion estimation combined with motion correction can decrease motion-related blurring substantially. Furthermore, SNR can be increased by up to 70\% while maintaining spatial resolution at the same level as is provided by the ECG-gated FDK. The presented framework provides excellent image quality for cardiac C-arm CT.},
  number = {11},
  journal = {IEEE Transactions on Medical Imaging},
  author = {Pr{\"u}mmer, M. and Hornegger, Joachim and Lauritsch, Guenter and Wigstr{\"o}m, Lars and Girard-Hughes, Erin and Fahrig, Rebecca},
  month = nov,
  year = {2009},
  keywords = {Algorithms,Animals,Computer Simulation,Cone-Beam Computed Tomography,Electrocardiography,Heart,Image Processing; Computer-Assisted,Motion,Phantoms; Imaging,Signal Processing; Computer-Assisted,Surgery; Computer-Assisted,Swine},
  pages = {1836--1849},
  pmid = {19884068}
}

@article{hansis_evaluation_2008,
  title = {Evaluation of {{Iterative Sparse Object Reconstruction From Few Projections}} for 3-{{D Rotational Coronary Angiography}}},
  volume = {27},
  issn = {0278-0062},
  doi = {10.1109/TMI.2008.2006514},
  number = {11},
  journal = {IEEE Transactions on Medical Imaging},
  author = {Hansis, E. and Schafer, D. and Dossel, O. and Grass, M.},
  month = nov,
  year = {2008},
  pages = {1548--1555},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/S2UI34U9/Hansis et al. - 2008 - Evaluation of Iterative Sparse Object Reconstructi.pdf}
}

@inproceedings{chinn_fast_1995,
  title = {A Fast Preconditioned Conjugate Gradient Algorithm for Regularized {{WLS}} Reconstruction for {{PET}}},
  volume = {2},
  doi = {10.1109/NSSMIC.1995.510496},
  abstract = {In this paper, new theory and methods for a class of preconditioners are developed for accelerating the convergence rate of iterative reconstruction. A preconditioned conjugate gradient (PCG) iterative algorithm for weighted least squares reconstruction (WLS) is formulated for emission tomography. A linear regularization method is adopted and shown to be equivalent to penalized WLS reconstruction with quadratic penalty functions. Using simulated positron emission tomography (PET) data of the Hoffman brain phantom, it was shown that the convergence rate of the PCG using the new preconditioner is an order-of-magnitude faster compared to the standard conjugate gradient algorithm. It is also significantly faster than previously proposed preconditioners for emission tomography},
  booktitle = {1995 {{IEEE Nuclear Science Symposium}} and {{Medical Imaging Conference Record}}, 1995},
  author = {Chinn, G. and Huang, S.-C.},
  month = oct,
  year = {1995},
  keywords = {Acceleration,Biomedical imaging,Brain,Computational efficiency,Equations,Hoffman brain phantom,Image converters,Image reconstruction,Iterative algorithms,PET,Positron emission tomography,Vectors,algorithm theory,convergence,fast preconditioned conjugate gradient algorithm,iterative methods,iterative reconstruction convergence rate,least mean squares methods,medical diagnostic imaging,medical image processing,nuclear medicine,preconditioners,quadratic penalty functions,weighted least squares reconstruction},
  pages = {1297--1301 vol.2},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/ITC34Z5Q/Chinn and Huang - 1995 - A fast preconditioned conjugate gradient algorithm.pdf}
}

@article{lalush_improving_1994,
  title = {Improving the Convergence of Iterative Filtered Backprojection Algorithms},
  volume = {21},
  doi = {10.1118/1.597210},
  abstract = {Several authors have proposed variations of the iterative filtered backprojection (IFBP) reconstruction algorithms claiming fast initial convergence rates. We have found that these algorithms are trying to minimize an unusual squared-error criterion in a suboptimal way. As a result, existing IFBP algorithms are inefficient in the minimization of the criterion, and may become unstable at higher iteration numbers. We show that existing IFBP algorithms can be modified to use the steepest descent technique by simply optimizing the step size at each iteration. Further gains in convergence rates can be achieved with conjugate gradient IFBP algorithms derived from the same criterion. The steepest descent and conjugate gradient IFBP algorithms are guaranteed to converge, unlike some IFBP algorithms, and will do so in fewer iterations than existing IFBP algorithms.},
  number = {8},
  journal = {Medical Physics},
  author = {Lalush, David S. and Tsui, Benjamin M. W.},
  year = {1994},
  keywords = {Algorithms,attenuation,convergence,image forming,iterative methods,optimization,single photon emission computed tomography},
  pages = {1283--1286},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/KGS4KX92/Lalush and Tsui - 1994 - Improving the convergence of iterative filtered ba.pdf}
}

@inproceedings{ducros_non-linear_2016,
  address = {Bamberg, Germany},
  title = {Non-Linear Regularized Decomposition of Spectral x-Ray Projection Images},
  author = {Ducros, Nicolas and Rit, Simon and Sixou, Bruno and Peyrin, Francoise},
  month = jul,
  year = {2016},
  pages = {49--52},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/C8HN67BX/Ducros et al. - 2016 - Non-linear regularized decomposition of spectral x.pdf}
}

@article{desbat_compensation_2007,
  title = {Compensation of {{Some Time Dependent Deformations}} in {{Tomography}}},
  volume = {26},
  issn = {0278-0062},
  doi = {10.1109/TMI.2006.889743},
  abstract = {This work concerns 2D+t dynamic tomography. We show that a much larger class of deformations than the affine transforms can be compensated analytically within filtered back projection algorithms in 2D parallel beam and fan beam dynamic tomography. We present numerical experiments on the Shepp and Logan phantom showing that nonaffine deformations can be compensated. A generalization to 3D cone beam tomography is proposed},
  number = {2},
  journal = {IEEE Transactions on Medical Imaging},
  author = {Desbat, L. and Roux, S. and Grangeat, P.},
  month = feb,
  year = {2007},
  keywords = {2D parallel beam dynamic tomography,3D cone beam tomography,Algorithms,Analytic reconstruction,Artifacts,Computed tomography,Geometry,Heart,Image reconstruction,Imaging phantoms,Imaging; Three-Dimensional,Motion,Motion compensation,Projection algorithms,Radiographic Image Enhancement,Radiographic Image Interpretation; Computer-Assisted,Reproducibility of Results,Sensitivity and Specificity,Time Factors,Tomography; X-Ray Computed,affine transforms,algorithm design and analysis,attenuation,biomechanics,computerised tomography,deformation,deformation compensation,dynamic tomography,fan beam dynamic tomography,filtered backprojection algorithms,image sampling,medical image processing,phantom,phantoms,time dependent deformation compensation},
  pages = {261--269},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/IP33J9R3/login.html}
}

@article{lo_projection_1992,
  title = {{{PROJECTION DOMAIN COMPENSATION OF MISSING ANGLES FOR FAN}}-{{BEAM CT}}   {{RECONSTRUCTION}}},
  volume = {16},
  issn = {0895-6111},
  doi = {10.1016/0895-6111(92)90028-8},
  abstract = {An improved method is proposed for fan-beam computed tomographic (Cr)   reconstruction from data with limited views. Compensation for the   missing projections for fan-beam CT can be partially accomplished by   using the coincident ray or by an interpolation technique using circular   sample theory. In this article, the authors propose a more accurate   compensation method for the missing projections whether the coincident   ray pairs exist or not. The fan-beam reprojection algorithm, which is   the inverse operator of the convolution filter, was extended from the   projection space iteration reconstruction-reprojection (PSIRR) in   parallel beam geometry. In addition, this algorithm was validated by   applying the Shepp-Logan phantom for a computer simulation in the   equi-angular fan-beam CT geometry.},
  language = {English},
  number = {4},
  journal = {Computerized Medical Imaging and Graphics},
  author = {LO, SCB and LOU, SLA and MUN, SK},
  year = {JUL-AUG 1992},
  keywords = {Algorithms,Computed tomography,computed-tomography,fan-beam ct   reconstruction,filtered back-projection,image-reconstruction,limited data,limited view ct reprojection,reprojection,sinogram},
  pages = {259--269},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/BGTKQ77C/full_record.html},
  note = {WOS:A1992JJ48100003}
}

@article{girard_contrast-enhanced_2015,
  title = {Contrast-{{Enhanced C}}-Arm {{Computed Tomography Imaging}} of {{Myocardial Infarction}} in the {{Interventional Suite}}},
  volume = {50},
  issn = {1536-0210},
  doi = {10.1097/RLI.0000000000000138},
  abstract = {OBJECTIVES: Cardiac C-arm computed tomography (CT) uses a standard C-arm fluoroscopy system rotating around the patient to provide CT-like images during interventional procedures without moving the patient to a conventional CT scanner. We hypothesized that C-arm CT can be used to visualize and quantify the size of perfusion defects and late enhancement resulting from a myocardial infarction (MI) using contrast-enhanced techniques similar to previous CT and magnetic resonance imaging studies.
MATERIALS AND METHODS: A balloon occlusion followed by reperfusion in a coronary artery was used to study acute and subacute MI in 12 swine. Electrocardiographically gated C-arm CT images were acquired the day of infarct creation (n = 6) or 4 weeks after infarct creation (n = 6). The images were acquired immediately after contrast injection, then at 1 minute, and every 5 minutes up to 30 minutes with no additional contrast. The volume of the infarct as measured on C-arm CT was compared against pathology.
RESULTS: The volume of acute MI, visualized as a combined region of hyperenhancement with a hypoenhanced core, correlated well with pathologic staining (concordance correlation, 0.89; P $<$ 0.0001; mean [SD] difference, 0.67 [2.98]cm3). The volume of subacute MI, visualized as a region of hyperenhancement, correlated well with pathologic staining at imaging times 5 to 15 minutes after contrast injection (concordance correlation, 0.82; P $<$ 0.001; mean difference, -0.64 [1.94]cm3).
CONCLUSIONS: C-arm CT visualization of acute and subacute MI is possible in a porcine model, but improvement in the imaging technique is important before clinical use. Visualization of MI in the catheterization laboratory may be possible and could provide 3-dimensional images for guidance during interventional procedures.},
  language = {ENG},
  number = {6},
  journal = {Investigative Radiology},
  author = {Girard, Erin E. and Al-Ahmad, Amin and Rosenberg, Jarrett and Luong, Richard and Moore, Teri and Lauritsch, G{\"u}nter and Chan, Frandics and Lee, David P. and Fahrig, Rebecca},
  month = jun,
  year = {2015},
  keywords = {Animals,Contrast Media,Disease Models; Animal,Female,Heart,Image Enhancement,Magnetic Resonance Imaging; Interventional,Myocardial Infarction,Swine,Tomography; X-Ray Computed},
  pages = {384--391},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/KGERDPIE/Girard et al. - 2015 - Contrast-Enhanced C-arm Computed Tomography Imagin.pdf},
  pmid = {25635589},
  pmcid = {PMC4424068}
}

@article{hugo_-line_2007,
  title = {On-{{Line Target Position Localization}} in the {{Presence}} of {{Respiration}}: {{A Comparison}} of Two {{Methods}}},
  volume = {69},
  issn = {03603016},
  shorttitle = {On-{{Line Target Position Localization}} in the {{Presence}} of {{Respiration}}},
  doi = {10.1016/j.ijrobp.2007.08.023},
  language = {en},
  number = {5},
  journal = {International Journal of Radiation Oncology*Biology*Physics},
  author = {Hugo, Geoffrey D. and Liang, Jian and Campbell, Jonathan and Yan, Di},
  month = dec,
  year = {2007},
  pages = {1634--1641},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/XH7ER778/hugo2007b.pdf}
}

@article{metz_nonrigid_2011,
  title = {Nonrigid Registration of Dynamic Medical Imaging Data Using {{nD}}+t {{B}}-Splines and a Groupwise Optimization Approach},
  volume = {15},
  issn = {13618415},
  doi = {10.1016/j.media.2010.10.003},
  language = {en},
  number = {2},
  journal = {Medical Image Analysis},
  author = {Metz, C.T. and Klein, S. and Schaap, M. and {van Walsum}, T. and Niessen, W.J.},
  month = apr,
  year = {2011},
  pages = {238--249},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/JJ6MS2KQ/metz2011.pdf}
}

@article{abbas_effects_2013,
  title = {Effects of Sparse Sampling Schemes on Image Quality in Low-Dose {{CT}}},
  volume = {40},
  issn = {00942405},
  doi = {10.1118/1.4825096},
  number = {11},
  journal = {Medical Physics},
  author = {Abbas, Sajid and Lee, Taewon and Shin, Sukyoung and Lee, Rena and Cho, Seungryong},
  year = {2013},
  pages = {111915},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/HF2XI8HQ/Abbas et al. - 2013 - Effects of sparse sampling schemes on image qualit.pdf}
}

@article{millon_clinical_2012,
  title = {Clinical and Histological Significance of Gadolinium Enhancement in Carotid Atherosclerotic Plaque},
  volume = {43},
  issn = {1524-4628},
  doi = {10.1161/STROKEAHA.112.662692},
  abstract = {BACKGROUND AND PURPOSE: Although the ability of MRI to investigate carotid plaque composition is well established, the mechanism and the significance of plaque gadolinium (Gd) enhancement remain unknown. We evaluated clinical and histological significance of Gd enhancement of carotid plaque in patients undergoing endarterectomy for carotid stenosis.
METHODS: Sixty-nine patients scheduled for a carotid endarterectomy prospectively underwent a 3-T MRI. Carotid plaque enhancement was assessed on T1-weighted images performed before and 5 minutes after Gd injection. Enhancement was recorded according to its localization. Histological analysis was performed of the entire plaque and of the area with matched contrast enhancement on MR images.
RESULTS: Gd enhancement was observed in 59\% patients. Three types of carotid plaques were identified depending on enhancement location (shoulder region, shoulder and fibrous cap, and central in the plaque). Fibrous cap rupture, intraplaque hemorrhage, and plaque Gd enhancement was significantly more frequent in symptomatic than in asymptomatic patients (P=0.043, P$<$0.0001, and P=0.034, respectively). After histological analysis, Gd enhancement was significantly associated with vulnerable plaque (American Heart Association VI, P=0.006), neovascularization (P$<$0.0001), macrophages (P=0.030), and loose fibrosis (P$<$0.0001). Prevalence of neovessels, macrophages, and loose fibrosis in the area of Gd enhancement was 97\%, 87\%, and 80\%, respectively, and was different depending on the enhancement location in the plaque. Fibrous cap status and composition were different depending on the type of plaque.
CONCLUSIONS: Gd enhancement of carotid plaque is associated with vulnerable plaque phenotypes and related to an inflammatory process.},
  language = {eng},
  number = {11},
  journal = {Stroke; a journal of cerebral circulation},
  author = {Millon, Antoine and Boussel, Loic and Brevet, Marie and Mathevet, Jean-Louis and Canet-Soulas, Emmanuelle and Mory, Cyril and Scoazec, Jean-Yves and Douek, Philippe},
  month = nov,
  year = {2012},
  keywords = {Carotid Stenosis,Endarterectomy; Carotid,Gadolinium,Humans,Image Enhancement,Image Interpretation; Computer-Assisted,Intracranial Arteriosclerosis,Magnetic Resonance Imaging,Plaque; Atherosclerotic,Sensitivity and Specificity},
  pages = {3023--3028},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/KJ47UR9E/Millon et al. - 2012 - Clinical and histological significance of gadolini.pdf},
  pmid = {22923447}
}

@article{tuy_inversion_1983,
  title = {An {{Inversion Formula}} for {{Cone}}-{{Beam Reconstruction}}},
  volume = {43},
  issn = {0036-1399, 1095-712X},
  doi = {10.1137/0143035},
  number = {3},
  journal = {SIAM Journal on Applied Mathematics},
  author = {Tuy, Heang K.},
  month = jun,
  year = {1983},
  pages = {546--552},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/ADZ5G5F8/Tuy - 1983 - An Inversion Formula for Cone-Beam Reconstruction.pdf}
}

@article{yan_hybrid_2014,
  title = {A Hybrid Reconstruction Algorithm for Fast and Accurate {{4D}} Cone-Beam {{CT}} Imaging},
  volume = {41},
  issn = {0094-2405},
  doi = {10.1118/1.4881326},
  abstract = {Purpose: 4D cone beam CT (4D-CBCT) has been utilized in radiation therapy to provide 4D image guidance in lung and upper abdomen area. However, clinical application of 4D-CBCT is currently limited due to the long scan time and low image quality. The purpose of this paper is to develop a new 4D-CBCT reconstruction method that restores volumetric images based on the 1-min scan data acquired with a standard 3D-CBCT protocol. Methods: The model optimizes a deformation vector field that deforms a patient-specific planning CT (p-CT), so that the calculated 4D-CBCT projections match measurements. A forward-backward splitting (FBS) method is invented to solve the optimization problem. It splits the original problem into two well-studied subproblems, i.e., image reconstruction and deformable image registration. By iteratively solving the two subproblems, FBS gradually yields correct deformation information, while maintaining high image quality. The whole workflow is implemented on a graphic-processing-unit to improve efficiency. Comprehensive evaluations have been conducted on a moving phantom and three real patient cases regarding the accuracy and quality of the reconstructed images, as well as the algorithm robustness and efficiency. Results: The proposed algorithm reconstructs 4D-CBCT images from highly under-sampled projection data acquired with 1-min scans. Regarding the anatomical structure location accuracy, 0.204 mm average differences and 0.484 mm maximum difference are found for the phantom case, and the maximum differences of 0.3\textendash{}0.5 mm for patients 1\textendash{}3 are observed. As for the image quality, intensity errors below 5 and 20 HU compared to the planning CT are achieved for the phantom and the patient cases, respectively. Signal-noise-ratio values are improved by 12.74 and 5.12 times compared to results from FDK algorithm using the 1-min data and 4-min data, respectively. The computation time of the algorithm on a NVIDIA GTX590 card is 1\textendash{}1.5 min per phase. Conclusions: High-quality 4D-CBCT imaging based on the clinically standard 1-min 3D CBCT scanning protocol is feasible via the proposed hybrid reconstruction algorithm.},
  number = {7},
  journal = {Medical Physics},
  author = {Yan, Hao and Zhen, Xin and Folkerts, Michael and Li, Yongbao and Pan, Tinsu and Cervino, Laura and Jiang, Steve B. and Jia, Xun},
  month = jul,
  year = {2014},
  keywords = {Cone beam computed tomography,Image reconstruction,Medical image artifacts,Medical image reconstruction,Three dimensional image processing},
  pages = {071903},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/4RECGXX2/Yan et al. - 2014 - A hybrid reconstruction algorithm for fast and acc.pdf}
}

@article{rit_quantification_2012,
  title = {Quantification of the {{Variability}} of {{Diaphragm Motion}} and {{Implications}} for {{Treatment Margin Construction}}},
  volume = {82},
  issn = {03603016},
  doi = {10.1016/j.ijrobp.2011.06.1986},
  language = {en},
  number = {3},
  journal = {International Journal of Radiation Oncology*Biology*Physics},
  author = {Rit, Simon and {van Herk}, Marcel and Zijp, Lambert and Sonke, Jan-Jakob},
  month = mar,
  year = {2012},
  pages = {e399--e407},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/U5DQIM3U/Rit et al. - 2012 - Quantification of the Variability of Diaphragm Mot.pdf}
}

@article{selesnick_dual-tree_2005,
  title = {The Dual-Tree Complex Wavelet Transform},
  volume = {22},
  number = {6},
  journal = {Signal Processing Magazine, IEEE},
  author = {Selesnick, Ivan W. and Baraniuk, Richard G. and Kingsbury, Nick G.},
  year = {2005},
  pages = {123--151},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/NFMJ27X4/CWT_Tutorial.pdf}
}

@article{christensen_consistent_2001,
  title = {Consistent Image Registration},
  volume = {20},
  number = {7},
  journal = {Medical Imaging, IEEE Transactions on},
  author = {Christensen, Gary E. and Johnson, Hans J.},
  year = {2001},
  pages = {568--582},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/54Q8BVU5/Christensen and Johnson - 2001 - Consistent image registration.pdf}
}

@inproceedings{xu_comparative_2006,
  title = {A Comparative Study of Popular Interpolation and Integration Methods for Use in Computed Tomography},
  booktitle = {Biomedical {{Imaging}}: {{Nano}} to {{Macro}}, 2006. 3rd {{IEEE International Symposium}} On},
  publisher = {{IEEE}},
  author = {Xu, Fang and Mueller, Klaus},
  year = {2006},
  pages = {1252--1255},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/4VIMTVI8/xu2006a.pdf}
}

@article{pfister_tomographic_2013,
  title = {Tomographic Reconstruction with Adaptive Sparsifying Transforms},
  author = {Pfister, Luke},
  year = {2013},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/WE7JP2QC/Luke_Pfister.pdf}
}

@article{jacques_dequantizing_2009,
  title = {Dequantizing {{Compressed Sensing}}: {{When Oversampling}} and {{Non}}-{{Gaussian Constraints Combine}}},
  volume = {abs/0902.2367},
  issn = {0018-9448},
  shorttitle = {Dequantizing {{Compressed Sensing}}},
  doi = {10.1109/TIT.2010.2093310},
  abstract = {In this paper we study the problem of recovering sparse or compressible signals from uniformly quantized measurements. We present a new class of convex optimization programs, or decoders, coined...},
  number = {1},
  journal = {ResearchGate},
  author = {Jacques, Laurent and Hammond, David K. and Fadili, Jalal M.},
  month = feb,
  year = {2009},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/5Q4MSMHS/Jacques et al. - 2009 - Dequantizing Compressed Sensing When Oversampling.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/IADJ2GWH/220489868_Dequantizing_Compressed_Sensing_When_Oversampling_and_Non-Gaussian_Constraints_Combin.html}
}

@article{rit_--fly_2009,
  title = {On-the-Fly Motion-Compensated Cone-Beam {{CT}} Using an a Priori Model of the Respiratory Motion},
  volume = {36},
  issn = {0094-2405},
  abstract = {Respiratory motion causes artifacts in cone-beam (CB) CT images acquired on slow rotating scanners integrated with linear accelerators. Respiration-correlated CBCT has been proposed to correct for the respiratory motion but only a subset of the CB projections is used to reconstruct each frame of the 4D CBCT image and, therefore, adequate image quality requires long acquisition times. In this article, the authors develop an on-the-fly solution to estimate and compensate for the respiratory motion in the reconstruction of a 3D CBCT image from all the CB projections. An a priori motion model of the patient respiratory cycle is estimated from the 4D planning CT. During the acquisition, the model is correlated with the respiration using a respiratory signal extracted from the CB projections. The estimated motion is next compensated for in an optimized reconstruction algorithm. The motion compensated for is forced to be null on average over the acquisition time to ensure that the compensation results in a CBCT image which describes the mean position of each organ, even if the a priori motion model is inaccurate. Results were assessed on simulated, phantom, and patient data. In all experiments, blur was visually reduced by motion-compensated CBCT. Simulations showed robustness to inaccuracies of the motion model observed on patient data such as amplitude variations, phase shifts, and setup errors, thus proving the efficiency of the compensation using an a priori motion model. Noise and view-aliasing artifacts were lower on motion-compensated CBCT images with 1 min scan than on respiration-correlated CBCT images with 4 min scan. Finally, on-the-fly motion estimation and motion-compensated reconstruction were within the acquisition time of the CB projections and the CBCT image available a few seconds after the end of the acquisition. In conclusion, the authors developed and implemented a method for correcting the respiratory motion during the treatment fractions which can replace respiration-correlated CBCT for improving image quality while decreasing acquisition time.},
  language = {eng},
  number = {6},
  journal = {Medical physics},
  author = {Rit, Simon and Wolthaus, Jochem W H and {van Herk}, Marcel and Sonke, Jan-Jakob},
  month = jun,
  year = {2009},
  keywords = {Algorithms,Artifacts,Cone-Beam Computed Tomography,Humans,Lung Neoplasms,Phantoms; Imaging,Radiographic Image Enhancement,Radiographic Image Interpretation; Computer-Assisted,Reproducibility of Results,Respiratory Mechanics,Respiratory-Gated Imaging Techniques,Sensitivity and Specificity},
  pages = {2283--2296},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/STU4ISP3/Rit et al. - 2009 - On-the-fly motion-compensated cone-beam CT using a.pdf},
  pmid = {19610317}
}

@inproceedings{ritschl_new_2010,
  title = {A New Approach to Limited Angle Tomography Using the Compressed Sensing Framework},
  doi = {10.1117/12.844303},
  author = {Ritschl, Ludwig and Bergner, Frank and Kachelriess, Marc},
  year = {2010},
  pages = {76222H--76222H--9},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/Q4T9ME8E/Ritschl et al. - 2010 - A new approach to limited angle tomography using t.PDF}
}

@book{boyd_foundations_2011,
  title = {Foundations and {{Trends}} in {{Optimization}}},
  publisher = {{Courier Dover Publications}},
  author = {Boyd, Stephen and Ye, Yinyu},
  year = {2011},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/U68ASFZX/Boyd and Ye - 2011 - Foundations and Trends in Optimization.pdf}
}

@article{tian_low-dose_2011,
  title = {Low-Dose {{4DCT}} Reconstruction via Temporal Nonlocal Means},
  volume = {38},
  issn = {0094-2405},
  abstract = {PURPOSE: Four-dimensional computed tomography (4DCT) has been widely used in cancer radiotherapy for accurate target delineation and motion measurement for tumors in the thorax and upper abdomen areas. However, its prolonged scanning duration causes a considerable increase of radiation dose compared to conventional CT, which is a major concern in its clinical application. This work is to develop a new algorithm to reconstruct 4DCT images from undersampled projections acquired at low mA s levels in order to reduce the imaging dose.
METHODS: Conventionally, each phase of 4DCT is reconstructed independently using the filtered backprojection (FBP) algorithm. The basic idea of the authors' new algorithm is that by utilizing the common information among different phases, the input information required to reconstruct the image of high quality, and thus the imaging dose, can be reduced. The authors proposed a temporal nonlocal means (TNLM) method to explore the interphase similarity. All phases of the 4DCT images are reconstructed simultaneously by minimizing a cost function consisting of a data fidelity term and a TNLM regularization term. The authors utilized a modified forward-backward splitting algorithm and a Gauss-Jacobi iteration method to efficiently solve the minimization problem. The algorithm was also implemented on a graphics processing unit (GPU) to improve the computational speed. The authors' reconstruction algorithm has been tested on a digital NCAT thorax phantom in three low dose scenarios: All projections with low mA s level, undersampled projections with high mA s level, and undersampled projections with low mA s level.
RESULTS: In all three low dose scenarios, the new algorithm generates visually much better CT images containing less image noise and streaking artifacts compared to the standard FBP algorithm. Quantitative analysis shows that by comparing the authors' TNLM algorithm to the standard FBP algorithm, the contrast-to-noise ratio has been improved by a factor of 3.9-10.2 and the signal-to-noise ratio has been improved by a factor of 2.1-5.9, depending on the cases. In the situation of undersampled projection data, the majority of the streaks in the images reconstructed by FBP can be suppressed using the authors' algorithm. The total reconstruction time for all ten phases of a slice ranges from 40 to 90 s on an NVIDIA Tesla C1060 GPU card.
CONCLUSIONS: The experimental results indicate that the authors' new algorithm outperforms the conventional FBP algorithm in effectively reducing the image artifacts due to undersampling and suppressing the image noise due to the low mA s level.},
  language = {eng},
  number = {3},
  journal = {Medical physics},
  author = {Tian, Zhen and Jia, Xun and Dong, Bin and Lou, Yifei and Jiang, Steve B},
  month = mar,
  year = {2011},
  keywords = {Algorithms,Four-Dimensional Computed Tomography,Image Processing; Computer-Assisted,Phantoms; Imaging,Radiation Dosage,Radiography; Thoracic,Time Factors},
  pages = {1359--1365},
  pmid = {21520846}
}

@article{gay_breath-holding_1994,
  title = {Breath-Holding Capability of Adults. {{Implications}} for Spiral Computed Tomography, Fast-Acquisition Magnetic Resonance Imaging, and Angiography},
  volume = {29},
  issn = {0020-9996},
  abstract = {PURPOSE: The breath-holding capabilities of various groups of individuals were evaluated to develop protocols so that patients undergoing spiral computed tomography (CT), digital angiography, and breath-hold magnetic resonance imaging (MRI) can be studied successfully.
METHODS: Twenty-five outpatients and 25 inpatients (all adults) were studied before undergoing body CT. Each subject was asked to hold his or her breath for as long as possible. Then each patient was asked to perform as many repetitive 12-second breath holds as possible. These data were correlated with demographic and historical information.
RESULTS: The maximum breath-hold time for inpatients and those outpatients who were heavy smokers or had chronic obstructive pulmonary disease (COPD) or congestive heart failure (CHF) was 18 to 32 seconds (95\% confidence interval) with a mean of 25 seconds. For all other outpatients, breath-hold time was 38 to 56 seconds (mean = 45 seconds). The 95\% confidence interval for the number of 12-second breath holds for these two groups was 4 to 6 breath holds (mean = 4.9) and 6 to 7 breath holds (mean = 6.6), respectively. One inpatient could not hold his breath at all and three others were only able to hold their breath once for short periods. The sex and age of the patient had no significant effect on breath-holding performance.
CONCLUSIONS: Breath-holding protocols must account for the diminished capabilities of most inpatients, and outpatients who are heavy smokers or have COPD or CHF. Most outpatients who are not heavy smokers or without COPD or CHF can achieve a single breath hold of 38 seconds, or up to six 12-second breath holds.},
  language = {eng},
  number = {9},
  journal = {Investigative radiology},
  author = {Gay, S B and Sistrom, C L and Holder, C A and Suratt, P M},
  month = sep,
  year = {1994},
  keywords = {Angiography; Digital Subtraction,Female,Heart Failure,Humans,Inpatients,Lung Diseases; Obstructive,Magnetic Resonance Imaging,Male,Middle Aged,Outpatients,Respiration,Smoking,Tomography; X-Ray Computed},
  pages = {848--851},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/CXKS4ICN/Gay et al. - 1994 - Breath-holding capability of adults. Implications .pdf},
  pmid = {7995705}
}

@article{moreau_fonctions_1962,
  title = {Fonctions Convexes Duales et Points Proximaux Dans Un Espace Hilbertien},
  volume = {255},
  journal = {Comptes Rendus de l'Acad{\'e}mie des Sciences (Paris)},
  author = {Moreau, J. J.},
  year = {1962},
  pages = {2897--2899},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/RNDFKTBZ/246208982_Fonctions_convexes_duales_et_points_proximaux_dans_un_espace_hilbertien.html}
}

@article{nett_tomosynthesis_2008,
  title = {Tomosynthesis via {{Total Variation Minimization Reconstruction}} and {{Prior Image Constrained Compressed Sensing}} ({{PICCS}}) on a {{C}}-Arm {{System}}},
  volume = {6913},
  issn = {1018-4732},
  doi = {10.1117/12.771294},
  abstract = {Recently, foundational mathematical theory, compressed sensing (CS), has been developed which enables accurate reconstruction from greatly undersampled frequency information (Candes et. al. and Donoho). Using numerical phantoms it has been demonstrated that CS reconstruction (e.g. minimizing the $\mathscr{l}$1 norm of the discrete gradient of the image) offers promise for computed tomography. However, when using experimental CT projection data the undersampling factors enabled were smaller than in numerical simulations. An extension to CS has recently been proposed wherein a prior image is utilized as a constraint in the image reconstruction procedure (i.e. Prior Image Constrained Compressed Sensing - PICCS). Experimental results are demonstrated here from a clinical C-arm system, highlighting one application of PICCS in reducing radiation exposure during interventional procedures while preserving high image quality. In this study a range of view angles has been investigated from very limited angle aquisitions (e.g. tomosythesis) to undersampled CT acquisitions.},
  journal = {Proceedings - Society of Photo-Optical Instrumentation Engineers},
  author = {Nett, Brian and Tang, Jie and Leng, Shuai and Chen, Guang-Hong},
  month = mar,
  year = {2008},
  pages = {nihpa92672},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/EAU5QRD4/Nett et al. - 2008 - Tomosynthesis via Total Variation Minimization Rec.pdf},
  pmid = {19756260}
}

@article{gao_4d_2012,
  title = {{{4D}} Cone Beam {{CT}} via Spatiotemporal Tensor Framelet},
  volume = {39},
  issn = {0094-2405},
  doi = {10.1118/1.4762288},
  abstract = {Purpose: On-board 4D cone beam CT (4DCBCT) offers respiratory phase-resolved volumetric imaging, and improves the accuracy of target localization in image guided radiation therapy. However, the clinical utility of this technique has been greatly impeded by its degraded image quality, prolonged imaging time, and increased imaging dose. The purpose of this letter is to develop a novel iterative 4DCBCT reconstruction method for improved image quality, increased imaging speed, and reduced imaging dose., Methods: The essence of this work is to introduce the spatiotemporal tensor framelet (STF), a high-dimensional tensor generalization of the 1D framelet for 4DCBCT, to effectively take into account of highly correlated and redundant features of the patient anatomy during respiration, in a multilevel fashion with multibasis sparsifying transform. The STF-based algorithm is implemented on a GPU platform for improved computational efficiency. To evaluate the method, 4DCBCT full-fan scans were acquired within 30 s, with a gantry rotation of 200$^\circ$; STF is also compared with a state-of-art reconstruction method via spatiotemporal total variation regularization., Results: Both the simulation and experimental results demonstrate that STF-based reconstruction achieved superior image quality. The reconstruction of 20 respiratory phases took less than 10 min on an NVIDIA Tesla C2070 GPU card. The STF codes are available at https://sites.google.com/site/spatiotemporaltensorframelet., Conclusions: By effectively utilizing the spatiotemporal coherence of the patient anatomy among different respiratory phases in a multilevel fashion with multibasis sparsifying transform, the proposed STF method potentially enables fast and low-dose 4DCBCT with improved image quality.},
  number = {11},
  journal = {Medical Physics},
  author = {Gao, Hao and Li, Ruijiang and Lin, Yuting and Xing, Lei},
  month = nov,
  year = {2012},
  pages = {6943--6946},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/VXJPINHR/Gao et al. - 2012 - 4D cone beam CT via spatiotemporal tensor framelet.pdf},
  pmid = {23127087},
  pmcid = {PMC3494730}
}

@article{grass_three-dimensional_1999,
  title = {Three-Dimensional Reconstruction of High Contrast Objects Using {{C}}-Arm Image Intensifier Projection Data},
  volume = {23},
  issn = {0895-6111},
  abstract = {The reconstruction of three-dimensional (3D) objects from 2D X-ray cone-beam projections using a circular source path is most commonly done with an algorithm according to Feldkamp et al. [Feldkamp LA, Davis LC, Kress JW. Practical cone-beam algorithms. J Opt Soc Am A 1984;6:612-619]. An adaptation of this so-called Feldkamp method to cone-beam projections acquired with a C-arm system is presented here. In a phantom study, reconstruction results obtained along real source-detector trajectories of a C-arm system are compared to reconstruction results obtained from projections acquired from a full-circular trajectory and from one consisting of two full orthogonal circles, which fulfills Tuy's sufficiency condition. The straightforward application of Feldkamp's method adapted to projection data obtained with a C-arm system illustrates the 3D imaging potential of image intensifier based cone-beam computed tomography. Reconstruction results from projection data of different patients acquired with a motorized C-arm system such as vessel structures filled with contrast agent and bones are presented.},
  number = {6},
  journal = {Computerized Medical Imaging and Graphics: The Official Journal of the Computerized Medical Imaging Society},
  author = {Grass, M and Koppe, R and Klotz, E and Proksa, R and Kuhn, M H and Aerts, H and {Op de Beek}, J and Kemkers, R},
  year = {1999 Nov-Dec},
  keywords = {Algorithms,Angiography,Cerebral Arteries,Cervical Vertebrae,Computer Simulation,Contrast Media,Elbow Joint,Humans,Image Processing; Computer-Assisted,Models; Anatomic,Phantoms; Imaging,Rotation,Tomography; X-Ray Computed},
  pages = {311--321},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/TQ96MDCB/Grass et al. - 1999 - Three-dimensional reconstruction of high contrast .pdf},
  pmid = {10634143}
}

@article{mory_removing_2014,
  title = {Removing Streak Artifacts from {{ECG}}-Gated Reconstructions Using Deconvolution},
  volume = {22},
  issn = {1095-9114},
  doi = {10.3233/XST-140423},
  abstract = {BACKGROUND: 4D cardiac computed tomography aims at reconstructing the beating heart from a series of 2D projections and the simultaneously acquired electrocardiogram. Each cardiac phase is reconstructed by exploiting the subset of projections acquired during this particular cardiac phase only. In these conditions, the Feldkamp, Davis and Kress method (FDK) generates large streak artifacts in the reconstructed volumes, hampering the medical interpretation. These artifacts can be substantially reduced by deconvolution methods.
OBJECTIVE: The aim of this paper is to compare two 4D cardiac CT reconstruction methods based on deconvolution, and to evaluate their practical benefits on two applications: cardiac micro CT and human cardiac C-arm CT.
METHODS: The first evaluated method builds upon inverse filtering. It has been proposed recently and demonstrated on 4D cardiac micro CT. The second one is an iterative deconvolution method, and turns out equivalent to an ECG-gated Iterative Filtered Back Projection (ECG-gated IFBP).
RESULTS: Results are presented on simulated data in 2D parallel beam, 2D fan beam and 3D cone beam geometries.
CONCLUSIONS: Both methods are efficient on the cardiac micro CT simulations, but insufficient to handle 4D human cardiac C-Arm CT simulations. Overall, ECG-gated IFPB largely outperforms the inverse filtering method.},
  language = {eng},
  number = {2},
  journal = {Journal of X-Ray Science and Technology},
  author = {Mory, Cyril and Auvray, Vincent and Zhang, Bo and Grass, Michael and Sch{\"a}fer, Dirk and Rit, Simon and Peyrin, Fran{\c c}oise and Douek, Philippe and Boussel, Lo{\"\i}c},
  year = {2014},
  pages = {253--270},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/8AP227QC/Mory et al. - 2014 - Removing streak artifacts from ECG-gated reconstru.pdf},
  pmid = {24699351}
}

@article{rohkohl_interventional_2010,
  title = {Interventional {{4D}} Motion Estimation and Reconstruction of Cardiac Vasculature without Motion Periodicity Assumption},
  volume = {14},
  issn = {1361-8415},
  doi = {16/j.media.2010.05.003},
  abstract = {Anatomical and functional information of cardiac vasculature is a key component in the field of interventional cardiology. With the technology of C-arm CT it is possible to reconstruct static intraprocedural 3D images from angiographic projection data. Current approaches attempt to add the temporal dimension (4D). In the assumption of periodic heart motion, ECG-gating techniques can be used. However, arrhythmic heart signals and slight breathing motion are degrading image quality frequently.
To overcome those problems, we present a reconstruction method based on a 4D time-continuous B-spline motion field. The temporal component of the motion field is parameterized by the acquisition time and does not assume a periodic heart motion. The analytic dynamic FDK-reconstruction formula is used directly for the motion estimation and image reconstruction.
In a physical phantom experiment two vessels of size 3.1~mm and 2.3~mm were reconstructed using the proposed method and an algorithm with periodicity assumption. For a periodic motion both methods obtained an error of 0.1~mm. For a non-periodic motion the proposed method was superior, obtaining an error of 0.3~mm/0.2~mm in comparison to 1.2~mm/1.0~mm for the algorithm with periodicity assumption. For a clinical test case of a left coronary artery it could be further shown that the method is capable to produce diameter measurements with an absolute error of 0.1~mm compared to state-of-the-art measurement tools from orthogonal coronary angiography. Further, it is shown for three different clinical cases (left/right coronary artery, coronary sinus) that the proposed method is able to handle a large variability of vascular structures and motion patterns. The complete algorithm is hardware-accelerated using the GPU requiring a computation time of less than 3~min for typical clinical scenarios.},
  number = {5},
  journal = {Medical Image Analysis},
  author = {Rohkohl, C. and Lauritsch, G. and Biller, L. and Pr{\"u}mmer, M. and Boese, J. and Hornegger, J.},
  month = oct,
  year = {2010},
  keywords = {3D Reconstruction,Cardiac C-arm CT,Coronary arteries,Motion compensation,Motion estimation},
  pages = {687--694},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/ANEHE3MV/Rohkohl et al. - 2010 - Interventional 4D motion estimation and reconstruc.pdf}
}

@article{boudoulas_linear_1981,
  title = {Linear Relationship between Electrical Systole, Mechanical Systole, and Heart Rate},
  volume = {80},
  issn = {0012-3692},
  abstract = {The relationship between the duration of electrical systole (QT) and heart rate (HR) and the relationship between the QT interval and total electromechanical systole (QS2) were studied in the resting state in 200 patients (100 males and 100 females) without evidence of cardiovascular disease. A linear relationship was found between the QT and HR in males and females (males, QT = 521 msec - 2.0 HR, r = .91; females, QT = 511 msec - 1.8 HR, r = .90). In 20 male and 20 female subjects, the relationship between QT and QS2 was studied. The QT was slightly shorter but paralleled the QS2 (males QT = 529 msec - 2.1 HR, QS2 = 541 msec - 2.2 HR; females QT = 511 msec - 1.9 HR, QS2 = 540 msec - 2.0 HR). Thus, over the physiologic range of resting HR, a linear relationship exists between QT and HR. The QT interval is slightly shorter but parallels the QS2 in patients without heart disease. These linear relationships permit a direct comparison of the duration of electrical and mechanical systole.},
  language = {eng},
  number = {5},
  journal = {Chest},
  author = {Boudoulas, H and Geleris, P and Lewis, R P and Rittgers, S E},
  month = nov,
  year = {1981},
  keywords = {Adult,Electrocardiography,Female,Heart Rate,Humans,Male,Middle Aged,Myocardial Contraction,Sex Factors,Systole},
  pages = {613--617},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/ZWDGXM8W/Boudoulas et al. - 1981 - Linear relationship between electrical systole, me.pdf},
  pmid = {7297154}
}

@article{wu_evaluation_2008,
  title = {Evaluation of Deformable Registration of Patient Lung {{4DCT}} with Subanatomical Region Segmentations},
  volume = {35},
  issn = {00942405},
  doi = {10.1118/1.2828378},
  language = {en},
  number = {2},
  journal = {Medical Physics},
  author = {Wu, Ziji and Rietzel, Eike and Boldea, Vlad and Sarrut, David and Sharp, Gregory C.},
  year = {2008},
  pages = {775},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/ZUW5GCGV/Wu et al. - 2008 - Evaluation of deformable registration of patient l.pdf}
}

@article{barber_mocca_2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1510.08842},
  primaryClass = {math},
  title = {{{MOCCA}}: Mirrored Convex/Concave Optimization for Nonconvex Composite Functions},
  shorttitle = {{{MOCCA}}},
  abstract = {Many optimization problems arising in high-dimensional statistics decompose naturally into a sum of several terms, where the individual terms are relatively simple but the composite objective function can only be optimized with iterative algorithms. In this paper, we are interested in optimization problems of the form F(Kx) + G(x), where K is a fixed linear transformation, while F and G are functions that may be nonconvex and/or nondifferentiable. In particular, if either of the terms are nonconvex, existing alternating minimization techniques may fail to converge; other types of existing approaches may instead be unable to handle nondifferentiability. We propose the MOCCA (mirrored convex/concave) algorithm, a primal/dual optimization approach that takes local convex approximation to each term at every iteration. Inspired by optimization problems arising in computed tomography (CT) imaging, this algorithm can handle a range of nonconvex composite optimization problems, and offers theoretical guarantees for convergence when the overall problem is approximately convex (that is, any concavity in one term is balanced out by convexity in the other term). Empirical results show fast convergence for several structured signal recovery problems.},
  journal = {arXiv:1510.08842 [math]},
  author = {Barber, Rina Foygel and Sidky, Emil Y.},
  month = oct,
  year = {2015},
  keywords = {Mathematics - Optimization and Control},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/J29HV6CD/Barber and Sidky - 2015 - MOCCA mirrored convexconcave optimization for no.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/3D5JDPBH/1510.html}
}

@article{van_cittert_zum_1931,
  title = {Zum {{Einfluss}} Der {{Spaltbreite}} Auf Die {{Intensit{\"a}tsverteilung}} in {{Spektrallinien}}. {{II}}},
  volume = {69},
  issn = {1434-6001, 1434-601X},
  doi = {10.1007/BF01391351},
  number = {5-6},
  journal = {Zeitschrift f{\"u}r Physik},
  author = {Van Cittert, P. H.},
  month = may,
  year = {1931},
  pages = {298--308},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/UU6RGJN8/1931ZPhy...69..html}
}

@article{andersen_generalized_2013,
  title = {Generalized Row-Action Methods for Tomographic Imaging},
  issn = {1017-1398, 1572-9265},
  doi = {10.1007/s11075-013-9778-8},
  language = {en},
  journal = {Numerical Algorithms},
  author = {Andersen, Martin S. and Hansen, Per Christian},
  month = nov,
  year = {2013},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/7CIARVW5/Andersen and Hansen - 2013 - Generalized row-action methods for tomographic ima.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/6346VSRQ/10.html}
}

@article{chen_novel_2005,
  title = {A Novel Extension of the Parallel-Beam Projection-Slice Theorem to Divergent Fan-Beam and Cone-Beam Projections},
  volume = {32},
  issn = {0094-2405},
  abstract = {The general goal of this paper is to extend the parallel-beam projection-slice theorem to divergent fan-beam and cone-beam projections without rebinning the divergent fan-beam and cone-beam projections into parallel-beam projections directly. The basic idea is to establish a novel link between the local Fourier transform of the projection data and the Fourier transform of the image object. Analogous to the two- and three-dimensional parallel-beam cases, the measured projection data are backprojected along the projection direction and then a local Fourier transform is taken for the backprojected data array. However, due to the loss of the shift invariance of the image object in a single view of the divergent-beam projections, the measured projection data is weighted by a distance dependent weight w(r) before the local Fourier transform is performed. The variable r in the weighting function w(r) is the distance from the backprojected point to the x-ray source position. It is shown that a special choice of the weighting function, w(r)=1/r, will facilitate the calculations and a simple relation can be established between the Fourier transform of the image function and the local Fourier transform of the 1/r-weighted backprojection data array. Unlike the parallel-beam cases, a one-to-one correspondence does not exist for a local Fourier transform of the backprojected data array and a single line in the two-dimensional (2D) case or a single slice in the 3D case of the Fourier transform of the image function. However, the Fourier space of the image object can be built up after the local Fourier transforms of the 1/r-weighted backprojection data arrays are shifted and then summed in a laboratory frame. Thus the established relations Eq. (27) and Eq. (29) between the Fourier space of the image object and the Fourier transforms of the backprojected data arrays can be viewed as a generalized projection-slice theorem for divergent fan-beam and cone-beam projections. Once the Fourier space of the image function is built up, an inverse Fourier transform could be performed to reconstruct tomographic images from the divergent beam projections. Due to the linearity of the Fourier transform, an image reconstruction step can be performed either when the complete Fourier space is available or in parallel with the building of the Fourier space. Numerical simulations are performed to verify the generalized projection-slice theorem by using a disc phantom in the fan-beam case.},
  number = {3},
  journal = {Medical physics},
  author = {Chen, Guang-Hong and Leng, Shuai and Mistretta, Charles A},
  month = mar,
  year = {2005},
  keywords = {Algorithms,Brain,Humans,Imaging; Three-Dimensional,Information Storage and Retrieval,Phantoms; Imaging,Radiographic Image Enhancement,Radiographic Image Interpretation; Computer-Assisted,Reproducibility of Results,Sensitivity and Specificity,Tomography; X-Ray Computed},
  pages = {654--665},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/4PQU6PC2/Chen et al. - 2005 - A novel extension of the parallel-beam projection-.pdf},
  pmid = {15839337}
}

@article{needell_paved_2014,
  series = {Special Issue on Sparse Approximate Solution of Linear Systems},
  title = {Paved with Good Intentions: {{Analysis}} of a Randomized Block {{Kaczmarz}} Method},
  volume = {441},
  issn = {0024-3795},
  shorttitle = {Paved with Good Intentions},
  doi = {10.1016/j.laa.2012.12.022},
  abstract = {The block Kaczmarz method is an iterative scheme for solving overdetermined least-squares problems. At each step, the algorithm projects the current iterate onto the solution space of a subset of the constraints. This paper describes a block Kaczmarz algorithm that uses a randomized control scheme to choose the subset at each step. This algorithm is the first block Kaczmarz method with an (expected) linear rate of convergence that can be expressed in terms of the geometric properties of the matrix and its submatrices. The analysis reveals that the algorithm is most effective when it is given a good row paving of the matrix, a partition of the rows into well-conditioned blocks. The operator theory literature provides detailed information about the existence and construction of good row pavings. Together, these results yield an efficient block Kaczmarz scheme that applies to many overdetermined least-squares problem.},
  journal = {Linear Algebra and its Applications},
  author = {Needell, Deanna and Tropp, Joel A.},
  month = jan,
  year = {2014},
  keywords = {Algebraic reconstruction technique,Block Kaczmarz,Matrix paving,Projections onto convex sets},
  pages = {199--221},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/DIWAQ9NV/Needell and Tropp - 2014 - Paved with good intentions Analysis of a randomiz.pdf}
}

@article{long_multi-material_2014,
  title = {Multi-{{Material Decomposition Using Statistical Image Reconstruction}} for {{Spectral CT}}},
  volume = {33},
  issn = {0278-0062, 1558-254X},
  doi = {10.1109/TMI.2014.2320284},
  number = {8},
  journal = {IEEE Transactions on Medical Imaging},
  author = {Long, Yong and Fessler, Jeffrey A.},
  month = aug,
  year = {2014},
  pages = {1614--1626},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/T7S3XCEI/Long and Fessler - 2014 - Multi-Material Decomposition Using Statistical Ima.pdf}
}

@inproceedings{rit_reconstruction_2013,
  title = {The {{Reconstruction Toolkit}} ({{RTK}}), an Open-Source Cone-Beam {{CT}} Reconstruction Toolkit Based on the {{Insight Toolkit}} ({{ITK}})},
  booktitle = {Proceedings of the {{International Conference}} on the {{Use}} of {{Computers}} in {{Radiation Therapy}} ({{ICCR}})},
  author = {Rit, Simon and Vila Oliva, Marc and Brousmiche, S{\'e}bastien and Labarbe, Rudi and Sarrut, David and Sharp, Gregory C.},
  year = {2013},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/2AZVH65E/793.html}
}

@article{chung_duration_2004,
  title = {Duration of Diastole and Its Phases as a Function of Heart Rate during Supine Bicycle Exercise},
  volume = {287},
  copyright = {Copyright \textcopyright{} 2004 by the American Physiological Society},
  doi = {10.1152/ajpheart.00404.2004},
  abstract = {The duration of diastole can be defined in terms of mechanical events. Mechanical diastolic duration (MDD) is comprised by the phases of early rapid filling (E wave), diastasis, and late atrial filling (A wave). The effect of heart rate (HR) on diastolic duration is predictable from kinematic modeling and known cellular physiology. To determine the dependence of MDD of each phase and the velocity time integral (VTI) on HR, simultaneous transmitral Doppler flow velocities and ECG were recorded during supine bicycle exercise in healthy volunteers. Durations, peak values, and VTI using triangular approximation for E- and A-wave shape were measured. MDD, defined as the interval from the start of the E wave to end of the A wave, was fit as an algebraic function of HR by MDD = BMDD + MLMDD$\cdot$HR + MIMDD/HR, derivable from first principles, where BMDD is a constant, and MLMDD and MIMDD are the constant coefficients of the linear and inverse HR dependent terms. Excellent correlation was observed (r2 = 0.98). E- and A-wave durations were found to be very nearly independent of HR: 100\% increase in HR generated only an 18\% decrease in E-wave duration and 16\% decrease in A-wave duration. VTI was similarly very nearly independent of HR. Diastasis duration closely tracked MDD as a function of HR. We conclude that the elimination of diastasis and merging of E and A waves of nearly fixed durations primarily govern changes in MDD. These observations support the perspective that E- and A-wave durations are primarily governed by the rules of mechanical oscillation that are minimally HR dependent.},
  language = {en},
  number = {5},
  journal = {American Journal of Physiology - Heart and Circulatory Physiology},
  author = {Chung, Charles S. and Karamanoglu, Mustafa and Kov{\'a}cs, S{\'a}ndor J.},
  month = jan,
  year = {2004},
  pages = {H2003--H2008},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/AE86K5NV/Chung et al. - 2004 - Duration of diastole and its phases as a function .pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/73VIAJCW/H2003.html},
  pmid = {15217800},
  note = {The duration of diastole can be defined in terms of mechanical events. Mechanical diastolic duration (MDD) is comprised by the phases of early rapid filling (E wave), diastasis, and late atrial filling (A wave). The effect of heart rate (HR) on diastolic duration is predictable from kinematic modeling and known cellular physiology. To determine the dependence of MDD of each phase and the velocity time integral (VTI) on HR, simultaneous transmitral Doppler flow velocities and ECG were recorded during supine bicycle exercise in healthy volunteers. Durations, peak values, and VTI using triangular approximation for E- and A-wave shape were measured. MDD, defined as the interval from the start of the E wave to end of the A wave, was fit as an algebraic function of HR by MDD = B MDD + M LMDD$\cdot$HR + M IMDD/HR, derivable from first principles, where B MDD is a constant, and M LMDD and M IMDD are the constant coefficients of the linear and inverse HR dependent terms. Excellent correlation was observed ( r 2 = 0.98). E- and A-wave durations were found to be very nearly independent of HR: 100\% increase in HR generated only an 18\% decrease in E-wave duration and 16\% decrease in A-wave duration. VTI was similarly very nearly independent of HR. Diastasis duration closely tracked MDD as a function of HR. We conclude that the elimination of diastasis and merging of E and A waves of nearly fixed durations primarily govern changes in MDD. These observations support the perspective that E- and A-wave durations are primarily governed by the rules of mechanical oscillation that are minimally HR dependent.}
}

@incollection{schafer_limited_2012,
  title = {Limited Angle {{C}}-Arm Tomography and Segmentation for Guidance of Atrial Fibrillation Ablation Procedures},
  booktitle = {Medical {{Image Computing}} and {{Computer}}-{{Assisted Intervention}}\textendash{}{{MICCAI}} 2012},
  publisher = {{Springer}},
  author = {Sch{\"a}fer, Dirk and Meyer, Carsten and Bullens, Roland and Saalbach, Axel and Eshuis, Peter},
  year = {2012},
  pages = {634--641},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/PQRRRR45/SchÃ¤fer et al. - 2012 - Limited angle C-arm tomography and segmentation fo.pdf}
}

@inproceedings{yamakawa_two-step_2014,
  title = {Two-Step Iterative Reconstruction of Region-of-Interest with Truncated Projection in Computed Tomography},
  volume = {9033},
  doi = {10.1117/12.2042228},
  abstract = {Iteratively reconstructing data only inside the region of interest (ROI) is widely used to acquire CT images in less
computation time while maintaining high spatial resolution. A method that subtracts projected data outside the ROI from
full-coverage measured data has been proposed. A serious problem with this method is that the accuracy of the measured
data confined inside the ROI decreases according to the truncation error outside the ROI. We propose a two-step iterative
method that reconstructs image inside the full-coverage in addition to a conventional iterative method inside the ROI to
reduce the truncation error inside full-coverage images. Statistical information (e.g., quantum-noise distributions)
acquired by detected X-ray photons is generally used in iterative methods as a photon weight to efficiently reduce image
noise. Our proposed method applies one of two kinds of weights (photon or constant weights) chosen adaptively by
taking into consideration the influence of truncation error. The effectiveness of the proposed method compared with that
of the conventional method was evaluated in terms of simulated CT values by using elliptical phantoms and an abdomen
phantom. The standard deviation of error and the average absolute error of the proposed method on the profile curve
were respectively reduced from 3.4 to 0.4 [HU] and from 2.8 to 0.8 [HU] compared with that of the conventional method.
As a result, applying a suitable weight on the basis of a target object made it possible to effectively reduce the errors in
CT images.},
  author = {Yamakawa, Keisuke and Kojima, Shinichi},
  year = {2014},
  pages = {90333B--90333B--7}
}

@article{duan_few-view_2009,
  title = {Few-{{View Projection Reconstruction With}} an {{Iterative Reconstruction}}-{{Reprojection Algorithm}} and {{TV Constraint}}},
  volume = {56},
  issn = {0018-9499},
  doi = {10.1109/TNS.2008.2009990},
  abstract = {In applications of tomographic imaging, insufficient data problems can take various forms, such as few-view projection imaging which enables rapid scanning with lower X-ray dose. In this work, an iterative reconstruction-reprojection (IRR) algorithm with total variation (TV) constraint is developed for few-view projections. The IRR algorithm is used to estimate the missing projection data by iterative extrapolation between projection and image space. TV minimization is a popular image restoration method with edge preserving. In recent studies, it has been successfully used for reconstructing images from sparse samplings, such as few-view projections. Our method is derived from this work. The combination of IRR and TV achieves both estimation in projection space and regularization in image space, which accelerates the convergence of the iterations. To improve the quality of the image reconstructed from few-view fan-beam projections, a short-scan type IRR is also approached to reduce the redundancy of projection data. An improved weighting function is proposed for few-view short-scan projection reconstruction by the filtered backprojection (FBP) algorithm. Numerical simulations show that the IRR-TV algorithm is effective for the few-view problem of reconstructing sparse-gradient images.},
  number = {3},
  journal = {Nuclear Science, IEEE Transactions on},
  author = {Duan, Xinhui and Zhang, Li and Xing, Yuxiang and Chen, Zhiqiang and Cheng, Jianping},
  month = jun,
  year = {2009},
  keywords = {Image reconstruction,TV constraint,X-ray dose,backpropagation,computed tomography imaging,computerised tomography,diagnostic radiography,dosimetry,extrapolation,few-view projection reconstruction,filtered backprojection algorithm,image restoration method,image sampling,image spacing,iterative extrapolation,iterative methods,iterative reconstruction-reprojection algorithm,medical image processing,sparse sampling,total variation minimisation},
  pages = {1377 --1382},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/ZJCKB2TP/articleDetails.html}
}

@inproceedings{clackdoyle_accurate_2016,
  title = {Accurate {{Transaxial Region}}-of-{{Interest Reconstruction}} in {{Helical CT}}?},
  abstract = {Helical CT scanners with tens or hundreds of detector rows have been the standard for about 10 years. Image reconstruction theory has been well studied for this geometry: the main theoretical result is that as long as the helix pitch does not exceed the axial extent of the detectors, then mathematically exact image reconstruction is possible in principle. Region-of-Interest (ROI) reconstruction refers to techniques for reconstructing a sub-volume from a reduced dataset. Our motivation in looking at this problem is the potential for dramatically reduced radiation dose that should be achievable. For ``reduced dose'' to be meaningful, the reconstruction quality inside the ROI should be comparable to the situation for full-field scanning. In this work we examine the theoretical issues behind ROI reconstruction in helical CT. We do not attempt to quantify the dose reduction here. In the theory section below, we discuss helical ROI reconstruction in terms of current image reconstruction theory. Although transverse truncation and ROI reconstruction in helical CT has been studied previously, we provide a detailed analysis of two specific ROIs. In the following section we provide example reconstructions performed with iterative methods because analytic algorithms have not yet been developed for the theoretically reconstructible cases under consideration.},
  language = {en},
  author = {Clackdoyle, Rolf and Momey, Fabien and Desbat, Laurent and Rit, Simon},
  month = oct,
  year = {2016},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/HCGW2XPZ/hal-01385635v1.html}
}

@inproceedings{maass_empirical_2011,
  title = {Empirical Multiple Energy Calibration ({{EMEC}}) for Material-Selective {{CT}}},
  isbn = {978-1-4673-0120-6 978-1-4673-0118-3 978-1-4673-0119-0},
  doi = {10.1109/NSSMIC.2011.6153810},
  publisher = {{IEEE}},
  author = {Maass, Nicole and Sawall, Stefan and Knaup, Michael and Kachelriess, Marc},
  month = oct,
  year = {2011},
  pages = {4222--4229}
}

@article{schulze_artefacts_2011,
  title = {Artefacts in {{CBCT}}: A Review},
  volume = {40},
  issn = {0250-832X},
  shorttitle = {Artefacts in {{CBCT}}},
  doi = {10.1259/dmfr/30642039},
  abstract = {Artefacts are common in today's cone beam CT (CBCT). They are induced by discrepancies between the mathematical modelling and the actual physical imaging process. Since artefacts may interfere with the diagnostic process performed on CBCT data sets, every user should be aware of their presence. This article aims to discuss the most prominent artefacts identified in the scientific literature and review the existing knowledge on these artefacts. We also briefly review the basic three-dimensional (3D) reconstruction concept applied by today's CBCT scanners, as all artefacts are more or less directly related to it.},
  number = {5},
  journal = {Dentomaxillofacial Radiology},
  author = {Schulze, R and Heil, U and Gro$\beta$, D and Bruellmann, DD and Dranischnikow, E and Schwanecke, U and Schoemer, E},
  month = jul,
  year = {2011},
  pages = {265--273},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/GHU6ZKSG/Schulze et al. - 2011 - Artefacts in CBCT a review.pdf},
  pmid = {21697151},
  pmcid = {PMC3520262}
}

@article{sunnegardh_regularized_2008,
  title = {Regularized Iterative Weighted Filtered Backprojection for Helical Cone-Beam {{CT}}},
  volume = {35},
  copyright = {\textcopyright{} 2008 American Association of Physicists in Medicine},
  doi = {10.1118/1.2966353},
  abstract = {Contemporary reconstruction methods employed for clinical helical cone-beam computed tomography (CT) are analytical (noniterative) but mathematically nonexact, i.e., the reconstructed image contains so called cone-beam artifacts, especially for higher cone angles. Besides cone artifacts, these methods also suffer from windmill artifacts: alternating dark and bright regions creating spiral-like patterns occurring in the vicinity of high z-direction derivatives. In this article, the authors examine the possibility to suppress cone and windmill artifacts by means of iterative application of nonexact three-dimensional filtered backprojection, where the analytical part of the reconstruction brings about accelerated convergence. Specifically, they base their investigations on the weighted filtered backprojection method [Stierstorfer et al., Phys. Med. Biol. 49, 2209\textendash{}2218 (2004)]. Enhancement of high frequencies and amplification of noise is a common but unwanted side effect in many acceleration attempts. They have employed linear regularization to avoid these effects and to improve the convergence properties of the iterative scheme. Artifacts and noise, as well as spatial resolution in terms of modulation transfer functions and slice sensitivity profiles have been measured. The results show that for cone angles up to $\pm$2.78$^\circ$, cone artifacts are suppressed and windmill artifacts are alleviated within three iterations. Furthermore, regularization parameters controlling spatial resolution can be tuned so that image quality in terms of spatial resolution and noise is preserved. Simulations with higher number of iterations and long objects (exceeding the measured region) verify that the size of the reconstructible region is not reduced, and that the regularization greatly improves the convergence properties of the iterative scheme. Taking these results into account, and the possibilities to extend the proposed method with more accurate modeling of the acquisition process, the authors believe that iterative improvement with non-exact methods is a promising technique for medical CT applications.},
  number = {9},
  journal = {Medical Physics},
  author = {Sunnegardh, Johan and Danielsson, Per-Erik},
  year = {2008},
  keywords = {Image reconstruction,computerised tomography,convergence of numerical methods,image denoising,iterative methods,medical image processing},
  pages = {4173--4185},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/AS9TMDZR/Sunnegardh and Danielsson - 2008 - Regularized iterative weighted filtered backprojec.PDF}
}

@article{joseph_improved_1982,
  title = {An {{Improved Algorithm}} for {{Reprojecting Rays}} through {{Pixel Images}}},
  issn = {0278-0062},
  doi = {10.1109/TMI.1982.4307572},
  number = {3},
  journal = {Medical Imaging, IEEE Transactions on},
  author = {Joseph, Peter M.},
  year = {1982},
  pages = {192 -- 196},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/ZBQKIXCA/Joseph - 1982 - An Improved Algorithm for Reprojecting Rays throug.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/HFK3C8VP/3222245_An_Improved_Algorithm_for_Reprojecting_Rays_through_Pixel_Images.html}
}

@article{yu_compressed_2009,
  title = {Compressed Sensing Based Interior Tomography},
  volume = {54},
  issn = {0031-9155, 1361-6560},
  doi = {10.1088/0031-9155/54/9/014},
  number = {9},
  journal = {Physics in Medicine and Biology},
  author = {Yu, Hengyong and Wang, Ge},
  month = may,
  year = {2009},
  pages = {2791--2805},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/IDRFW7AI/pmb9_9_014.pdf}
}

@article{rit_comparison_2009,
  title = {Comparison of {{Analytic}} and {{Algebraic Methods}} for {{Motion}}-{{Compensated Cone}}-{{Beam CT Reconstruction}} of the {{Thorax}}},
  volume = {28},
  issn = {0278-0062, 1558-254X},
  doi = {10.1109/TMI.2008.2008962},
  number = {10},
  journal = {IEEE Transactions on Medical Imaging},
  author = {Rit, Simon and Sarrut, D. and Desbat, L.},
  month = oct,
  year = {2009},
  pages = {1513--1525},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/CVZC3BJB/Rit et al. - 2009 - Comparison of Analytic and Algebraic Methods for M.pdf}
}

@article{rangarajan_image_2002,
  title = {Image Denoising Using Wavelets},
  journal = {Wavelet and Time Frequencies},
  author = {Rangarajan, Raghuram and Venkataramanan, Ramji and Shah, Siddharth},
  year = {2002},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/P82MIHXX/Report.pdf}
}

@article{sawatzky_proximal_2014,
  title = {Proximal {{ADMM}} for {{Multi}}-{{Channel Image Reconstruction}} in {{Spectral X}}-Ray {{CT}}},
  volume = {33},
  issn = {0278-0062, 1558-254X},
  doi = {10.1109/TMI.2014.2321098},
  number = {8},
  journal = {IEEE Transactions on Medical Imaging},
  author = {Sawatzky, Alex and Xu, Qiaofeng and Schirra, Carsten O. and Anastasio, Mark A.},
  month = aug,
  year = {2014},
  pages = {1657--1668},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/MXK29QJA/sawatzky2014.pdf}
}

@article{hsieh_step-and-shoot_2006,
  title = {Step-and-Shoot Data Acquisition and Reconstruction for Cardiac x-Ray Computed Tomography},
  volume = {33},
  issn = {0094-2405},
  abstract = {Coronary artery imaging with x-ray computed tomography (CT) is one of the most recent advancements in CT clinical applications. Although existing "state-of-the-art" clinical protocols today utilize helical data acquisition, it suffers from the lack of ability to handle irregular heart rate and relatively high x-ray dose to patients. In this paper, we propose a step-and-shoot data acquisition protocol that significantly overcomes these shortcomings. The key to the proposed protocol is the large volume coverage (40 mm) enabled by the cone beam CT scanner, which allows the coverage of the entire heart in 3 to 4 steps. In addition, we propose a gated complementary reconstruction algorithm that overcomes the longitudinal truncation problem resulting from the cone beam geometry. Computer simulations, phantom experiments, and clinical studies were conducted to validate our approach.},
  language = {eng},
  number = {11},
  journal = {Medical physics},
  author = {Hsieh, Jiang and Londt, John and Vass, Melissa and Li, Jay and Tang, Xiangyang and Okerlund, Darin},
  month = nov,
  year = {2006},
  keywords = {Algorithms,Computer Simulation,Coronary Angiography,Humans,Information Storage and Retrieval,Models; Cardiovascular,Phantoms; Imaging,Radiographic Image Enhancement,Radiographic Image Interpretation; Computer-Assisted,Reproducibility of Results,Sensitivity and Specificity,Tomography; X-Ray Computed},
  pages = {4236--4248},
  pmid = {17153402}
}

@article{mory_4d_2015,
  title = {{{4D}} Cone-Beam Computed Tomography Combining Total Variation Regularization and Motion Compensation},
  volume = {115},
  issn = {0167-8140},
  shorttitle = {{{PD}}-0143},
  doi = {10.1016/S0167-8140(15)40141-0},
  language = {English},
  journal = {Radiotherapy and Oncology},
  author = {Mory, C. and Rit, S.},
  month = apr,
  year = {2015},
  pages = {S68--S69},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/EK7UJAGG/Mory and Rit - 2015 - 4D cone-beam computed tomography combining total v.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/Z8I2RZ3Z/abstract.html}
}

@article{bergner_investigation_2010,
  title = {An Investigation of {{4D}} Cone-Beam {{CT}} Algorithms for Slowly Rotating Scanners},
  volume = {37},
  issn = {00942405},
  doi = {10.1118/1.3480986},
  number = {9},
  journal = {Medical Physics},
  author = {Bergner, Frank and Berkus, Timo and Oelhafen, Markus and Kunz, Patrik and Pan, Tinsu and Grimmer, Rainer and Ritschl, Ludwig and Kachelrie{\ss}, Marc},
  year = {2010},
  pages = {5044},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/GSXIIQEA/Bergner et al. - 2010 - An investigation of 4D cone-beam CT algorithms for.pdf}
}

@article{go_heart_2013,
  title = {Heart {{Disease}} and {{Stroke Statistics}}\textemdash{}2013 {{Update A Report From}} the {{American Heart Association}}},
  volume = {127},
  issn = {0009-7322, 1524-4539},
  doi = {10.1161/CIR.0b013e31828124ad},
  language = {en},
  number = {1},
  journal = {Circulation},
  author = {Go, Alan S. and Mozaffarian, Dariush and Roger, V{\'e}ronique L. and Benjamin, Emelia J. and Berry, Jarett D. and Borden, William B. and Bravata, Dawn M. and Dai, Shifan and Ford, Earl S. and Fox, Caroline S. and Franco, Sheila and Fullerton, Heather J. and Gillespie, Cathleen and Hailpern, Susan M. and Heit, John A. and Howard, Virginia J. and Huffman, Mark D. and Kissela, Brett M. and Kittner, Steven J. and Lackland, Daniel T. and Lichtman, Judith H. and Lisabeth, Lynda D. and Magid, David and Marcus, Gregory M. and Marelli, Ariane and Matchar, David B. and McGuire, Darren K. and Mohler, Emile R. and Moy, Claudia S. and Mussolino, Michael E. and Nichol, Graham and Paynter, Nina P. and Schreiner, Pamela J. and Sorlie, Paul D. and Stein, Joel and Turan, Tanya N. and Virani, Salim S. and Wong, Nathan D. and Woo, Daniel and Turner, Melanie B.},
  month = jan,
  year = {2013},
  pages = {e6--e245},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/ANPD2EQ2/Go et al. - 2013 - Heart Disease and Stroke Statisticsâ€”2013 Update A .pdf}
}

@article{langet_compressed_2012,
  title = {Compressed Sensing Dynamic Reconstruction in Rotational Angiography},
  volume = {15},
  abstract = {This work tackles three-dimensional reconstruction of tomographic acquisitions in C-arm-based rotational angiography. The relatively slow rotation speed of C-arm systems involves motion artifacts that limit the use of three-dimensional imaging in interventional procedures. The main contribution of this paper is a reconstruction algorithm that deals with the temporal variations due to intra-arterial injections. Based on a compressed-sensing approach, we propose a multiple phase reconstruction with spatio-temporal constraints. The algorithm was evaluated by qualitative and quantitative assessment of image quality on both numerical phantom experiments and clinical data from vascular C-arm systems. In this latter case, motion artifacts reduction was obtained in spite of the cone-beam geometry, the short-scan acquisition, and the truncated and subsampled data.},
  number = {Pt 1},
  journal = {Medical image computing and computer-assisted intervention: MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention},
  author = {Langet, H{\'e}l{\`e}{\~n}e and Riddell, Cyril and Trousset, Yves and Tenenhaus, Arthur and Lahalle, Elisabeth and Fleury, Gilles and Paragios, Nikos},
  year = {2012},
  keywords = {Algorithms,Angiography,Artifacts,Computer Simulation,Diagnostic Imaging,Humans,Image Processing; Computer-Assisted,Imaging; Three-Dimensional,Models; Statistical,Motion,Phantoms; Imaging,Rotation,Software,Time Factors},
  pages = {223--230},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/5E7S7FMM/Langet et al. - 2012 - Compressed sensing dynamic reconstruction in rotat.pdf},
  pmid = {23285555}
}

@article{wang_incremental_2014,
  title = {Incremental Constraint Projection Methods for Variational Inequalities},
  volume = {150},
  issn = {0025-5610},
  doi = {10.1007/s10107-014-0769-x},
  number = {2},
  journal = {Mathematical Programming},
  author = {Wang, Mengdi and Bertsekas, Dimitri},
  year = {2014},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/V743EJQM/Wang and Bertsekas - 2014 - Incremental constraint projection methods for vari.pdf}
}

@article{blondel_reconstruction_2006,
  title = {Reconstruction of Coronary Arteries from a Single Rotational {{X}}-Ray Projection Sequence},
  volume = {25},
  issn = {0278-0062},
  doi = {10.1109/TMI.2006.873224},
  abstract = {Cardiovascular diseases remain the primary cause of death in developed countries. In most cases, exploration of possibly underlying coronary artery pathologies is performed using X-ray coronary angiography. Current clinical routine in coronary angiography is directly conducted in two-dimensional projection images from several static viewing angles. However, for diagnosis and treatment purposes, coronary artery reconstruction is highly suitable. The purpose of this study is to provide physicians with a three-dimensional (3-D) model of coronary arteries, e.g., for absolute 3-D measures for lesion assessment, instead of direct projective measures deduced from the images, which are highly dependent on the viewing angle. In this paper, we propose a novel method to reconstruct coronary arteries from one single rotational X-ray projection sequence. As a side result, we also obtain an estimation of the coronary artery motion. Our method consists of three main consecutive steps: 1) 3-D reconstruction of coronary artery centerlines, including respiratory motion compensation; 2) coronary artery four-dimensional motion computation; 3) 3-D tomographic reconstruction of coronary arteries, involving compensation for respiratory and cardiac motions. We present some experiments on clinical datasets, and the feasibility of a true 3-D Quantitative Coronary Analysis is demonstrated.},
  number = {5},
  journal = {IEEE Transactions on Medical Imaging},
  author = {Blondel, Christophe and Malandain, Gr{\'e}goire and Vaillant, R{\'e}gis and Ayache, Nicholas},
  month = may,
  year = {2006},
  keywords = {Algorithms,Artificial Intelligence,Coronary Angiography,Humans,Imaging; Three-Dimensional,Information Storage and Retrieval,Pattern Recognition; Automated,Radiographic Image Enhancement,Radiographic Image Interpretation; Computer-Assisted,Reproducibility of Results,Sensitivity and Specificity,Subtraction Technique},
  pages = {653--663},
  pmid = {16689269}
}

@article{bauschke_compositions_2012,
  title = {Compositions and Convex Combinations of Asymptotically Regular Firmly Nonexpansive Mappings Are Also Asymptotically Regular},
  volume = {2012},
  number = {1},
  journal = {Fixed Point Theory and Applications},
  author = {Bauschke, Heinz H. and Mart{\'\i}n-M{\'a}rquez, Victoria and Moffat, Sarah M. and Wang, Xianfu},
  year = {2012},
  pages = {1--11},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/QTVX7SEE/Bauschke et al. - 2012 - Compositions and convex combinations of asymptotic.pdf}
}

@article{snyder_image_2006,
  title = {Image Reconstruction for Transmission Tomography When Projection Data   Are Incomplete},
  volume = {51},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/51/21/015},
  abstract = {Two iterative methods are developed for forming a maximum-likelihood   estimate of the attenuation density in a patient or object for   transmission tomography when projection data are incomplete. The methods   converge monotonically to the same limit points. Results of testing the   methods with both simulated and real data are given.},
  language = {English},
  number = {21},
  journal = {Physics in Medicine and Biology},
  author = {Snyder, Donald L. and O'Sullivan, Joseph A. and Murphy, Ryan J. and Politte, David G. and Whiting, Bruce R. and Williamson, Jeffrey F.},
  month = nov,
  year = {2006},
  keywords = {Artifacts,algorithm,cone-beam ct,limited data,local tomography,position,ray computed-tomography,reduction,scan,truncation},
  pages = {5603--5619},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/CWFHVIVV/full_record.html},
  note = {WOS:000242528800015}
}

@article{noo_two-step_2004,
  title = {A Two-Step {{Hilbert}} Transform Method for {{2D}} Image Reconstruction},
  volume = {49},
  issn = {0031-9155},
  abstract = {The paper describes a new accurate two-dimensional (2D) image reconstruction method consisting of two steps. In the first step, the backprojected image is formed after taking the derivative of the parallel projection data. In the second step, a Hilbert filtering is applied along certain lines in the differentiated backprojection (DBP) image. Formulae for performing the DBP step in fanbeam geometry are also presented. The advantage of this two-step Hilbert transform approach is that in certain situations, regions of interest (ROIs) can be reconstructed from truncated projection data. Simulation results are presented that illustrate very similar reconstructed image quality using the new method compared to standard filtered backprojection, and that show the capability to correctly handle truncated projections. In particular, a simulation is presented of a wide patient whose projections are truncated laterally yet for which highly accurate ROI reconstruction is obtained.},
  language = {eng},
  number = {17},
  journal = {Physics in Medicine and Biology},
  author = {Noo, Fr{\'e}d{\'e}ric and Clackdoyle, Rolf and Pack, Jed D.},
  month = sep,
  year = {2004},
  keywords = {Algorithms,Humans,Image Processing; Computer-Assisted,Models; Statistical,Models; Theoretical,Phantoms; Imaging,Radiographic Image Enhancement,Radiographic Image Interpretation; Computer-Assisted,Tomography; X-Ray Computed},
  pages = {3903--3923},
  pmid = {15470913}
}

@article{hu_higher_2012,
  title = {Higher {{Degree Total Variation}} ({{HDTV}}) {{Regularization}} for {{Image Recovery}}},
  volume = {21},
  issn = {1057-7149},
  doi = {10.1109/TIP.2012.2183143},
  abstract = {We introduce novel image regularization penalties to overcome the practical problems associated with the classical total variation (TV) scheme. Motivated by novel reinterpretations of the classical TV regularizer, we derive two families of functionals involving higher degree partial image derivatives; we term these families as isotropic and anisotropic higher degree TV (HDTV) penalties, respectively. The isotropic penalty is the mixed norm of the directional image derivatives, while the anisotropic penalty is the separable norm of directional derivatives. These functionals inherit the desirable properties of standard TV schemes such as invariance to rotations and translations, preservation of discontinuities, and convexity. The use of mixed norms in isotropic penalties encourages the joint sparsity of the directional derivatives at each pixel, thus encouraging isotropic smoothing. In contrast, the fully separable norm in the anisotropic penalty ensures the preservation of discontinuities, while continuing to smooth along the line like features; this scheme thus enhances the linenlike image characteristics analogous to standard TV. We also introduce efficient majorize-minimize algorithms to solve the resulting optimization problems. The numerical comparison of the proposed scheme with classical TV penalty, current second-degree methods, and wavelet algorithms clearly demonstrate the performance improvement. Specifically, the proposed algorithms minimize the staircase and ringing artifacts that are common with TV and wavelet schemes, while better preserving the singularities. We also observe that anisotropic HDTV penalty provides consistently improved reconstructions compared with the isotropic HDTV penalty.},
  number = {5},
  journal = {IEEE Transactions on Image Processing},
  author = {Hu, Yue and Jacob, M.},
  year = {2012},
  keywords = {Algorithms,Artifacts,Closed-form solutions,HDTV,Higher degree total variation (HDTV),Image Enhancement,Image Interpretation; Computer-Assisted,Image edge detection,Image reconstruction,Joints,Reproducibility of Results,Sensitivity and Specificity,Smoothing methods,Vectors,anisotropic higher degree TV penalty,classical TV regularizer,classical total variation scheme,high degree partial image derivatives,higher degree total variation regularization,image recovery,image regularization penalty,isotropic HDTV penalty,isotropic smoothing,majorize minimize (MM),majorize-minimize algorithms,optimisation,optimization problems,second-degree methods,wavelet algorithms,wavelet transforms},
  pages = {2559--2571},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/6HEAUQSD/Hu and Jacob - 2012 - Higher Degree Total Variation (HDTV) Regularizatio.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/JWIHBR7P/login.html}
}

@article{barber_algorithm_2016,
  title = {An Algorithm for Constrained One-Step Inversion of Spectral {{CT}} Data},
  volume = {61},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/61/10/3784},
  abstract = {We develop a primal-dual algorithm that allows for one-step inversion of spectral CT transmission photon counts data to a basis map decomposition. The algorithm allows for image constraints to be enforced on the basis maps during the inversion. The derivation of the algorithm makes use of a local upper bounding quadratic approximation to generate descent steps for non-convex spectral CT data discrepancy terms, combined with a new convex-concave optimization algorithm. Convergence of the algorithm is demonstrated on simulated spectral CT data. Simulations with noise and anthropomorphic phantoms show examples of how to employ the constrained one-step algorithm for spectral CT data.},
  language = {en},
  number = {10},
  journal = {Physics in Medicine and Biology},
  author = {Barber, Rina Foygel and Sidky, Emil Y. and Schmidt, Taly Gilat and Pan, Xiaochuan},
  year = {2016},
  pages = {3784},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/5C68VU73/Barber et al. - 2016 - An algorithm for constrained one-step inversion of.pdf}
}

@article{gordon_geometric_1983,
  title = {Geometric {{Deconvolution}}: {{A Meta}}-{{Algorithm}} for {{Limited View Computed Tomography}}},
  volume = {BME-30},
  issn = {0018-9294},
  shorttitle = {Geometric {{Deconvolution}}},
  doi = {10.1109/TBME.1983.325082},
  abstract = {Images reconstructed using a limited number of projections spanning a narrow angular range suffer from a systematic geometric distortion due to the two-dimensional point spread function of the reconstruction process. Applying the projection theorem, we show that the problem of removing this distortion reduces to that of estimating the one-dimensional spread function and deconvolving projections computed for a complementary set of new angles from the initial reconstruction. A second reconstruction is performed using the deconvolved projections along with the original set of projections, thus incorporating wider angular coverage. We present here initial results of such geometric deconvolution performed via inverse filtering using fast Fourier transform techniques. While the results are noisy due to well-known problems associated with inverse filtering, they illustrate the plausibility of the underlying ideas.},
  language = {English},
  number = {12},
  journal = {IEEE Transactions on Biomedical Engineering},
  author = {Gordon, Richard and Rangayyan, Rangaraj M},
  month = dec,
  year = {1983},
  keywords = {Biology computing,Biomedical Engineering,Computed tomography,Deconvolution,Distortion measurement,Filtering,Image reconstruction,Image resolution,Mathematics,Reconstruction algorithms,Testing,Tomography; X-Ray Computed,X-ray imaging},
  pages = {806--810}
}

@article{kyriakou_empirical_2010,
  title = {Empirical Beam Hardening Correction ({{EBHC}}) for {{CT}}},
  volume = {37},
  issn = {0094-2405},
  doi = {10.1118/1.3477088},
  abstract = {Purpose: Due to x-ray beam polychromaticity and scattered radiation, attenuation measurements tend to be underestimated. Cupping and beam hardening artifacts become apparent in the reconstructedCTimages. If only one material such as water, for example, is present, these artifacts can be reduced by precorrecting the rawdata. Higher order beam hardening artifacts, as they result when a mixture of materials such as water and bone, or water and bone and iodine is present, require an iterative beam hardening correction where the image is segmented into different materials and those are forward projected to obtain new rawdata. Typically, the forward projection must correctly model the beam polychromaticity and account for all physical effects, including the energy dependence of the assumed materials in the patient, the detector response, and others. We propose a new algorithm that does not require any knowledge about spectra or attenuation coefficients and that does not need to be calibrated. The proposed method corrects beam hardening in single energy CT data. Methods: The onlya priori knowledge entering EBHC is the segmentation of the object into different materials. Materials other than water are segmented from the original image, e.g., by using simple thresholding. Then, a (monochromatic) forward projection of these other materials is performed. The measured rawdata and the forward projected material-specific rawdata are monomially combined (e.g., multiplied or squared) and reconstructed to yield a set of correction volumes. These are then linearly combined and added to the original volume. The combination weights are determined to maximize the flatness of the new and corrected volume. EBHC is evaluated using data acquired with a modern cone-beam dual-source spiral CTscanner (Somatom Definition Flash, Siemens Healthcare, Forchheim, Germany), with a modern dual-source micro-CT scanner (TomoScope Synergy Twin, CTImaging GmbH, Erlangen, Germany), and with a modern C-arm CTscanner (Axiom Artis dTA, Siemens Healthcare, Forchheim, Germany). A large variety of phantom, small animal, and patient data were used to demonstrate the data and system independence of EBHC. Results: Although no physics apart from the initial segmentation procedure enter the correction process, beam hardening artifacts were significantly reduced by EBHC. The image quality for clinical CT, micro-CT, and C-arm CT was highly improved. Only in the case of C-arm CT, where high scatter levels and calibration errors occur, the relative improvement was smaller. Conclusions: The empirical beam hardening correction is an interesting alternative to conventional iterative higher order beam hardening correction algorithms. It does not tend to over- or undercorrect the data. Apart from the segmentation step, EBHC does not require assumptions on the spectra or on the type of material involved. Potentially, it can therefore be applied to any CTimage.},
  number = {10},
  journal = {Medical Physics},
  author = {Kyriakou, Yiannis and Meyer, Esther and Prell, Daniel and Kachelrie{\ss}, Marc},
  month = oct,
  year = {2010},
  keywords = {Computed tomography,Image reconstruction,Medical image reconstruction,Medical image segmentation,Medical imaging},
  pages = {5179--5187},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/422R3QNI/Kyriakou et al. - 2010 - Empirical beam hardening correction (EBHC) for CT.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/MR8D43NQ/1.html}
}

@article{supanich_radiation_2009,
  title = {Radiation Dose Reduction in Time-Resolved {{CT}} Angiography Using Highly Constrained Back Projection Reconstruction},
  volume = {54},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/54/14/013},
  abstract = {Recently dynamic, time-resolved three-dimensional computed tomography angiography (CTA) has been introduced to the neurological imaging community. However, the radiation dose delivered to patients in time-resolved CTA protocol is a high and potential risk associated with the ionizing radiation dose. Thus, minimizing the radiation dose is highly desirable for time-resolved CTA. In order to reduce the radiation dose delivered during dynamic, contrast-enhanced CT applications, we introduce here the CT formulation of HighlY constrained back PRojection (HYPR) imaging. We explore the radiation dose reduction approaches of both acquiring a reduced number of projections for each image and lowering the tube current used during acquisition. We then apply HYPR image reconstruction to produce image sets at a reduced patient dose and with low image noise. Numerical phantom experiments and retrospective analysis of in vivo canine studies are used to assess the accuracy and quality of HYPR reduced dose image sets and validate our approach. Experimental results demonstrated that a factor of 6-8 times radiation dose reduction is possible when the HYPR algorithm is applied to time-resolved CTA exams.},
  number = {14},
  journal = {Physics in medicine and biology},
  author = {Supanich, Mark and Tao, Yinghua and Nett, Brian and Pulfer, Kari and Hsieh, Jiang and Turski, Patrick and Mistretta, Charles and Rowley, Howard and Chen, Guang-Hong},
  month = jul,
  year = {2009},
  keywords = {Algorithms,Angiography,Animals,Body Burden,Dogs,Image Enhancement,Image Interpretation; Computer-Assisted,Radiation Dosage,Radiometry,Reproducibility of Results,Sensitivity and Specificity,Tomography; X-Ray Computed},
  pages = {4575--4593},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/3JFQK234/Supanich et al. - 2009 - Radiation dose reduction in time-resolved CT angio.pdf},
  pmid = {19567941}
}

@article{schmitt_x-ray-based_2002,
  title = {An X-Ray-Based Method for the Determination of the Contrast Agent Propagation in 3-d Vessel Structures},
  volume = {21},
  number = {3},
  journal = {Medical Imaging, IEEE Transactions on},
  author = {Schmitt, H. and Grass, M. and Rasche, V. and Schramm, O. and Haehnel, S. and Sartor, K.},
  year = {2002},
  pages = {251--262},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/2ZKIIS7P/Schmitt et al. - 2002 - An x-ray-based method for the determination of the.pdf}
}

@phdthesis{sunnegaardh_iterative_2009,
  title = {Iterative {{Filtered Backprojection Methods}} for {{Helical Cone}}-{{Beam CT}}},
  school = {Link{\"o}ping University},
  author = {Sunnegaardh, Johan},
  year = {2009},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/TENXT7AN/Sunneg aa rdh - 2009 - Iterative Filtered Backprojection Methods for Heli.pdf}
}

@article{pan_why_2009,
  title = {Why Do Commercial {{CT}} Scanners Still Employ Traditional, Filtered Back-Projection for Image Reconstruction?},
  volume = {25},
  issn = {0266-5611, 1361-6420},
  doi = {10.1088/0266-5611/25/12/123009},
  number = {12},
  journal = {Inverse Problems},
  author = {Pan, Xiaochuan and Sidky, Emil Y and Vannier, Michael},
  month = dec,
  year = {2009},
  pages = {123009},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/ZDDJQGAZ/PMC2849113.html}
}

@article{taubmann_coping_2016,
  title = {Coping with Real World Data: {{Artifact}} Reduction and Denoising for Motion-Compensated Cardiac {{C}}-Arm {{CT}}},
  volume = {43},
  shorttitle = {Coping with Real World Data},
  number = {2},
  journal = {Medical physics},
  author = {Taubmann, Oliver and Maier, Andreas and Hornegger, Joachim and Lauritsch, G{\"u}nter and Fahrig, Rebecca},
  year = {2016},
  pages = {883--893},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/3UF3C3MX/Taubmann16-CWR.pdf}
}

@article{wang_simultaneous_2013,
  title = {Simultaneous Motion Estimation and Image Reconstruction ({{SMEIR}}) for {{4D}} Cone-Beam {{CT}}},
  volume = {40},
  issn = {0094-2405},
  doi = {10.1118/1.4821099},
  abstract = {Purpose: Image reconstruction and motion model estimation in four-dimensional cone-beam CT (4D-CBCT) are conventionally handled as two sequential steps. Due to the limited number of projections at each phase, the image quality of 4D-CBCT is degraded by view aliasing artifacts, and the accuracy of subsequent motion modeling is decreased by the inferior 4D-CBCT. The objective of this work is to enhance both the image quality of 4D-CBCT and the accuracy of motion model estimation with a novel strategy enabling simultaneous motion estimation and image reconstruction (SMEIR). Methods: The proposed SMEIR algorithm consists of two alternating steps: (1) model-based iterative image reconstruction to obtain a motion-compensated primary CBCT (m-pCBCT) and (2) motion model estimation to obtain an optimal set of deformation vector fields (DVFs) between the m-pCBCT and other 4D-CBCT phases. The motion-compensated image reconstruction is based on the simultaneous algebraic reconstruction technique (SART) coupled with total variation minimization. During the forward- and backprojection of SART, measured projections from an entire set of 4D-CBCT are used for reconstruction of the m-pCBCT by utilizing the updated DVF. The DVF is estimated by matching the forward projection of the deformed m-pCBCT and measured projections of other phases of 4D-CBCT. The performance of the SMEIR algorithm is quantitatively evaluated on a 4D NCAT phantom. The quality of reconstructed 4D images and the accuracy of tumor motion trajectory are assessed by comparing with those resulting from conventional sequential 4D-CBCT reconstructions (FDK and total variation minimization) and motion estimation (demons algorithm). The performance of the SMEIR algorithm is further evaluated by reconstructing a lung cancer patient 4D-CBCT. Results: Image quality of 4D-CBCT is greatly improved by the SMEIR algorithm in both phantom and patient studies. When all projections are used to reconstruct a 3D-CBCT by FDK, motion-blurring artifacts are present, leading to a 24.4\% relative reconstruction error in the NACT phantom. View aliasing artifacts are present in 4D-CBCT reconstructed by FDK from 20 projections, with a relative error of 32.1\%. When total variation minimization is used to reconstruct 4D-CBCT, the relative error is 18.9\%. Image quality of 4D-CBCT is substantially improved by using the SMEIR algorithm and relative error is reduced to 7.6\%. The maximum error (MaxE) of tumor motion determined from the DVF obtained by demons registration on a FDK-reconstructed 4D-CBCT is 3.0, 2.3, and 7.1 mm along left\textendash{}right (L-R), anterior\textendash{}posterior (A-P), and superior\textendash{}inferior (S-I) directions, respectively. From the DVF obtained by demons registration on 4D-CBCT reconstructed by total variation minimization, the MaxE of tumor motion is reduced to 1.5, 0.5, and 5.5 mm along L-R, A-P, and S-I directions. From the DVF estimated by SMEIR algorithm, the MaxE of tumor motion is further reduced to 0.8, 0.4, and 1.5 mm along L-R, A-P, and S-I directions, respectively. Conclusions: The proposed SMEIR algorithm is able to estimate a motion model and reconstruct motion-compensated 4D-CBCT. The SMEIR algorithm improves image reconstruction accuracy of 4D-CBCT and tumor motion trajectory estimation accuracy as compared to conventional sequential 4D-CBCT reconstruction and motion estimation.},
  number = {10},
  journal = {Medical Physics},
  author = {Wang, Jing and Gu, Xuejun},
  month = oct,
  year = {2013},
  keywords = {Cancer,Cone beam computed tomography,Image reconstruction,Medical image reconstruction,Medical imaging},
  pages = {101912},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/C3QN7N8R/Wang and Gu - 2013 - Simultaneous motion estimation and image reconstru.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/QG5XTVNP/1.html}
}

@article{moret_3d_1998,
  title = {{{3D}} Rotational Angiography: {{Clinical}} Value in Endovascular Treatment},
  volume = {42},
  shorttitle = {{{3D}} Rotational Angiography},
  journal = {medicamundi},
  author = {Moret, J. and Kemkers, R. and {Op de Beek}, J. and Koppe, R. and Klotz, E. and Grass, M.},
  year = {1998},
  pages = {8--14},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/A5MJDB5C/Moret et al. - 1998 - 3D rotational angiography Clinical value in endov.pdf}
}

@inproceedings{hao_improved_2012,
  title = {An Improved Non-Local Means Regularized Iterative Reconstruction Method for Low-Dose Dental {{CBCT}}},
  doi = {10.1109/NSSMIC.2012.6551780},
  abstract = {In clinical practice, the widespread use of dental CBCT scanners has been limited because of cost, availability and radiation dose considerations. The recently proposed non-local means (NLM) method achieves excellent performance in digital image denoising, which can also be used in low-dose CBCT noise reduction. In this paper, we explore an improved non-local means (INLM) regularized iterative reconstruction method for low-dose CBCT with a low-mAs scanning protocol. By using a pre-classification and similar block searching method, the INLM filtering is more efficient than conventional NLM method. Clinical experiments have been conducted and demonstrate its performance in low-dose CBCT volume reconstruction. Compared with FDK method, the proposed method improves PSNR, image quality while preserves structure and detail information in low-dose CBCT imaging.},
  booktitle = {2012 {{IEEE Nuclear Science Symposium}} and {{Medical Imaging Conference}} ({{NSS}}/{{MIC}})},
  author = {Hao, Jia and Zhang, Li and Li, Liang and Kang, Kejun},
  year = {2012},
  keywords = {dental CBCT,iterative reconstruction algorithm,low-dose,non-local means},
  pages = {3422--3425},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/78JD5TCV/M17-046.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/TGHUZQMX/login.html}
}

@inproceedings{nguyen_fast_2015,
  title = {Fast and {{Effective L0 Gradient Minimization}} by {{Region Fusion}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Nguyen, Rang MH and Brown, Michael S.},
  year = {2015},
  pages = {208--216},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/24XRDUZR/ICCV2015_L0_Gradient.pdf}
}

@article{boussel_assessment_2008,
  title = {Assessment of Acute Myocardial Infarction Using {{MDCT}} after Percutaneous Coronary Intervention: Comparison with {{MRI}}},
  volume = {191},
  issn = {1546-3141},
  shorttitle = {Assessment of Acute Myocardial Infarction Using {{MDCT}} after Percutaneous Coronary Intervention},
  doi = {10.2214/AJR.07.3404},
  abstract = {OBJECTIVE

Imaging to determine myocardial infarct size is difficult in the emergency setting because the current gold standards, MRI and nuclear medicine techniques, are difficult to perform in unstable patients. Delayed enhanced MDCT has recently been proposed as a technique to study contrast uptake in infarcted myocardium. In this study, we compared the extent of acute myocardial infarction as measured by delayed enhanced MDCT performed immediately after percutaneous coronary intervention (PCI) without an additional iodine injection with that measured by delayed gadolinium-enhanced MRI.

SUBJECTS AND METHODS

Nineteen consecutive patients presenting with primary acute myocardial infarction underwent delayed enhanced MDCT immediately after coronary angioplasty and underwent delayed enhanced MRI within 8 days of angioplasty. Only patients with a thrombolysis in myocardial infarction (TIMI) score of 0 or 1 of the culprit coronary artery before endovascular angioplasty and TIMI score of 2 or 3 after angioplasty were selected. Comparison of delayed enhanced MDCT and delayed enhanced MRI was performed by three observers and focused on identifying the involved segments and determining the transmural extent of enhancement and infarct size.

RESULTS

The mean signal intensity was significantly higher in the involved territory than in healthy myocardium: 197 +/- 81 H versus 71 +/- 20 H, respectively (p $<$ 0.0001). We found significant agreement between delayed enhanced MDCT and delayed enhanced MRI for the number of involved segments, transmural extent of enhancement, and infarct size (r(2) = 0.74, 0.76, and 0.67, respectively; p $<$ 0.0001) with good interobserver reproducibility (kappa = 0.8).

CONCLUSION

The results of our study show that delayed enhanced MDCT allows accurate visualization of early myocardial contrast uptake compared with delayed enhanced MRI and does not require an additional contrast injection after PCI.},
  number = {2},
  journal = {AJR. American Journal of Roentgenology},
  author = {Boussel, Lo{\"\i}c and Ribagnac, Michael and Bonnefoy, Eric and Staat, Patrick and Elicker, Brett M and Revel, Didier and Douek, Philippe},
  month = aug,
  year = {2008},
  keywords = {Adult,Aged,Angioplasty; Balloon,Contrast Media,Female,Heterocyclic Compounds,Humans,Imaging; Three-Dimensional,Magnetic Resonance Imaging,Male,Middle Aged,Myocardial Infarction,Organometallic Compounds,Reproducibility of Results,Tomography; X-Ray Computed,Treatment Outcome},
  pages = {441--447},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/T39S3GCM/Boussel et al. - 2008 - Assessment of acute myocardial infarction using MD.pdf},
  pmid = {18647915}
}

@article{zijp_extraction_,
  title = {Extraction of the Respiratory Signal from Sequential Thorax {{Cone}}-{{Beam X}}-Ray Images},
  author = {Zijp, Lambert and Sonke, Jan-Jakob and {van Herk}, Marcel},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/AJV5WS2R/Zijp et al. - Extraction of the respiratory signal from sequenti.pdf}
}

@inproceedings{chen_prior_2008-1,
  address = {San Diego, CA, USA},
  title = {Prior Image Constrained Compressed Sensing ({{PICCS}})},
  doi = {10.1117/12.770532},
  author = {Chen, Guang-Hong and Tang, Jie and Leng, Shuai},
  editor = {Oraevsky, Alexander A. and Wang, Lihong V.},
  month = feb,
  year = {2008},
  pages = {685618--685618--14},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/NW9R63U4/Chen et al. - 2008 - Prior image constrained compressed sensing (PICCS).pdf}
}

@article{li_enhanced_2007,
  title = {Enhanced {{4D}} Cone-Beam {{CT}} with Inter-Phase Motion Model},
  volume = {34},
  issn = {00942405},
  doi = {10.1118/1.2767144},
  language = {en},
  number = {9},
  journal = {Medical Physics},
  author = {Li, Tianfang and Koong, Albert and Xing, Lei},
  year = {2007},
  pages = {3688},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/BE6NRH9W/Li et al. - 2007 - Enhanced 4D cone-beam CT with inter-phase motion m.pdf}
}

@article{qi_performance_2011,
  title = {Performance Studies of Four-Dimensional Cone Beam Computed Tomography},
  volume = {56},
  issn = {1361-6560},
  doi = {10.1088/0031-9155/56/20/013},
  abstract = {Four-dimensional cone beam computed tomography (4DCBCT) has been proposed to characterize the breathing motion of tumors before radiotherapy treatment. However, when the acquired cone beam projection data are retrospectively gated into several respiratory phases, the available data to reconstruct each phase is under-sampled and thus causes streaking artifacts in the reconstructed images. To solve the under-sampling problem and improve image quality in 4DCBCT, various methods have been developed. This paper presents performance studies of three different 4DCBCT methods based on different reconstruction algorithms. The aims of this paper are to study (1) the relationship between the accuracy of the extracted motion trajectories and the data acquisition time of a 4DCBCT scan and (2) the relationship between the accuracy of the extracted motion trajectories and the number of phase bins used to sort projection data. These aims will be applied to three different 4DCBCT methods: conventional filtered backprojection reconstruction (FBP), FBP with McKinnon-Bates correction (MB) and prior image constrained compressed sensing (PICCS) reconstruction. A hybrid phantom consisting of realistic chest anatomy and a moving elliptical object with known 3D motion trajectories was constructed by superimposing the analytical projection data of the moving object to the simulated projection data from a chest CT volume dataset. CBCT scans with gantry rotation times from 1 to 4 min were simulated, and the generated projection data were sorted into 5, 10 and 20 phase bins before different methods were used to reconstruct 4D images. The motion trajectories of the moving object were extracted using a fast free-form deformable registration algorithm. The root mean square errors (RMSE) of the extracted motion trajectories were evaluated for all simulated cases to quantitatively study the performance. The results demonstrate (1) longer acquisition times result in more accurate motion delineation for each method; (2) ten or more phase bins are necessary in 4DCBCT to ensure sufficient temporal resolution in tumor motion and (3) to achieve the same performance as FBP-4DCBCT with a 4 min data acquisition time, MB-4DCBCT and PICCS-4DCBCT need about 2- and 1 min data acquisition times, respectively.},
  language = {eng},
  number = {20},
  journal = {Physics in medicine and biology},
  author = {Qi, Zhihua and Chen, Guang-Hong},
  month = oct,
  year = {2011},
  keywords = {Algorithms,Cone-Beam Computed Tomography,Four-Dimensional Computed Tomography,Humans,Movement,Radiography; Thoracic,Respiration},
  pages = {6709--6721},
  pmid = {21965275},
  pmcid = {PMC3365579}
}

@article{zhao_gpu_2013,
  title = {{{GPU}} Based Iterative Cone-Beam {{CT}} Reconstruction Using Empty Space Skipping Technique},
  volume = {21},
  issn = {1095-9114},
  doi = {10.3233/XST-130366},
  abstract = {Iterative reconstruction of high-resolution cone-beam CT data is still a difficult task due to the demand for vast amounts of computer cycles and associated memory. In order to improve the performance of iterative algorithms for cone-beam CT reconstruction, an acceleration approach integrating GPU acceleration, empty space skipping and multi-resolution technique is proposed. The approach divides the reconstructed volume into equally sized blocks, and empty blocks are identified by reconstructing an initial low-resolution volume and segmenting it with threshold method. Then all non-empty blocks are packed into a new volume, which is initialized by interpolating the low resolution volume and reconstructed at full resolution using iterative algorithms. Finally these non-empty blocks are rearranged to get the reconstructed high-resolution volume. The whole process is implemented in parallel based on GPU. Since only the voxels in non-empty blocks are calculated, the number of considered voxels is greatly reduced, which translates directly into substantial computation, memory requirements and data transfer savings. The method is evaluated by reconstructing images from simulated projection data of phantom and CT datasets. The results indicate that our approach significantly improves the performance of iterative reconstruction while maintaining a high image quality, compared to conventional GPU-based approaches.},
  language = {eng},
  number = {1},
  journal = {Journal of X-ray science and technology},
  author = {Zhao, Xing and Hu, Jing-Jing and Yang, Tao},
  year = {2013},
  keywords = {Algorithms,Animals,Beetles,Computer Graphics,Cone-Beam Computed Tomography,Foot,Head,Humans,Image Processing; Computer-Assisted,Phantoms; Imaging},
  pages = {53--69},
  pmid = {23507852}
}

@article{feldkamp_practical_1984,
  title = {Practical Cone-Beam Algorithm},
  volume = {1},
  doi = {10.1364/JOSAA.1.000612},
  abstract = {A convolution-backprojection formula is deduced for direct reconstruction of a three-dimensional density function from a set of two-dimensional projections. The formula is approximate but has useful properties, including errors that are relatively small in many practical instances and a form that leads to convenient computation. It reduces to the standard fan-beam formula in the plane that is perpendicular to the axis of rotation and contains the point source. The algorithm is applied to a mathematical phantom as an example of its performance.},
  number = {6},
  journal = {Journal of the Optical Society of America A},
  author = {Feldkamp, L. A. and Davis, L. C. and Kress, J. W.},
  month = jun,
  year = {1984},
  pages = {612--619},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/GPKXEEFX/Feldkamp et al. - 1984 - Practical cone-beam algorithm.pdf}
}

@article{clackdoyle_data_2016-1,
  title = {Data {{Consistency Conditions}} for {{Cone}}-{{Beam Projections}} on a {{Circular Trajectory}}},
  volume = {23},
  issn = {1070-9908, 1558-2361},
  doi = {10.1109/LSP.2016.2616026},
  number = {12},
  journal = {IEEE Signal Processing Letters},
  author = {Clackdoyle, Rolf and Desbat, Laurent and Lesaint, Jerome and Rit, Simon},
  month = dec,
  year = {2016},
  pages = {1746--1750},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/HGD6SSB3/clackdoyle2016.pdf}
}

@article{oskoui_comparative-study_1989,
  title = {A {{COMPARATIVE}}-{{STUDY OF}} 3 {{RECONSTRUCTION METHODS FOR A LIMITED}}-{{VIEW}}   {{COMPUTER}}-{{TOMOGRAPHY PROBLEM}}},
  volume = {8},
  issn = {0278-0062},
  doi = {10.1109/42.20360},
  language = {English},
  number = {1},
  journal = {Ieee Transactions on Medical Imaging},
  author = {OSKOUI, P and STARK, H},
  month = mar,
  year = {1989},
  pages = {43--49},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/8HSVWUNQ/full_record.html},
  note = {WOS:A1989T335300005}
}

@inproceedings{desbat_algebraic_,
  title = {Algebraic and Analytic Reconstruction Methods for Dynamic Tomography},
  doi = {10.1109/IEMBS.2007.4352393},
  abstract = {In this work, we discuss algebraic and analytic approaches for dynamic tomography. We present a framework of dynamic tomography for both algebraic and analytic approaches. We finally present numerical experiments.},
  booktitle = {29th {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}}, 2007. {{EMBS}} 2007},
  author = {Desbat, L. and Rit, S. and Clackdoyle, R. and Mennessier, C. and Promayon, E. and Ntalampeki, S.},
  year = {Aug.},
  keywords = {Computed tomography,Deformable models,Geometry,Heart,Image reconstruction,Nuclear measurements,Positron emission tomography,Reconstruction algorithms,Time measurement,algebraic reconstruction method,analytic reconstruction method,attenuation,computerised tomography,dynamic tomography,nuclear medicine,patient deformation,patient movement},
  pages = {726--730},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/HHSC5FMK/Desbat et al. - Algebraic and analytic reconstruction methods for .pdf}
}

@article{nassi_iterative_1982,
  title = {Iterative Reconstruction-Reprojection: An Algorithm for Limited Data Cardiac-Computed Tomography},
  volume = {29},
  issn = {0018-9294},
  shorttitle = {Iterative Reconstruction--Reprojection},
  doi = {10.1109/TBME.1982.324900},
  language = {eng},
  number = {5},
  journal = {IEEE transactions on bio-medical engineering},
  author = {Nassi, M and Brody, W R and Medoff, B P and Macovski, A},
  month = may,
  year = {1982},
  keywords = {Heart Diseases,Mathematics,Time Factors,Tomography; X-Ray Computed},
  pages = {333--341},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/D8AP7D7X/Nassi et al. - 1982 - Iterative reconstruction-reprojection an algorith.pdf},
  pmid = {7084960}
}

@article{manzke_helical_2005,
  title = {Helical Cardiac Cone Beam {{CT}} Reconstruction with Large Area Detectors: A Simulation Study},
  volume = {50},
  issn = {0031-9155},
  shorttitle = {Helical Cardiac Cone Beam {{CT}} Reconstruction with Large Area Detectors},
  doi = {10.1088/0031-9155/50/7/016},
  abstract = {Retrospectively gated cardiac volume CT imaging has become feasible with the introduction of heart rate adaptive cardiac CT reconstruction algorithms. The development in detector technology and the rapid introduction of multi-row detectors has demanded reconstruction schemes which account for the cone geometry. With the extended cardiac reconstruction (ECR) framework, the idea of approximate helical cone beam CT has been extended to be used with retrospective gating, enabling heart rate adaptive cardiac cone beam reconstruction. In this contribution, the ECR technique is evaluated for systems with an increased number of detector rows, which leads to larger cone angles. A simulation study has been carried out based on a 4D cardiac phantom consisting of a thorax model and a dynamic heart insert. Images have been reconstructed for different detector set-ups. Reconstruction assessment functions have been calculated for the detector set-ups employing different rotation times, relative pitches and heart rates. With the increased volume coverage of large area detector systems, low-pitch scans become feasible without resulting in extensive scan times, inhibiting single breath hold acquisitions. ECR delivers promising image results when being applied to systems with larger cone angles.},
  number = {7},
  journal = {Physics in medicine and biology},
  author = {Manzke, R and Koken, P and Hawkes, D and Grass, M},
  month = apr,
  year = {2005},
  keywords = {Algorithms,Electrocardiography,Equipment Design,Equipment Failure Analysis,Feasibility Studies,Heart,Imaging; Three-Dimensional,Movement,Phantoms; Imaging,Radiographic Image Enhancement,Radiographic Image Interpretation; Computer-Assisted,Tomography; Spiral Computed,Transducers},
  pages = {1547--1568},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/7WBDMIWD/Manzke et al. - 2005 - Helical cardiac cone beam CT reconstruction with l.pdf},
  pmid = {15798343}
}

@article{sauer_local_1993,
  title = {A Local Update Strategy for Iterative Reconstruction from Projections},
  volume = {41},
  number = {2},
  journal = {Signal Processing, IEEE Transactions on},
  author = {Sauer, Ken and Bouman, Charles},
  year = {1993},
  pages = {534--548},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/BFKRS62D/sp2.pdf}
}

@book{dukes_dukes_2004,
  title = {Dukes' Physiology of Domestic Animals},
  isbn = {978-0-8014-4238-4},
  abstract = {This is a major revision and redesign of a classic work in the field of veterinary medicine first published by Cornell University Press in 1933. For the Twelfth Edition of Dukes' Physiology of Domestic Animals, William O. Reece has overseen the writing of an essentially new book that retains what was best about its predecessors. Long a standard text for veterinary practitioners and other professionals who seek to refresh their knowledge of particular subjects, Dukes' Physiology of Domestic Animals will now appeal to a new generation of students of veterinary medicine and animal science and to biomedical researchers. Section editors known for their scientific expertise and teaching skill oversaw the book's six main sections: The Body Fluids and Blood; Renal and Respiratory Function and Acid-Base Balance; The Cardiovascular System; Digestion, Absorption, and Metabolism; Endocrinology, Reproduction, and Lactation; and Nervous System, Special Senses, Muscle, and Temperature Regulation. This edition includes a complete listing of each chapter's contents immediately after the chapter head, study questions that highlight the major concepts of each major subsection, self-evaluation exercises at the end of the chapters, suggestions for further reading, and a comprehensive index.},
  language = {en},
  publisher = {{Comstock Pub./Cornell University Press}},
  author = {Dukes, Henry Hugh and Reece, William O.},
  month = aug,
  year = {2004},
  keywords = {Medical / Physiology,Medical / Veterinary Medicine / General,Physiology; Comparative,Science / Life Sciences / Anatomy \& Physiology,Technology \& Engineering / Agriculture / General,Veterinary physiology}
}

@article{liu_x-ray_2003,
  title = {X-Ray Micro-{{CT}} with a Displaced Detector Array: Application to Helical Cone-Beam Reconstruction},
  volume = {30},
  issn = {0094-2405},
  shorttitle = {X-Ray Micro-{{CT}} with a Displaced Detector Array},
  abstract = {In x-ray micro-CT applications, it is useful to increase the field of view by offsetting a two-dimensional (2D) detector array. In this technical note, we briefly review the methods for image reconstruction with an asymmetric 2D detector array, elaborate on the use of an associated weighting scheme in the case of helical/spiral cone-beam scanning, and perform a series of numerical tests to demonstrate helical cone-beam image reconstruction with such an arrangement.},
  language = {eng},
  number = {10},
  journal = {Medical Physics},
  author = {Liu, Vinson and Lariviere, Nicholas R. and Wang, Ge},
  month = oct,
  year = {2003},
  keywords = {Algorithms,Computer Simulation,Humans,Image Processing; Computer-Assisted,Phantoms; Imaging,Radiographic Image Enhancement,Tomography; X-Ray Computed,X-Rays},
  pages = {2758--2761},
  pmid = {14596314}
}

@article{wolthaus_comparison_2008,
  title = {Comparison of {{Different Strategies}} to {{Use Four}}-{{Dimensional Computed Tomography}} in {{Treatment Planning}} for {{Lung Cancer Patients}}},
  volume = {70},
  issn = {03603016},
  doi = {10.1016/j.ijrobp.2007.11.042},
  language = {en},
  number = {4},
  journal = {International Journal of Radiation Oncology*Biology*Physics},
  author = {Wolthaus, Jochem W.H. and Sonke, Jan-Jakob and {van Herk}, Marcel and Belderbos, Jos{\'e} S.A. and Rossi, Maddalena M.G. and Lebesque, Joos V. and Damen, Eug{\`e}ne M.F.},
  month = mar,
  year = {2008},
  pages = {1229--1238},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/7KKZ4TDJ/wolthaus2008.pdf}
}

@article{kachelriess_electrocardiogram-correlated_1998,
  title = {Electrocardiogram-Correlated Image Reconstruction from Subsecond Spiral Computed Tomography Scans of the Heart},
  volume = {25},
  journal = {Medical physics},
  author = {Kachelriess, Marc and Kalender, Willi A.},
  year = {1998},
  pages = {2417},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/UEFDKIIP/Kachelriess and Kalender - 1998 - Electrocardiogram-correlated image reconstruction .pdf}
}

@article{ziegler_iterative_2008,
  title = {Iterative Reconstruction of a Region of Interest for Transmission Tomography},
  volume = {35},
  issn = {00942405},
  doi = {10.1118/1.2870219},
  language = {en},
  number = {4},
  journal = {Medical Physics},
  author = {Ziegler, Andy and Nielsen, Tim and Grass, Michael},
  year = {2008},
  pages = {1317},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/7F9H6KJZ/Ziegler et al. - 2008 - Iterative reconstruction of a region of interest f.pdf}
}

@article{lv_spectral_2012,
  title = {Spectral {{CT}} in Patients with Small {{HCC}}: Investigation of Image Quality and Diagnostic Accuracy},
  volume = {22},
  issn = {1432-1084},
  shorttitle = {Spectral {{CT}} in Patients with Small {{HCC}}},
  doi = {10.1007/s00330-012-2485-3},
  abstract = {OBJECTIVE: To assess image quality and diagnostic accuracy of monochromatic imaging from spectral CT in patients with small HCC ($\leq$3~cm).
METHODS: Twenty-seven patients with 31 HCC underwent spectral CT to generate conventional 140-kVp polychromatic images (group A) and monochromatic images with energy levels from 40 to 140~keV (group B) during the late arterial phase (LAP) and the portal venous phase (PVP). Two-sample t tests compared the tumour-to-liver contrast-to-noise ratio (CNR) and mean image noise. Lesion detection for LAP, reader confidence and readers' subjective evaluations of image quality were recorded.
RESULTS: Highest CNRs in group B were distributed at 40, 50 and 70~keV. Higher CNR values and lesion conspicuity scores (LCS) were obtained in group B than in group A (CNR 3.36\,$\pm$\,2.07 vs. 1.47\,$\pm$\,0.89 in LAP; 2.29\,$\pm$\,2.26 vs. 1.58\,$\pm$\,1.75 in PVP; LCS 2.82, 2.84, 2.63 and 2.53 at 40-70~keV, respectively, vs. 1.95) (P\,$<$\,0.001). Lowest image noise for group B was at 70~keV, resulting in higher image quality than that in group A (4.70 vs. 4.07; P\,$<$\,0.001).
CONCLUSION: Monochromatic energy levels of 40-70~keV can increase detectability in small HCC and this increase might not result in image quality degradation.
KEY POINTS: \textbullet{} Spectral computed tomography may help the detection of small hepatocellular carcinoma. \textbullet{} Monochromatic energy levels of 40-70~keV increase the sensitivity for detection. \textbullet{} Prospective study showed that monochromatic imaging provides greater diagnostic confidence. \textbullet{} Monochromatic energy level of 70~keV improves the overall image quality.},
  language = {ENG},
  number = {10},
  journal = {European Radiology},
  author = {Lv, Peijie and Lin, Xiao Zhu and Chen, Kemin and Gao, Jianbo},
  month = oct,
  year = {2012},
  keywords = {Adult,Aged,Carcinoma; Hepatocellular,Female,Humans,Liver Neoplasms,Male,Middle Aged,Prospective Studies,Reproducibility of Results,Tomography; X-Ray Computed},
  pages = {2117--2124},
  pmid = {22618521}
}

@inproceedings{gordon_artifacts_1973,
  address = {Washington D.C.},
  title = {Artifacts in {{Reconstructions Made}} from a {{Few Projections}}},
  booktitle = {Proceedings, {{First International Joint Conference}} on {{Pattern Recognition}}},
  author = {Gordon, R.},
  month = nov,
  year = {1973},
  pages = {275--285},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/ER4M8G24/Gordon - 1973 - Artifacts in Reconstructions Made from a Few Proje.pdf}
}

@article{shearer_parallel_,
  title = {Parallel Image Restoration with Spatially Variant Point Spread Function - {{Description}} and First Clinical Results},
  author = {Shearer, A. and Gorman, G. and Gorman, Gerard and Carthy, Peter Mc and Jelen, Lukasz},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/B44PGPKF/Shearer et al. - Parallel image restoration with spatially variant .pdf}
}

@inproceedings{momey_b-spline_2012,
  title = {A {{B}}-Spline Based and Computationally Performant Projector for Iterative Reconstruction in Tomography - {{Application}} to Dynamic {{X}}-Ray Gated {{CT}}},
  abstract = {Data modelization in tomography is a key point for iterative reconstruction. The design of the projector starts with the representation of the object of interest, decomposed on a discrete basis of functions. Standard models of projector such as ray driven, or more advanced models such as distance driven, use simple cubic voxels, which result in modelization errors due to their anisotropic behaviour. Moreover approximations made at the projection step increase these errors. Long, Fessler and Balter reduce approximation errors by projecting the cubic voxels more accurately. However anisotropy errors still hold. Spherically symmetric volume elements (blobs) eradicate them, but at the cost of increased complexity. We propose a compromise between these two approaches by using B-splines as basis functions. Their quasi-isotropic behaviour allows to avoid projection inconsistencies, while conserving local influence. Small approximations transform the exact footprint (projection of the basis function) into a separable function, which does not depend on the angle of projection, and is easier and faster to integrate on detector pixels. We obtain a more accurate projector, with no additional computation cost. Such an improvement is particularly of interest in the case of dynamic gated X-ray CT, which can be considered as a tomographic reconstruction problem with very few projection data, and for which we show some preliminary results, with an original method of iterative reconstruction, using spatio-temporal regularization of the 'space + time' sequence, and making no use of motion estimation.},
  booktitle = {2012 {{Second International Conference}} on {{Image Formation}} in {{X}}-{{Ray Computed Tomography}}},
  author = {Momey, Fabien and Denis, Lo{\"\i}c and Mennessier, Catherine and Thi{\'e}baut, {\'E}ric and Becker, Jean-Marie and Desbat, Laurent},
  month = jun,
  year = {2012},
  keywords = {B-splines,Inverse problems,Iterative reconstruction,Tomography,dynamic tomography,gated CT,model,projector},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/QDZZZG32/Momey et al. - 2012 - A B-spline based and computationally performant pr.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/IIZJTZFG/ujm-00715785.html}
}

@inproceedings{knaup_simple_2010,
  title = {Simple {{ROI Cone}}\textendash{}{{Beam Computed Tomography}}},
  booktitle = {Proc. of the {{CT Meeting}}, {{The}} First International Conference on Image Formation in {{X}}-Ray {{Computed Tomography}}},
  author = {Knaup, Michael and {Maa$\backslash$s s}, Clemens and Sawall, Stefan and {Kachelrie$\backslash$s s}, Marc},
  year = {2010},
  pages = {194--199},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/7KAZZBQJ/ROITomography_CTMeeting2010.pdf}
}

@article{muller_evaluation_2013,
  title = {Evaluation of Interpolation Methods for Surface-Based Motion Compensated Tomographic Reconstruction for Cardiac Angiographic {{C}}-Arm Data},
  volume = {40},
  issn = {0094-2405},
  doi = {10.1118/1.4789593},
  abstract = {Purpose: For interventional cardiac procedures, anatomical and functional information about the cardiac chambers is of major interest. With the technology of angiographic C-arm systems it is possible to reconstruct intraprocedural three-dimensional (3D) images from 2D rotational angiographic projection data (C-arm CT). However, 3D reconstruction of a dynamic object is a fundamental problem in C-arm CT reconstruction. The 2D projections are acquired over a scan time of several seconds, thus the projection data show different states of the heart. A standard FDK reconstruction algorithm would use all acquired data for a filtered backprojection and result in a motion-blurred image. In this approach, a motion compensated reconstruction algorithm requiring knowledge of the 3D heart motion is used. The motion is estimated from a previously presented 3D dynamic surface model. This dynamic surface model results in a sparse motion vector field (MVF) defined at control points. In order to perform a motion compensated reconstruction, a dense motion vector field is required. The dense MVF is generated by interpolation of the sparse MVF. Therefore, the influence of different motion interpolation methods on the reconstructed image quality is evaluated.Methods: Four different interpolation methods, thin-plate splines (TPS), Shepard's method, a smoothed weighting function, and a simple averaging, were evaluated. The reconstruction quality was measured on phantom data, a porcine model as well as on in vivo clinical data sets. As a quality index, the 2D overlap of the forward projected motion compensated reconstructed ventricle and the segmented 2D ventricle blood pool was quantitatively measured with the Dice similarity coefficient and the mean deviation between extracted ventricle contours. For the phantom data set, the normalized root mean square error (nRMSE) and the universal quality index (UQI) were also evaluated in 3D image space.Results: The quantitative evaluation of all experiments showed that TPS interpolation provided the best results. The quantitative results in the phantom experiments showed comparable nRMSE of $\approx$0.047 $\pm$ 0.004 for the TPS and Shepard's method. Only slightly inferior results for the smoothed weighting function and the linear approach were achieved. The UQI resulted in a value of $\approx$ 99\% for all four interpolation methods. On clinical human data sets, the best results were clearly obtained with the TPS interpolation. The mean contour deviation between the TPS reconstruction and the standard FDK reconstruction improved in the three human cases by 1.52, 1.34, and 1.55 mm. The Dice coefficient showed less sensitivity with respect to variations in the ventricle boundary.Conclusions: In this work, the influence of different motion interpolation methods on left ventricle motion compensated tomographic reconstructions was investigated. The best quantitative reconstruction results of a phantom, a porcine, and human clinical data sets were achieved with the TPS approach. In general, the framework of motion estimation using a surface model and motion interpolation to a dense MVF provides the ability for tomographic reconstruction using a motion compensation technique.},
  language = {ENG},
  number = {3},
  journal = {Medical physics},
  author = {M{\"u}ller, Kerstin and Schwemmer, Chris and Hornegger, Joachim and Zheng, Yefeng and Wang, Yang and Lauritsch, G{\"u}nter and Rohkohl, Christopher and Maier, Andreas K and Schultz, Carl and Fahrig, Rebecca},
  month = mar,
  year = {2013},
  pages = {031107},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/U5GGTSPB/MuÌˆller et al. - 2013 - Evaluation of interpolation methods for surface-ba.pdf},
  pmid = {23464287}
}

@article{nien_fast_2015,
  title = {Fast {{X}}-{{Ray CT Image Reconstruction Using}} a {{Linearized Augmented Lagrangian Method With Ordered Subsets}}},
  volume = {34},
  issn = {0278-0062, 1558-254X},
  doi = {10.1109/TMI.2014.2358499},
  number = {2},
  journal = {IEEE Transactions on Medical Imaging},
  author = {Nien, Hung and Fessler, Jeffrey A.},
  month = feb,
  year = {2015},
  pages = {388--399},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/I4AHSF7D/Nien and Fessler - 2015 - Fast X-Ray CT Image Reconstruction Using a Lineari.pdf}
}

@article{brehm_self-adapting_2012,
  title = {Self-Adapting Cyclic Registration for Motion-Compensated Cone-Beam {{CT}} in Image-Guided Radiation Therapy},
  volume = {39},
  issn = {0094-2405},
  doi = {10.1118/1.4766435},
  abstract = {Purpose: In image-guided radiation therapy an additional kV imaging system next to the linear particle accelerator provides information for an accurate patient positioning. However, the acquisition time of the system is much longer than the patient\&apos;s breathing cycle due to the low gantry rotation speed. Our purpose is a cyclic registration in the context of motion-compensated image reconstruction that provides high quality respiratory-correlated 4D volumes for on-board flat panel detector cone-beam CT scans. Methods: Based on the small motion assumption, widely used within registration algorithms, a strategy is developed for motion estimation. In this strategy temporal restrictions are incorporated, for example, the cyclic motion patterns of respiration. The resultant cyclic registration method is to show less sensitivity on image artifacts, in particular on artifacts due to projection data sparsification. Using a new cyclic registration method a motion estimation is performed on respiratory-correlated reconstructions, and the obtained motion vector fields are used for motion compensation. Results: The proposed cyclic registration is evaluated in the context of motion-compensated image reconstruction using simulated data and patient data. Motion artifacts of 3D standard reconstructions can be significantly reduced by the resulting cyclic motion compensation. The method outperforms the respiratory-correlated reconstructions regarding sparse-view artifacts and maintains the high temporal resolution at the same time. Image artifacts show only minor to almost no effect on the motion estimation using the cyclic registration. Conclusions: The cyclic motion compensation approach provides respiratory-correlated volumes with high image quality. The cyclic motion estimation is of such low sensitivity to sparse-view artifacts, that it is capable to determine high quality motion vector fields based on reconstructions of low sampled data.},
  number = {12},
  journal = {Medical Physics},
  author = {Brehm, Marcus and Paysan, Pascal and Oelhafen, Markus and Kunz, Patrik and Kachelrie{\ss}, Marc},
  month = dec,
  year = {2012},
  keywords = {Medical image artifacts,Medical image reconstruction,Medical imaging,Motion estimation,Vector fields},
  pages = {7603--7618},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/9Z6HMGEU/Brehm et al. - 2012 - Self-adapting cyclic registration for motion-compe.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/NJXBE9AM/1.html}
}

@article{bian_evaluation_2010,
  title = {Evaluation of Sparse-View Reconstruction from Flat-Panel-Detector Cone-Beam {{CT}}},
  volume = {55},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/55/22/001},
  abstract = {Flat-panel-detector x-ray cone-beam computed tomography (CBCT) is used in a rapidly increasing host of imaging applications, including image-guided surgery and radiotherapy. The purpose of the work is to investigate and evaluate image reconstruction from data collected at projection views significantly fewer than what is used in current CBCT imaging. Specifically, we carried out imaging experiments using a bench-top CBCT system that was designed to mimic imaging conditions in image-guided surgery and radiotherapy; we applied an image reconstruction algorithm based on constrained total-variation (TV)-minimization to data acquired with sparsely sampled view-angles and conducted extensive evaluation of algorithm performance. Results of the evaluation studies demonstrate that, depending upon scanning conditions and imaging tasks, algorithms based on constrained TV-minimization can reconstruct images of potential utility from a small fraction of the data used in typical, current CBCT applications. A practical implication of the study is that the optimization of algorithm design and implementation can be exploited for considerably reducing imaging effort and radiation dose in CBCT.},
  language = {en},
  number = {22},
  journal = {Physics in Medicine and Biology},
  author = {Bian, Junguo and Siewerdsen, Jeffrey H. and Han, Xiao and Sidky, Emil Y. and Prince, Jerry L. and Pelizzari, Charles A. and Pan, Xiaochuan},
  month = nov,
  year = {2010},
  pages = {6575},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/52CI2CQG/Bian et al. - 2010 - Evaluation of sparse-view reconstruction from flat.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/75HWVBM6/001.html}
}

@article{henry_stark_investigation_1981,
  title = {An {{Investigation}} of {{Computerized Tomography}} by {{Direct Fourier Inversion}} and {{Optimum Interpolation}}},
  issn = {0018-9294},
  doi = {10.1109/TBME.1981.324736},
  number = {7},
  journal = {Biomedical Engineering, IEEE Transactions on},
  author = {Henry Stark, John W. Woods},
  year = {1981},
  pages = {496 -- 505},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/USE9M2GF/3037472_An_Investigation_of_Computerized_Tomography_by_Direct_Fourier_Inversion_and_Optimum_Int.html}
}

@article{lefkimmiatis_hessian-based_2012,
  title = {Hessian-{{Based Norm Regularization}} for {{Image Restoration With Biomedical Applications}}},
  volume = {21},
  issn = {1057-7149},
  doi = {10.1109/TIP.2011.2168232},
  abstract = {We present nonquadratic Hessian-based regularization methods that can be effectively used for image restoration problems in a variational framework. Motivated by the great success of the total-variation (TV) functional, we extend it to also include second-order differential operators. Specifically, we derive second-order regularizers that involve matrix norms of the Hessian operator. The definition of these functionals is based on an alternative interpretation of TV that relies on mixed norms of directional derivatives. We show that the resulting regularizers retain some of the most favorable properties of TV, i.e., convexity, homogeneity, rotation, and translation invariance, while dealing effectively with the staircase effect. We further develop an efficient minimization scheme for the corresponding objective functions. The proposed algorithm is of the iteratively reweighted least-square type and results from a majorization-minimization approach. It relies on a problem-specific preconditioned conjugate gradient method, which makes the overall minimization scheme very attractive since it can be applied effectively to large images in a reasonable computational time. We validate the overall proposed regularization framework through deblurring experiments under additive Gaussian noise on standard and biomedical images.},
  number = {3},
  journal = {IEEE Transactions on Image Processing},
  author = {Lefkimmiatis, S. and Bourquard, A. and Unser, M.},
  year = {2012},
  keywords = {AWGN,Algorithms,Biomedical imaging,Eigenvalues and eigenfunctions,Frobenius norm,Gaussian noise,Hessian matrices,Hessian matrix,Hessian-based norm regularization,Image Enhancement,Image Interpretation; Computer-Assisted,Laplace equations,Minimization,Pattern Recognition; Automated,TV,additive Gaussian noise,biomedical applications,biomedical images,conjugate gradient method,conjugate gradient methods,deblurring experiments,homogeneity,image deblurring,image restoration,iteratively reweighted least squares,least squares approximations,linear inverse problems,majorization-minimization approach,majorizationâ€“minimization (MM) algorithms,medical image processing,minimisation,minimization scheme,second-order differential operators,second-order regularizers,spectral norm,total-variation functional,translation invariance},
  pages = {983--995},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/VD7DK78E/login.html}
}

@article{wang_high-quality_2013,
  title = {High-Quality Four-Dimensional Cone-Beam {{CT}} by Deforming Prior Images},
  volume = {58},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/58/2/231},
  abstract = {Due to a limited number of projections at each phase, severe view aliasing artifacts are present in four-dimensional cone beam computed tomography (4D-CBCT) when reconstruction is performed using conventional algorithms. In this work, we aim to obtain high-quality 4D-CBCT of lung cancer patients in radiation therapy by deforming the planning CT. The deformation vector fields (DVF) to deform the planning CT are estimated through matching the forward projection of the deformed prior image and measured on-treatment CBCT projection. The estimation of the DVF is formulated as an unconstrained optimization problem, where the objective function to be minimized is the sum of the squared difference between the forward projection of the deformed planning CT and the measured 4D-CBCT projection. A nonlinear conjugate gradient method is used to solve the DVF. As the number of the variables in the DVF is much greater than the number of measurements, the solution to such a highly ill-posed problem is very sensitive to the initials during the optimization process. To improve the estimation accuracy of DVF, we proposed a new strategy to obtain better initials for the optimization. In this strategy, 4D-CBCT is first reconstructed by total variation minimization. Demons deformable registration is performed to register the planning CT and the 4D-CBCT reconstructed by total variation minimization. The resulted DVF from demons registration is then used as the initial parameters in the optimization process. A 4D nonuniform rotational B-spline-based cardiac-torso (NCAT) phantom and a patient 4D-CBCT are used to evaluate the algorithm. Image quality of 4D-CBCT is substantially improved by using the proposed strategy in both NCAT phantom and patient studies. The proposed method has the potential to improve the temporal resolution of 4D-CBCT. Improved 4D-CBCT can better characterize the motion of lung tumors and will be a valuable tool for image-guided adaptive radiation therapy.},
  language = {en},
  number = {2},
  journal = {Physics in Medicine and Biology},
  author = {Wang, Jing and Gu, Xuejun},
  month = jan,
  year = {2013},
  pages = {231},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/PJ2GCKST/231.html}
}

@article{bian_optimization-based_2013,
  title = {Optimization-Based Image Reconstruction from Sparse-View Data in Offset-Detector {{CBCT}}},
  volume = {58},
  issn = {0031-9155, 1361-6560},
  doi = {10.1088/0031-9155/58/2/205},
  number = {2},
  journal = {Physics in Medicine and Biology},
  author = {Bian, Junguo and Wang, Jiong and Han, Xiao and Sidky, Emil Y and Shao, Lingxiong and Pan, Xiaochuan},
  month = jan,
  year = {2013},
  pages = {205--230},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/E48P7ZMU/Bian et al. - 2013 - Optimization-based image reconstruction from spars.pdf}
}

@inproceedings{lauritsch_towards_2005,
  title = {Towards Cardiac Angiographic Computed Tomography},
  volume = {4},
  booktitle = {Nuclear {{Science Symposium Conference Record}}, 2005 {{IEEE}}},
  author = {Lauritsch, G{\"u}nter and Boese, Jan and Kemeth, Herbert},
  year = {2005},
  pages = {2350--2354},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/QX2VZ67R/Lauritsch et al. - 2005 - Towards cardiac angiographic computed tomography.pdf}
}

@incollection{jia_4d_2010,
  series = {Lecture Notes in Computer Science},
  title = {{{4D Computed Tomography Reconstruction}} from {{Few}}-{{Projection Data}} via {{Temporal Non}}-Local {{Regularization}}},
  copyright = {\textcopyright{}2010 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-642-15704-2 978-3-642-15705-9},
  abstract = {4D computed tomography (4D-CT) is an important modality in medical imaging due to its ability to resolve patient anatomy motion in each respiratory phase. Conventionally 4D-CT is accomplished by performing the reconstruction for each phase independently as in a CT reconstruction problem. We propose a new 4D-CT reconstruction algorithm that explicitly takes into account the temporal regularization in a non-local fashion. By imposing a regularization of a temporal non-local means (TNLM) form, 4D-CT images at all phases can be reconstructed simultaneously based on extremely under-sampled x-ray projections. Our algorithm is validated in one digital NCAT thorax phantom and two real patient cases. It is found that our TNLM algorithm is capable of reconstructing the 4D-CT images with great accuracy. The experiments also show that our approach outperforms standard 4D-CT reconstruction methods with spatial regularization of total variation or tight frames.},
  number = {6361},
  booktitle = {Medical {{Image Computing}} and {{Computer}}-{{Assisted Intervention}} \textendash{} {{MICCAI}} 2010},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Jia, Xun and Lou, Yifei and Dong, Bin and Tian, Zhen and Jiang, Steve},
  editor = {Jiang, Tianzi and Navab, Nassir and Pluim, Josien P. W. and Viergever, Max A.},
  month = jan,
  year = {2010},
  keywords = {Computer Graphics,Computer Imaging; Vision; Pattern Recognition and Graphics,Image Processing and Computer Vision,Pattern Recognition,Simulation and Modeling,User Interfaces and Human Computer Interaction},
  pages = {143--150},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/S3MDQHWK/Jia et al. - 2010 - 4D Computed Tomography Reconstruction from Few-Pro.pdf}
}

@article{ramirez-giraldo_nonconvex_2011,
  title = {Nonconvex Prior Image Constrained Compressed Sensing ({{NCPICCS}}): {{Theory}} and Simulations on Perfusion {{CT}}},
  volume = {38},
  copyright = {\textcopyright{} 2011 American Association of Physicists in Medicine},
  shorttitle = {Nonconvex Prior Image Constrained Compressed Sensing ({{NCPICCS}})},
  doi = {10.1118/1.3560878},
  abstract = {Purpose: To present and evaluate a new image reconstruction method for dynamic CT based on a nonconvex prior image constrained compressed sensing (NCPICCS) algorithm. The authors systematically compared the undersampling potential, functional information recovery, and solution convergence speed of four compressed sensing (CS) based image reconstruction methods using perfusion CT data: Standard $\mathscr{l}$1-based CS, nonconvex CS (NCCS), and $\mathscr{l}$1-based and nonconvex CS, including an additional constraint based on a prior image (PICCS and NCPICCS, respectively).
Methods: The Shepp\textendash{}Logan phantom was modified such that its uppermost ellipses changed attenuation through time, simulating both an arterial input function (AIF) and a homogeneous tissue perfusion region. Data were simulated with and without Poisson noise added to the projection data and subsequently reconstructed with all four CS-based methods at four levels of undersampling: 20, 12, 6, and 4 projections. Root mean squared (RMS) error of reconstructed images and recovered time attenuation curves (TACs) were assessed as well as convergence speed. The performance of both PICCS and NCPICCS methods were also evaluated using a kidney perfusion animal experiment data set.
Results: All four CS-based methods were able to reconstruct the phantoms with 20 projections, with similar results on the RMS error of the recovered TACs. NCCS allowed accurate reconstructions with as few as 12 projections, PICCS with as few as six projections, and NCPICCS with as few as four projections. These results were consistent for noise-free and noisy data. NCPICCS required the fewest iterations to converge across all simulation conditions, followed by PICCS, NCCS, and then CS. On animal data, at the lowest level of undersampling tested (16 projections), the image quality of NCPICCS was better than PICCS with fewer streaking artifacts, while the TAC accuracy on the selected region of interest was comparable.
Conclusions: The authors have presented a novel method for image reconstruction using highly undersampled dynamic CT data. The NCPICCS method takes advantage of the information provided by a prior image, as in PICCS, but employs a more general nonconvex sparsity measure [such as the $\mathscr{l}$p-norm (0$<$p $\leq$ 1)] rather than the conventional convex $\mathscr{l}$1-norm. Despite the lack of guarantees of a globally optimal solution, the proposed nonconvex extension of PICCS consistently allowed for image reconstruction from fewer samples than the analogous $\mathscr{l}$1-based PICCS method. Both nonconvex sparsity measures as well as prior image information (when available) significantly reduced the number of iterations required for convergence, potentially providing computational advantages for practical implementation of CS-based image reconstruction techniques.},
  number = {4},
  journal = {Medical Physics},
  author = {Ramirez-Giraldo, J. C. and Trzasko, J. and Leng, S. and Yu, L. and Manduca, A. and McCollough, C. H.},
  year = {2011},
  keywords = {Image reconstruction,blood vessels,computerised tomography,kidney,medical image processing,noise,phantoms},
  pages = {2157--2167},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/UIEFBMP8/Ramirez-Giraldo et al. - 2011 - Nonconvex prior image constrained compressed sensi.pdf}
}

@article{beck_fast_2009,
  title = {Fast Gradient-Based Algorithms for Constrained Total Variation Image Denoising and Deblurring Problems},
  volume = {18},
  number = {11},
  journal = {Image Processing, IEEE Transactions on},
  author = {Beck, Amir and Teboulle, Marc},
  year = {2009},
  pages = {2419--2434},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/ZRBSPK5S/Beck and Teboulle - 2009 - Fast gradient-based algorithms for constrained tot.pdf}
}

@article{bauschke_baillon-haddad_2009,
  title = {The {{Baillon}}-{{Haddad}} Theorem Revisited},
  journal = {arXiv preprint arXiv:0906.0807},
  author = {Bauschke, Heinz H. and Combettes, Patrick L.},
  year = {2009},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/V53FK5QK/Bauschke and Combettes - 2009 - The Baillon-Haddad theorem revisited.pdf}
}

@article{schmitt_reconstruction_2005,
  title = {Reconstruction of Blood Propagation in Three-Dimensional Rotational {{X}}-Ray Angiography ({{3D}}-{{RA}})},
  volume = {29},
  number = {7},
  journal = {Computerized Medical Imaging and Graphics},
  author = {Schmitt, H. and Grass, M. and Suurmond, R. and K{\"o}hler, T. and Rasche, V. and H{\"a}hnel, S. and Heiland, S.},
  year = {2005},
  pages = {507--520},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/76D48F8P/Schmitt et al. - 2005 - Reconstruction of blood propagation in three-dimen.pdf}
}

@article{delmon_registration_2013,
  title = {Registration of Sliding Objects Using Direction Dependent {{B}}-Splines Decomposition},
  volume = {58},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/58/5/1303},
  abstract = {Sliding motion is a challenge for deformable image registration because it leads to discontinuities in the sought deformation. In this paper, we present a method to handle sliding motion using multiple B-spline transforms. The proposed method decomposes the sought deformation into sliding regions to allow discontinuities at their interfaces, but prevents unrealistic solutions by forcing those interfaces to match. The method was evaluated on 16 lung cancer patients against a single B-spline transform approach and a multi B-spline transforms approach without the sliding constraint at the interface. The target registration error (TRE) was significantly lower with the proposed method (TRE = 1.5 mm) than with the single B-spline approach (TRE = 3.7 mm) and was comparable to the multi B-spline approach without the sliding constraint (TRE = 1.4 mm). The proposed method was also more accurate along region interfaces, with 37\% less gaps and overlaps when compared to the multi B-spline transforms without the sliding constraint.},
  language = {en},
  number = {5},
  journal = {Physics in Medicine and Biology},
  author = {Delmon, V. and Rit, S. and Pinho, R. and Sarrut, D.},
  month = mar,
  year = {2013},
  pages = {1303},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/VKES3847/1303.html}
}

@article{i_b_mihaylov_modeling_2008,
  title = {Modeling of Carbon Fiber Couch Attenuation Properties with a Commercial Treatment Planning System},
  volume = {35},
  issn = {0094-2405},
  doi = {10.1118/1.2982135},
  number = {11},
  journal = {Medical physics},
  author = {I B Mihaylov, P. Corry},
  year = {2008},
  pages = {4982--8},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/ZCGZHERF/23651138_Modeling_of_carbon_fiber_couch_attenuation_properties_with_a_commercial_treatment_plan.html}
}

@article{chan_digital_2001,
  title = {The Digital {{TV}} Filter and Nonlinear Denoising},
  volume = {10},
  number = {2},
  journal = {Image Processing, IEEE Transactions on},
  author = {Chan, Tony F. and Osher, Stanley and Shen, Jianhong},
  year = {2001},
  pages = {231--241},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/TV2N7MSA/Chan et al. - 2001 - The digital TV filter and nonlinear denoising.pdf}
}

@article{kachelriess_kymogram_2002,
  title = {Kymogram Detection and Kymogram-Correlated Image Reconstruction from Subsecond Spiral Computed Tomography Scans of the Heart},
  volume = {29},
  issn = {0094-2405},
  abstract = {Subsecond single-slice, multi-slice or cone-beam spiral computed tomography (SSCT, MSCT, CBCT) offer great potential for improving heart imaging. Together with the newly developed phase-correlated cardiac reconstruction algorithms 180 degrees MCD and 180 degrees MCI [Med. Phys. 27, 1881-1902 (2000)] or related algorithms provided by the CT manufacturers, high image quality can be achieved. These algorithms require information about the cardiac motion, i.e., typically the simultaneously recorded electrocardiogram (ECG), to synchronize the reconstruction with the cardiac motion. Neither data acquired without ECG information (standard patients) nor acquisitions with corrupted ECG information can be handled adequately. We developed a method to extract the appropriate information about cardiac motion directly from the measured raw data (projection data). The so-called kymogram function is a measure of the cardiac motion as a function of time t or as a function of the projection angle alpha. In contrast to the ECG which is a global measure of the heart's electric excitation, the kymogram is a local measure of the heart motion at the z-position z(a) at projection angle a. The patient's local heart rate as well as the necessary synchronization information to be used with phase-correlated algorithms can be extracted from the kymogram by using a series of signal processing steps. The kymogram information is shown to be adequate to substitute the ECG information. Computer simulations with simulated ECG and patient measurements with simultaneously acquired ECG were carried out for a multislice scanner providing M = 4 slices to evaluate these new approaches. Both the ECG function and the kymogram function were used for reconstruction. Both were highly correlated regarding the periodicity information used for reconstruction. In 21 out of 25 consecutive cases the kymogram approach was equivalent to the ECG-correlated reconstruction; only minor differences in image quality between both methods were observed. For one patient the synchronization information detected by the ECG monitor turned out to be wrong; here, the kymogram constituted the only approach that provided useful reconstructions. Patient studies with 12 and 16 slices indicate the usefulness of our approach for cone-beam CT scans. Kymogram-correlated reconstructions also appear to have the potential to improve imaging of pericardial lung areas in general.},
  language = {eng},
  number = {7},
  journal = {Medical physics},
  author = {Kachelriess, Marc and Sennst, Dirk-Alexander and Maxlmoser, Wolfgang and Kalender, Willi A},
  month = jul,
  year = {2002},
  keywords = {Algorithms,Heart,Heart Rate,Humans,Image Processing; Computer-Assisted,Models; Theoretical,Movement,Time Factors,Tomography; X-Ray Computed},
  pages = {1489--1503},
  pmid = {12148730}
}

@book{wolberg_digital_1990,
  address = {Los Alamitos, Calif},
  title = {Digital {{Image Warping}}},
  isbn = {978-0-8186-8944-4},
  abstract = {This best\textendash{}selling, original text focuses on image reconstruction, real\textendash{}time texture mapping, separable algorithms, two\textendash{}pass transforms, mesh warping, and special effects. The text, containing all original material, begins with the history of the field and continues with a review of common terminology, mathematical preliminaries, and digital image acquisition. Later chapters discuss equations for spatial information, interpolation kernels, filtering problems, and fast\textendash{}warping techniques based on scanline algorithms.},
  language = {English},
  publisher = {{Wiley-Blackwell}},
  author = {Wolberg, George},
  month = jul,
  year = {1990}
}

@article{martin_complementary_2007,
  title = {Complementary and Alternative Medicine: Opportunities and Challenges},
  volume = {34},
  shorttitle = {Complementary and Alternative Medicine},
  number = {2},
  journal = {Journal of Health Care Finance},
  author = {Martin, W.M. and Long, H.},
  year = {2007},
  pages = {89--104},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/U743URVW/Martin and Long - 2007 - Complementary and alternative medicine opportunit.pdf}
}

@article{kuntz_constrained_2013,
  title = {Constrained Reconstructions for {{4D}} Intervention Guidance},
  volume = {58},
  issn = {0031-9155, 1361-6560},
  doi = {10.1088/0031-9155/58/10/3283},
  number = {10},
  journal = {Physics in Medicine and Biology},
  author = {Kuntz, J and Flach, B and Kueres, R and Semmler, W and Kachelrie{\ss}, M and Bartling, S},
  month = may,
  year = {2013},
  pages = {3283--3300},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/UMKSGJTV/Kuntz et al. - 2013 - Constrained reconstructions for 4D intervention gu.pdf}
}

@article{ming_jiang_convergence_2003,
  title = {Convergence of the Simultaneous Algebraic Reconstruction Technique ({{SART}}).},
  volume = {12},
  issn = {1057-7149},
  doi = {10.1109/TIP.2003.815295},
  number = {8},
  journal = {IEEE transactions on image processing : a publication of the IEEE Signal Processing Society},
  author = {Ming Jiang, Ge Wang},
  year = {2003},
  pages = {957--61},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/NA3DNPJW/Ming Jiang - 2003 - Convergence of the simultaneous algebraic reconstr.pdf}
}

@article{hoskovec_exact_2016,
  title = {Exact {{Fan}}-{{Beam Reconstruction With Arbitrary Object Translations}} and {{Truncated Projections}}},
  volume = {63},
  issn = {0018-9499},
  doi = {10.1109/TNS.2016.2519242},
  abstract = {This article proposes a new method for reconstructing two-dimensional (2D) computed tomography (CT) images from truncated and motion contaminated sinograms. The type of motion considered here is a sequence of rigid translations which are assumed to be known. The algorithm first identifies the sufficiency of angular coverage in each 2D point of the CT image to calculate the Hilbert transform from the local ``virtual'' trajectory which accounts for the motion and the truncation. By taking advantage of data redundancy in the full circular scan, our method expands the reconstructible region beyond the one obtained with chord-based methods. The proposed direct reconstruction algorithm is based on the Differentiated Back-Projection with Hilbert filtering (DBP-H). The motion is taken into account during backprojection which is the first step of our direct reconstruction, before taking the derivatives and inverting the finite Hilbert transform. The algorithm has been tested in a proof-of-concept study on Shepp-Logan phantom simulations with several motion cases and detector sizes.},
  number = {3},
  journal = {IEEE Transactions on Nuclear Science},
  author = {Hoskovec, J. and Clackdoyle, R. and Desbat, L. and Rit, S.},
  month = jun,
  year = {2016},
  keywords = {Computed tomography,Image reconstruction,Motion compensation,Reconstruction algorithms},
  pages = {1408--1418},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/W4P7VQAJ/Hoskovec et al. - 2016 - Exact Fan-Beam Reconstruction With Arbitrary Objec.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/5VMFKV9U/7478114.html}
}

@article{mc_kinnon_towards_1981,
  title = {Towards Imaging the Beating Heart Usefully with a Conventional {{CT}} Scanner},
  volume = {28},
  issn = {0018-9294},
  doi = {10.1109/TBME.1981.324785},
  number = {2},
  journal = {IEEE Transactions on Bio-Medical Engineering},
  author = {Mc Kinnon, G C and Bates, R H},
  month = feb,
  year = {1981},
  keywords = {Heart,Tomography; X-Ray Computed},
  pages = {123--127},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/M4Q6HT7C/Mc Kinnon et Bates - 1981 - Towards imaging the beating heart usefully with a .pdf},
  pmid = {7287019}
}

@article{stevendaal_motion-compensated_2008,
  title = {A Motion-Compensated Scheme for Helical Cone-Beam Reconstruction in Cardiac {{CT}} Angiography},
  volume = {35},
  copyright = {\textcopyright{} 2008 American Association of Physicists in Medicine},
  doi = {10.1118/1.2938733},
  abstract = {Since coronary heart disease is one of the main causes of death all over the world, cardiac computed tomography (CT) imaging is an application of very high interest in order to verify indications timely. Due to the cardiac motion, electrocardiogram (ECG) gating has to be implemented into the reconstruction of the measured projection data. However, the temporal and spatial resolution is limited due to the mechanical movement of the gantry and due to the fact that a finite angular span of projections has to be acquired for the reconstruction of each voxel. In this article, a motion-compensated reconstruction method for cardiac CT is described, which can be used to increase the signal-to-noise ratio or to suppress motion blurring. Alternatively, it can be translated into an improvement of the temporal and spatial resolution. It can be applied to the entire heart in common and to high contrast objects moving with the heart in particular, such as calcified plaques or devices like stents. The method is based on three subsequent steps: As a first step, the projection data acquired in low pitch helical acquisition mode together with the ECG are reconstructed at multiple phase points. As a second step, the motion-vector field is calculated from the reconstructed images in relation to the image in a reference phase. Finally, a motion-compensated reconstruction is carried out for the reference phase using those projections, which cover the cardiac phases for which the motion-vector field has been determined.},
  number = {7},
  journal = {Medical Physics},
  author = {van Stevendaal, U. and von Berg, J. and Lorenz, C. and Grass, M.},
  year = {2008},
  keywords = {Electrocardiography,Image reconstruction,Image resolution,Motion compensation,angiocardiography,computerised tomography,medical image processing},
  pages = {3239--3251},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/VT92GD5D/p3239_s1.html}
}

@inproceedings{wee_reversing_1998,
  title = {Reversing Motion Vector Fields},
  volume = {2},
  booktitle = {Image {{Processing}}, 1998. {{ICIP}} 98. {{Proceedings}}. 1998 {{International Conference}} On},
  publisher = {{IEEE}},
  author = {Wee, Susie J.},
  year = {1998},
  pages = {209--212},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/DX3XAXP5/wee1998.pdf}
}

@article{zinsser_systematic_2013,
  title = {Systematic Performance Optimization of Cone-Beam Back-Projection on the {{Kepler}} Architecture},
  journal = {Proceedings of the 12th Fully Three-Dimensional Image Reconstruction in Radiology and Nuclear Medicine},
  author = {Zinsser, Timo and Keck, Benjamin},
  year = {2013},
  pages = {225--228},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/XHKDVZ7U/Zinsser13-SPO.pdf}
}

@techreport{adant_conception_,
  title = {Conception et R{\'e}alisation d'un Fant{\^o}me Dynamique de Thorax Pour La Mise Au Point d'un Syst{\`e}me {{4D Cone Beam CT}}},
  author = {Adant, Beno{\^\i}t and Degaillier, Adrien},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/ED9GZMWM/Adant and Degaillier - Conception et rÃ©alisation dâ€™un fantÃ´me dynamique d.pdf}
}

@article{black_robust_1998,
  title = {Robust Anisotropic Diffusion},
  volume = {7},
  number = {3},
  journal = {Image Processing, IEEE Transactions on},
  author = {Black, Michael J. and Sapiro, Guillermo and Marimont, David H. and Heeger, David},
  year = {1998},
  pages = {421--432},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/UP5E7GQ2/Black et al. - 1998 - Robust anisotropic diffusion.pdf}
}

@book{press_numerical_1988,
  address = {New York, NY, USA},
  title = {Numerical Recipes in {{C}}: The Art of Scientific Computing},
  isbn = {0-521-35465-X},
  shorttitle = {Numerical Recipes in {{C}}},
  publisher = {{Cambridge University Press}},
  author = {Press, William H. and Flannery, Brian P. and Teukolsky, Saul A. and Vetterling, William T.},
  year = {1988}
}

@article{boykov_fast_2001,
  title = {Fast Approximate Energy Minimization via Graph Cuts},
  volume = {23},
  number = {11},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  author = {Boykov, Yuri and Veksler, Olga and Zabih, Ramin},
  year = {2001},
  pages = {1222--1239},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/6MHISMCH/Boykov et al. - 2001 - Fast approximate energy minimization via graph cut.pdf}
}

@article{sawall_imaging_2012,
  title = {Imaging of Cardiac Perfusion of Free-Breathing Small Animals Using Dynamic Phase-Correlated Micro-{{CT}}},
  volume = {39},
  issn = {0094-2405},
  doi = {10.1118/1.4762685},
  abstract = {PURPOSE: Mouse models of cardiac diseases have proven to be a valuable tool in preclinical research. The high cardiac and respiratory rates of free breathing mice prohibit conventional in vivo cardiac perfusion studies using computed tomography even if gating methods are applied. This makes a sacrification of the animals unavoidable and only allows for the application of ex vivo methods.
METHODS: To overcome this issue the authors propose a low dose scan protocol and an associated reconstruction algorithm that allows for in vivo imaging of cardiac perfusion and associated processes that are retrospectively synchronized to the respiratory and cardiac motion of the animal. The scan protocol consists of repetitive injections of contrast media within several consecutive scans while the ECG, respiratory motion, and timestamp of contrast injection are recorded and synchronized to the acquired projections. The iterative reconstruction algorithm employs a six-dimensional edge-preserving filter to provide low-noise, motion artifact-free images of the animal examined using the authors' low dose scan protocol.
RESULTS: The reconstructions obtained show that the complete temporal bolus evolution can be visualized and quantified in any desired combination of cardiac and respiratory phase including reperfusion phases. The proposed reconstruction method thereby keeps the administered radiation dose at a minimum and thus reduces metabolic inference to the animal allowing for longitudinal studies.
CONCLUSIONS: The authors' low dose scan protocol and phase-correlated dynamic reconstruction algorithm allow for an easy and effective way to visualize phase-correlated perfusion processes in routine laboratory studies using free-breathing mice.},
  language = {eng},
  number = {12},
  journal = {Medical physics},
  author = {Sawall, Stefan and Kuntz, Jan and Socher, Michaela and Knaup, Michael and Hess, Andreas and Bartling, Sonke and Kachelrie{\ss}, Marc},
  month = dec,
  year = {2012},
  keywords = {Animals,Blood Flow Velocity,Cardiac-Gated Imaging Techniques,Coronary Circulation,Coronary Vessels,Mice,Myocardial Perfusion Imaging,Reproducibility of Results,Respiratory Mechanics,Respiratory-Gated Imaging Techniques,Sensitivity and Specificity,X-Ray Microtomography},
  pages = {7499--7506},
  pmid = {23231299}
}

@article{shepp_maximum_1982,
  title = {Maximum Likelihood Reconstruction for Emission Tomography},
  volume = {1},
  issn = {0278-0062},
  doi = {10.1109/TMI.1982.4307558},
  abstract = {Previous models for emission tomography (ET) do not distinguish the physics of ET from that of transmission tomography. We give a more accurate general mathematical model for ET where an unknown emission density lambda = lambda(x, y, z) generates, and is to be reconstructed from, the number of counts n(*)(d) in each of D detector units d. Within the model, we give an algorithm for determining an estimate lambdainsertion mark of lambda which maximizes the probability p(n(*)|lambda) of observing the actual detector count data n(*) over all possible densities lambda. Let independent Poisson variables n(b) with unknown means lambda(b), b = 1, ..., B represent the number of unobserved emissions in each of B boxes (pixels) partitioning an object containing an emitter. Suppose each emission in box b is detected in detector unit d with probability p(b, d), d = 1, ..., D with p(b,d) a one-step transition matrix, assumed known. We observe the total number n(*) = n(*)(d) of emissions in each detector unit d and want to estimate the unknown lambda = lambda(b), b = 1, ..., B. For each lambda, the observed data n(*) has probability or likelihood p(n(*)|lambda). The EM algorithm of mathematical statistics starts with an initial estimate lambda(0) and gives the following simple iterative procedure for obtaining a new estimate lambdainsertion mark(new), from an old estimate lambdainsertion mark(old), to obtain lambdainsertion mark(k), k = 1, 2, ..., lambdainsertion mark(new)(b)= lambdainsertion mark(old)(b)Sum of (n(*)p(b,d) from d=1 to D/Sum of lambdainsertion mark()old(b('))p(b('),d) from b(')=1 to B), b=1,...B.},
  number = {2},
  journal = {IEEE Transactions on Medical Imaging},
  author = {Shepp, L A and Vardi, Y},
  year = {1982},
  pages = {113--122},
  pmid = {18238264}
}

@inproceedings{boykov_interactive_2001,
  title = {Interactive Graph Cuts for Optimal Boundary \& Region Segmentation of Objects in {{ND}} Images},
  volume = {1},
  booktitle = {Computer {{Vision}}, 2001. {{ICCV}} 2001. {{Proceedings}}. {{Eighth IEEE International Conference}} On},
  author = {Boykov, Yuri Y. and Jolly, M.-P.},
  year = {2001},
  pages = {105--112},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/KPCTZFGZ/Boykov and Jolly - 2001 - Interactive graph cuts for optimal boundary & regi.pdf}
}

@article{cho_cone-beam_1996,
  title = {Cone-Beam {{CT}} from Width-Truncated Projections},
  volume = {20},
  issn = {0895-6111},
  abstract = {In this paper we report cone-beam CT techniques that permit reconstruction from width-truncated projections. These techniques are variants of Feldkamp's filtered backprojection algorithm and assume quasi-redundancy of ray integrals. Two methods are derived and compared. The first method involves the use of preconvolution weighting of the truncated data. The second technique performs post-convolution weighting preceded by non-zero estimation of the missing information. The algorithms were tested using the three-dimensional Shepp-Logan head phantom. The results indicate that given an appropriate amount of overscan, satisfactory reconstruction can be achieved. These techniques can be used to solve the problem of undersized detectors.},
  language = {eng},
  number = {1},
  journal = {Computerized Medical Imaging and Graphics: The Official Journal of the Computerized Medical Imaging Society},
  author = {Cho, P. S. and Rudd, A. D. and Johnson, R. H.},
  year = {1996 Jan-Feb},
  keywords = {Algorithms,Computer Simulation,Phantoms; Imaging,Radiographic Image Enhancement,Tomography; X-Ray Computed},
  pages = {49--57},
  pmid = {8891422}
}

@article{radon_uber_1917,
  title = {{\"U}ber Die {{Bestimmung}} von {{Funktionen}} Durch Ihre {{Integralwerte}} L{\"a}ngs Gewisser {{Mannigfaltigkeiten}}},
  volume = {69},
  journal = {Akad. Wiss.},
  author = {Radon, J},
  year = {1917},
  pages = {262--277},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/DR4ZF552/Radon - 1917 - Ãœber die Bestimmung von Funktionen durch ihre Inte.pdf}
}

@inproceedings{nuyts_use_2007,
  title = {The Use of Mutual Information and Joint Entropy for Anatomical Priors in Emission Tomography - {{IEEE Xplore Document}}},
  booktitle = {Nuclear {{Science Symposium Conference Record}}, 2007 {{IEEE}}},
  author = {Nuyts, Johan},
  month = oct,
  year = {2007},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/MQRG2UVQ/Nuyts - 2007 - The use of mutual information and joint entropy fo.pdf}
}

@article{hansis_high-quality_2010,
  title = {High-Quality 3-{{D}} Coronary Artery Imaging on an Interventional {{C}}-Arm x-Ray System},
  volume = {37},
  issn = {0094-2405},
  abstract = {PURPOSE Three-dimensional (3-D) reconstruction of the coronary arteries during a cardiac catheter-based intervention can be performed from a C-arm based rotational x-ray angiography sequence. It can support the diagnosis of coronary artery disease, treatment planning, and intervention guidance. 3-D reconstruction also enables quantitative vessel analysis, including vessel dynamics from a time-series of reconstructions. METHODS The strong angular undersampling and motion effects present in gated cardiac reconstruction necessitate the development of special reconstruction methods. This contribution presents a fully automatic method for creating high-quality coronary artery reconstructions. It employs a sparseness-prior based iterative reconstruction technique in combination with projection-based motion compensation. RESULTS The method is tested on a dynamic software phantom, assessing reconstruction accuracy with respect to vessel radii and attenuation coefficients. Reconstructions from clinical cases are presented, displaying high contrast, sharpness, and level of detail. CONCLUSIONS The presented method enables high-quality 3-D coronary artery imaging on an interventional C-arm system.},
  number = {4},
  journal = {Medical Physics},
  author = {Hansis, Eberhard and Carroll, John D and Sch{\"a}fer, Dirk and D{\"o}ssel, Olaf and Grass, Michael},
  month = apr,
  year = {2010},
  keywords = {Algorithms,Artificial Intelligence,Coronary Artery Disease,Coronary Vessels,Diagnostic Imaging,Humans,Image Processing; Computer-Assisted,Imaging; Three-Dimensional,Motion,Pattern Recognition; Automated,Phantoms; Imaging,Radiology; Interventional,Radiotherapy Planning; Computer-Assisted,Reproducibility of Results,X-Rays},
  pages = {1601--1609},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/SBV4SN8Z/Hansis et al. - 2010 - High-quality 3-D coronary artery imaging on an int.pdf},
  pmid = {20443481}
}

@article{vandemeulebroucke_automated_2012,
  title = {Automated Segmentation of a Motion Mask to Preserve Sliding Motion in Deformable Registration of Thoracic {{CT}}},
  volume = {39},
  issn = {0094-2405},
  doi = {10.1118/1.3679009},
  abstract = {PURPOSE: Deformable registration generally relies on the assumption that the sought spatial transformation is smooth. Yet, breathing motion involves sliding of the lung with respect to the chest wall, causing a discontinuity in the motion field, and the smoothness assumption can lead to poor matching accuracy. In response, alternative registration methods have been proposed, several of which rely on prior segmentations. We propose an original method for automatically extracting a particular segmentation, called a motion mask, from a CT image of the thorax.
METHODS: The motion mask separates moving from less-moving regions, conveniently allowing simultaneous estimation of their motion, while providing an interface where sliding occurs. The sought segmentation is subanatomical and based on physiological considerations, rather than organ boundaries. We therefore first extract clear anatomical features from the image, with respect to which the mask is defined. Level sets are then used to obtain smooth surfaces interpolating these features. The resulting procedure comes down to a monitored level set segmentation of binary label images. The method was applied to sixteen inhale-exhale image pairs. To illustrate the suitability of the motion masks, they were used during deformable registration of the thorax.
RESULTS: For all patients, the obtained motion masks complied with the physiological requirements and were consistent with respect to patient anatomy between inhale and exhale. Registration using the motion mask resulted in higher matching accuracy for all patients, and the improvement was statistically significant. Registration performance was comparable to that obtained using lung masks when considering the entire lung region, but the use of motion masks led to significantly better matching near the diaphragm and mediastinum, for the bony anatomy and for the trachea. The use of the masks was shown to facilitate the registration, allowing to reduce the complexity of the spatial transformation considerably, while maintaining matching accuracy.
CONCLUSIONS: We proposed an automated segmentation method for obtaining motion masks, capable of facilitating deformable registration of the thorax. The use of motion masks during registration leads to matching accuracies comparable to the use of lung masks for the lung region but motion masks are more suitable when registering the entire thorax.},
  language = {eng},
  number = {2},
  journal = {Medical Physics},
  author = {Vandemeulebroucke, Jef and Bernard, Olivier and Rit, Simon and Kybic, Jan and Clarysse, Patrick and Sarrut, David},
  month = feb,
  year = {2012},
  keywords = {Algorithms,Artifacts,Humans,Pattern Recognition; Automated,Radiographic Image Enhancement,Radiographic Image Interpretation; Computer-Assisted,Radiography; Thoracic,Reproducibility of Results,Sensitivity and Specificity,Subtraction Technique,Tomography; X-Ray Computed},
  pages = {1006--1015},
  pmid = {22320810}
}

@article{tibshirani_regression_1994,
  title = {Regression {{Shrinkage}} and {{Selection Via}} the {{Lasso}}},
  volume = {58},
  abstract = {We propose a new method for estimation in linear models. The "lasso" minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly zero and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described. Keywords: regression, subset selection, shrinkage, quadratic programming. 1 Introduction Consider the usual regression situation: we h...},
  journal = {Journal of the Royal Statistical Society, Series B},
  author = {Tibshirani, Robert},
  year = {1994},
  pages = {267--288},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/BHVU6D9A/Tibshirani - 1994 - Regression Shrinkage and Selection Via the Lasso.pdf}
}

@article{seppenwoolde_precise_2002,
  title = {Precise and Real-Time Measurement of {{3D}} Tumor Motion in Lung Due to Breathing and Heartbeat, Measured during Radiotherapy},
  volume = {53},
  number = {4},
  journal = {International Journal of Radiation Oncology* Biology* Physics},
  author = {Seppenwoolde, Yvette and Shirato, Hiroki and Kitamura, Kei and Shimizu, Shinichi and {van Herk}, Marcel and Lebesque, Joos V. and Miyasaka, Kazuo},
  year = {2002},
  pages = {822--834},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/K6I4E89I/Seppenwoolde et al. - 2002 - Precise and real-time measurement of 3D tumor moti.pdf}
}

@article{thibault_three-dimensional_2007,
  title = {A Three-Dimensional Statistical Approach to Improved Image Quality for Multislice Helical {{CT}}},
  volume = {34},
  issn = {00942405},
  doi = {10.1118/1.2789499},
  language = {en},
  number = {11},
  journal = {Medical Physics},
  author = {Thibault, Jean-Baptiste and Sauer, Ken D. and Bouman, Charles A. and Hsieh, Jiang},
  year = {2007},
  pages = {4526},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/QIXZGCTI/Thibault et al. - 2007 - A three-dimensional statistical approach to improv.pdf}
}

@article{sidky_first-order_2013,
  title = {First-Order Convex Feasibility Algorithms for x-Ray {{CT}}},
  volume = {40},
  issn = {0094-2405},
  doi = {10.1118/1.4790698},
  abstract = {Purpose: Iterative image reconstruction (IIR) algorithms in computed tomography (CT) are based on algorithms for solving a particular optimization problem. Design of the IIR algorithm, therefore, is aided by knowledge of the solution to the optimization problem on which it is based. Often times, however, it is impractical to achieve accurate solution to the optimization of interest, which complicates design of IIR algorithms. This issue is particularly acute for CT with a limited angular-range scan, which leads to poorly conditioned system matrices and difficult to solve optimization problems. In this paper, we develop IIR algorithms which solve a certain type of optimization called convex feasibility. The convex feasibility approach can provide alternatives to unconstrained optimization approaches and at the same time allow for rapidly convergent algorithms for their solution\textemdash{}thereby facilitating the IIR algorithm design process. Methods: An accelerated version of the Chambolle-Pock (CP) algorithm is adapted to various convex feasibility problems of potential interest to IIR in CT. One of the proposed problems is seen to be equivalent to least-squares minimization, and two other problems provide alternatives to penalized, least-squares minimization. Results: The accelerated CP algorithms are demonstrated on a simulation of circular fan-beam CT with a limited scanning arc of 144$^\circ$. The CP algorithms are seen in the empirical results to converge to the solution of their respective convex feasibility problems. Conclusions: Formulation of convex feasibility problems can provide a useful alternative to unconstrained optimization when designing IIR algorithms for CT. The approach is amenable to recent methods for accelerating first-order algorithms which may be particularly useful for CT with limited angular-range scanning. The present paper demonstrates the methodology, and future work will illustrate its utility in actual CT application.},
  number = {3},
  journal = {Medical Physics},
  author = {Sidky, Emil Y. and J{\o}rgensen, Jakob S. and Pan, Xiaochuan},
  month = feb,
  year = {2013},
  keywords = {Computed tomography,Image reconstruction,Medical image quality,Medical image reconstruction,Medical imaging},
  pages = {031115},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/WVWIXBZ2/Sidky et al. - 2013 - First-order convex feasibility algorithms for x-ra.pdf}
}

@inproceedings{mory_ecg-gated_2012,
  title = {{{ECG}}-Gated {{C}}-Arm Computed Tomography Using {{L1}} Regularization},
  abstract = {Cardiac C-Arm computed tomography leads to a view-starved reconstruction problem because of electrocardiogram gating. The lack of data has to be compensated by a-priori information on the solution. While standard regularization is performed by minimizing a quadratic penalty term, recently proposed limited-view reconstruction techniques perform it by minimizing the L1-norm of the signal in a basis where it is supposed to be sparse. Although some algorithms are formulated with this generic L1-norm term, their practical implementations replace it by total variation, which leads to piecewise constant images. In this article, we investigate the benefits of using a different sparsifying basis, which can be chosen depending on the expected properties of the solution. It results in more flexibility in the reconstruction. We provide an in-depth description of the algorithm used for the minimization.},
  booktitle = {Signal {{Processing Conference}} ({{EUSIPCO}}), 2012 {{Proceedings}} of the 20th {{European}}},
  author = {Mory, C. and Zhang, B. and Auvray, V. and Grass, M. and Schafer, D. and Peyrin, F. and Rit, S. and Douek, P. and Boussel, L.},
  year = {2012},
  keywords = {C-Arm,Computed tomography,Cost function,ECG-gated C-arm computed tomography,ECG-gating,Electrocardiography,Heart,Image reconstruction,L1 regularization,TV,augmented Lagrangian,cardiac C-Arm computed tomography,computerised tomography,conjugate gradient,electrocardiogram gating,limited-view reconstruction technique,medical image processing,minimisation,phantoms,piecewise constant image,quadratic penalty term minimisation,sparsifying basis,view-starved reconstruction problem},
  pages = {2728--2732},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/TVEITX4W/Mory et al. - 2012 - ECG-gated C-arm computed tomography using L1 regul.pdf}
}

@article{isola_motion_2010,
  title = {Motion Compensated Iterative Reconstruction of a Region of Interest in Cardiac Cone-Beam {{CT}}},
  volume = {34},
  issn = {0895-6111},
  doi = {16/j.compmedimag.2009.08.004},
  abstract = {A method for motion compensated iterative CT reconstruction of a cardiac region-of-interest is presented. The algorithm is an ordered subset maximum likelihood approach with spherically symmetric basis functions, and it uses an ECG for gating. Since the straightforward application of iterative methods to CT data has the drawback that a field-of-view has to be reconstructed, which covers the complete volume contributing to the absorption, region-of-interest reconstruction is applied here. Despite gating, residual object motion within the reconstructed gating window leads to motion blurring in the reconstructed image. To limit this effect, motion compensation is applied. Hereto, a gated 4D reconstruction at multiple phases is generated for the region-of-interest, and a limited set of vascular landmarks are manually annotated throughout the cardiac phases. A dense motion vector field is obtained from these landmarks by scattered data interpolation. The method is applied to two clinical data sets at strongest motion phases. Comparing the method to standard gated iterative reconstruction results shows that motion compensation strongly improved reconstruction quality.},
  number = {2},
  journal = {Computerized Medical Imaging and Graphics},
  author = {Isola, A.A. and Ziegler, A. and Sch{\"a}fer, D. and K{\"o}hler, T. and Niessen, W.J. and Grass, M.},
  month = mar,
  year = {2010},
  keywords = {Cardiac motion estimation,Computed tomography (CT),Iterative reconstruction,Motion compensation,Region of interest},
  pages = {149--159},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/XZ7528X9/Isola et al. - 2010 - Motion compensated iterative reconstruction of a r.pdf}
}

@article{medina_markov_2006,
  title = {Markov Random Field Modeling for Three-Dimensional Reconstruction of the Left Ventricle in Cardiac Angiography},
  volume = {25},
  issn = {0278-0062},
  doi = {10.1109/TMI.2006.877444},
  number = {8},
  journal = {IEEE Transactions on Medical Imaging},
  author = {Medina, R. and Garreau, M. and Toro, J. and Breton, H.L. and Coatrieux, J.-L. and Jugo, D.},
  month = aug,
  year = {2006},
  pages = {1087--1100},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/DE8NS6RA/PMC1971113.html}
}

@article{adcock_breaking_2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1302.0561},
  primaryClass = {cs, math},
  title = {Breaking the Coherence Barrier: {{A}} New Theory for Compressed Sensing},
  shorttitle = {Breaking the Coherence Barrier},
  abstract = {This paper provides an extension of compressed sensing which bridges a substantial gap between existing theory and its current use in real-world applications. It introduces a mathematical framework that generalizes the three standard pillars of compressed sensing - namely, sparsity, incoherence and uniform random subsampling - to three new concepts: asymptotic sparsity, asymptotic incoherence and multilevel random sampling. The new theorems show that compressed sensing is also possible, and reveals several advantages, under these substantially relaxed conditions. The importance of this is threefold. First, inverse problems to which compressed sensing is currently applied are typically coherent. The new theory provides the first comprehensive mathematical explanation for a range of empirical usages of compressed sensing in real-world applications, such as medical imaging, microscopy, spectroscopy and others. Second, in showing that compressed sensing does not require incoherence, but instead that asymptotic incoherence is sufficient, the new theory offers markedly greater flexibility in the design of sensing mechanisms. Third, by using asymptotic incoherence and multi-level sampling to exploit not just sparsity, but also structure, i.e. asymptotic sparsity, the new theory shows that substantially improved reconstructions can be obtained from fewer measurements.},
  journal = {arXiv:1302.0561 [cs, math]},
  author = {Adcock, Ben and Hansen, Anders C. and Poon, Clarice and Roman, Bogdan},
  month = feb,
  year = {2013},
  keywords = {Computer Science - Information Theory,Mathematics - Numerical Analysis},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/9IHC3PNM/Adcock et al. - 2013 - Breaking the coherence barrier A new theory for c.pdf}
}

@article{van_heeswijk_motion_2012,
  title = {Motion Compensation Strategies in Magnetic Resonance Imaging},
  volume = {40},
  number = {2},
  journal = {Critical Reviews\texttrademark{} in Biomedical Engineering},
  author = {{van Heeswijk}, Ruud B. and Bonanno, Gabriele and Coppo, Simone and Coristine, Andrew and Kober, Tobias and Stuber, Matthias},
  year = {2012},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/5P5K8GFM/van Heeswijk et al. - 2012 - Motion compensation strategies in magnetic resonan.pdf}
}

@article{liu_material_2016,
  title = {Material Reconstruction for Spectral Computed Tomography with Detector Response Function},
  volume = {32},
  issn = {0266-5611, 1361-6420},
  doi = {10.1088/0266-5611/32/11/114001},
  number = {11},
  journal = {Inverse Problems},
  author = {Liu, Jiulong and Gao, Hao},
  month = nov,
  year = {2016},
  pages = {114001},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/DDZEHV39/Liu and Gao - 2016 - Material reconstruction for spectral computed tomo.pdf}
}

@article{glockler_multimodality_2013,
  title = {Multimodality {{3D}}-Roadmap for Cardiovascular Interventions in Congenital Heart Disease--a Single-Center, Retrospective Analysis of 78 Cases},
  volume = {82},
  issn = {1522-726X},
  doi = {10.1002/ccd.24646},
  abstract = {BACKGROUND: Modern fluoroscopic angiography systems permit rendering of three-dimensional volumetric data sets using rotational angiography (3D-RA). Along with magnetic resonance imaging (MRI) or multi-detector-row computed tomography (MDCT) data sets, they can be fused with live fluoroscopy images for roadmapping during therapeutic procedures, but the value of multimodality fused data sets has not been clarified.
METHODS: In a pediatric cardiac catheterization laboratory, we analyzed 78 interventional cardiovascular procedures in which 3D models of 3D-RA, MRI, or MDCT were used for 3D-guidance. Accuracy of 2D-3D registration as well as overall procedural benefit was independently rated by two pediatric interventionalists. Fluoroscopy time, radiation dose, and contrast dye consumption were evaluated and, grouping a subgroup analysis, the parameters were compared between patients who underwent stenting of aortic coarctation with and without 3D-roadmapping.
RESULTS: 3D-guiding was used in 78 cases, 75 of these cases were with accurate 2D-3D registration. 3D-roadmapping was rated superior to conventional biplane imaging in 74 cases. 3D-guidance was used in 64 cases to define the ideal C-arm angulation and in 60 cases for accurate device positioning. Median dose-area product in the total investigation was 706.3 $\mu$Gym(2) (104.8-7249.7 $\mu$Gym(2) ), 3.3 ml/kg (0.9-13.7 ml/kg) of contrast dye was used, and total fluoroscopy time was 14.5 min (2.9-68.1 min). Fluoroscopy time for 3D-guided stenting of aortic coarctation is significantly lower (8.35 versus 10.2 min; P = 0.04).
CONCLUSION: 3D-image fusion with live fluoroscopy can be applied successfully in catheter-based interventions of congenital heart disease. 3D-guidance facilitates catheter manipulations and interventions, allows preselection of ideal projection angles, reduces fluoroscopic time and the number of control angiographies.},
  language = {eng},
  number = {3},
  journal = {Catheterization and cardiovascular interventions: official journal of the Society for Cardiac Angiography \& Interventions},
  author = {Gl{\"o}ckler, Martin and Halbfa$\beta$, Julia and Koch, Andreas and Achenbach, Stephan and Dittrich, Sven},
  month = sep,
  year = {2013},
  pages = {436--442},
  pmid = {22936634}
}

@article{bhowmik_correlation_2012,
  title = {Correlation and Support Vector Machine Based Motion Artifacts Mitigation in {{3D}} Computer Tomography},
  volume = {20},
  issn = {1095-9114},
  doi = {10.3233/XST-2012-0325},
  abstract = {Head motion during Computer Tomographic (CT) studies can adversely affects the reconstructed image through distortion and other artifacts such as blurring and doubling, thereby contributing to misdiagnosis of diseases. In this paper, we propose a method to detect and mitigate motion artifacts in three-dimensional (3D) cone-beam CT system. Motion detection is achieved by comparing the correlation coefficient between the adjacent x-ray projections. Artifacts, caused by motion, are mitigated either by replacing motion corrupted projections with their counterpart 180$^\circ$ apart projections under certain conditions, or by estimating motion corrupted projections using Least Square Support Vector Machine (LS-SVM) based time series prediction. The method has been evaluated on 3D Shepp-Logan phantom. In this research, Feldkamp-David-Kress (FDK) based back-projection algorithm is used for 3D reconstruction process. Computer simulation validates the motion detection and artifacts elimination mechanism.},
  language = {eng},
  number = {2},
  journal = {Journal of X-ray science and technology},
  author = {Bhowmik, Ujjal Kumar and Adhami, Reza R},
  year = {2012},
  keywords = {Algorithms,Artifacts,Computer Simulation,Cone-Beam Computed Tomography,Head Movements,Humans,Imaging; Three-Dimensional,Models; Biological,Phantoms; Imaging,Reproducibility of Results,Support Vector Machines},
  pages = {141--160},
  pmid = {22635171}
}

@article{boyd_distributed_2011,
  title = {Distributed {{Optimization}} and {{Statistical Learning}} via the {{Alternating Direction Method}} of {{Multipliers}}},
  volume = {3},
  issn = {1935-8237},
  doi = {10.1561/2200000016},
  abstract = {Many problems of recent interest in statistics and machine learning can be posed in the framework of convex optimization. Due to the explosion in size and complexity of modern datasets, it is increasingly important to be able to solve problems with a very large number of features or training examples. As a result, both the decentralized collection or storage of these datasets as well as accompanying distributed solution methods are either necessary or at least highly desirable. In this review, we argue that the alternating direction method of multipliers is well suited to distributed convex optimization, and in particular to large-scale problems arising in statistics, machine learning, and related areas. The method was developed in the 1970s, with roots in the 1950s, and is equivalent or closely related to many other algorithms, such as dual decomposition, the method of multipliers, Douglas\textendash{}Rachford splitting, Spingarn's method of partial inverses, Dykstra's alternating projections, Bregman iterative algorithms for $\mathscr{l}$1 problems, proximal methods, and others. After briefly surveying the theory and history of the algorithm, we discuss applications to a wide variety of statistical and machine learning problems of recent interest, including the lasso, sparse logistic regression, basis pursuit, covariance selection, support vector machines, and many others. We also discuss general distributed optimization, extensions to the nonconvex setting, and efficient implementation, including some details on distributed MPI and Hadoop MapReduce implementations.},
  number = {1},
  journal = {Found. Trends Mach. Learn.},
  author = {Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan},
  month = jan,
  year = {2011},
  pages = {1--122},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/AZSVVZ4T/Boyd - 2010 - Distributed Optimization and Statistical Learning .pdf}
}

@article{hansis_projection-based_2008,
  title = {Projection-Based Motion Compensation for Gated Coronary Artery Reconstruction from Rotational x-Ray Angiograms},
  volume = {53},
  journal = {Physics in Medicine and Biology},
  author = {Hansis, E. and Sch{\"a}fer, D. and D{\"o}ssel, O. and Grass, M.},
  year = {2008},
  pages = {3807},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/SAR8RS23/Hansis et al. - 2008 - Projection-based motion compensation for gated cor.pdf}
}

@inproceedings{strauss_nibart_2009,
  title = {{{NIBART}}: {{A New Interval Based Algebraic Reconstruction Technique}} for {{Error Quantification}} of {{Emission Tomography Images}}},
  shorttitle = {{{NIBART}}},
  abstract = {This article presents a new algebraic method for reconstructing emission tomography images. This approach is mostly an interval extension of the conventional SIRT algorithm. One of the main characteristic of our approach is that the reconstructed activity associated with each pixel of the reconstructed image is an interval whose length can be considered as an estimate of the impact of the random variation of the measured activity on the reconstructed image. This work aims at investigating a new methodological concept for a reliable and robust quantification of reconstructed activities in scintigraphic images.},
  language = {en},
  booktitle = {Medical {{Image Computing}} and {{Computer}}-{{Assisted Intervention}} \textendash{} {{MICCAI}} 2009},
  publisher = {{Springer, Berlin, Heidelberg}},
  author = {Strauss, Olivier and Lahrech, Abdelkabir and Rico, Agn{\`e}s and Mariano-Goulart, Denis and Telle, Beno{\^\i}t},
  month = sep,
  year = {2009},
  pages = {148--155},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/I2JKB69U/Strauss et al. - 2009 - NIBART A New Interval Based Algebraic Reconstruct.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/7THV3TDT/10.html},
  doi = {10.1007/978-3-642-04268-3_19}
}

@phdthesis{rit_prise_2007,
  title = {Prise En Compte Du Mouvement Respiratoire Pour La Reconstruction d'images Tomodensitom$\dottedsquare$etriques},
  school = {Universit{\'e} Louis Lumi{\`e}re Lyon 2},
  author = {Rit, Simon},
  month = sep,
  year = {2007},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/6JJJSD7X/Rit - 2007 - Prise en compte du mouvement respiratoire pour la .pdf}
}

@article{zhang_two-dimensional_2007,
  title = {Two-Dimensional Iterative Region-of-Interest ({{ROI}}) Reconstruction from Truncated Projection Data},
  volume = {34},
  issn = {00942405},
  doi = {10.1118/1.2436969},
  number = {3},
  journal = {Medical Physics},
  author = {Zhang, B. and Zeng, G. L.},
  year = {2007},
  pages = {935--944},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/TBQFN9UM/Zhang and Zeng - 2007 - Two-dimensional iterative region-of-interest (ROI).pdf}
}

@article{kaczmarz_angenaherte_1937,
  title = {Angen{\"a}herte {{Aufl{\"o}sung}} von {{Systemen}} Linearer {{Gleichungen}}},
  volume = {35},
  journal = {Bulletin International de l'Acad{\'e}mie Polonaise des Sciences et des Lettres},
  author = {Kaczmarz, S},
  year = {1937},
  pages = {355--357},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/S5CUZHAE/Kaczmarz - 1937 - AngenÃ¤herte AuflÃ¶sung von Systemen linearer Gleich.pdf}
}

@inproceedings{bernchou_cbct_2015,
  address = {Barcelona},
  title = {{{CBCT}} Response Signals and Age Identify a Subgroup of {{NSCLC}} Patients with Low Lung Radioresponsiveness},
  author = {Bernchou, Uffe and Bentzen, S.M. and Hansen, O. and Schytte, T. and Bertelsen, A. and Hope, A.J. and Moseley, D. and Brink, C.},
  month = apr,
  year = {2015},
  pages = {303--304}
}

@article{janssens_diffeomorphic_2011,
  title = {Diffeomorphic {{Registration}} of {{Images}} with {{Variable Contrast Enhancement}}},
  volume = {2011},
  issn = {1687-4188},
  doi = {10.1155/2011/891585},
  abstract = {Nonrigid image registration is widely used to estimate
tissue deformations in highly deformable anatomies. Among
the existing methods, nonparametric registration algorithms
such as optical flow, or Demons, usually have the advantage of
being fast and easy to use. Recently, a diffeomorphic version
of the Demons algorithm was proposed. This provides the
advantage of producing invertible displacement fields, which
is a necessary condition for these to be physical. However,
such methods are based on the matching of intensities and
are not suitable for registering images with different contrast
enhancement. In such cases, a registration method based on the
local phase like the Morphons has to be used. In this paper, a
diffeomorphic version of the Morphons registration method is
proposed and compared to conventional Morphons, Demons,
and diffeomorphic Demons. The method is validated in the
context of radiotherapy for lung cancer patients on several
4D respiratory-correlated CT scans of the thorax with and without
variable contrast enhancement.},
  journal = {International Journal of Biomedical Imaging},
  author = {Janssens, Guillaume and Jacques, Laurent and {Orban de Xivry}, Jonathan and Geets, Xavier and Macq, Benoit},
  year = {2011},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/X9NVHGJ2/Janssens et al. - 2011 - Diffeomorphic Registration of Images with Variable.pdf},
  pmid = {21197460},
  pmcid = {PMC3005125}
}

@inproceedings{bertram_scatter_2006,
  title = {Scatter Correction for Cone-Beam Computed Tomography Using Simulated Object Models},
  doi = {10.1117/12.651027},
  author = {Bertram, Matthias and Wiegert, Jens and Rose, Georg},
  editor = {Flynn, Michael J. and Hsieh, Jiang},
  month = mar,
  year = {2006},
  pages = {61421C--61421C--12},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/XMEFQK4V/Bertram et al. - 2006 - Scatter correction for cone-beam computed tomograp.pdf}
}

@article{tang_performance_2009,
  title = {Performance Comparison between Total Variation ({{TV}})-Based Compressed Sensing and Statistical Iterative Reconstruction Algorithms},
  volume = {54},
  issn = {1361-6560},
  doi = {10.1088/0031-9155/54/19/008},
  abstract = {Of all available reconstruction methods, statistical iterative reconstruction algorithms appear particularly promising since they enable accurate physical noise modeling. The newly developed compressive sampling/compressed sensing (CS) algorithm has shown the potential to accurately reconstruct images from highly undersampled data. The CS algorithm can be implemented in the statistical reconstruction framework as well. In this study, we compared the performance of two standard statistical reconstruction algorithms (penalized weighted least squares and q-GGMRF) to the CS algorithm. In assessing the image quality using these iterative reconstructions, it is critical to utilize realistic background anatomy as the reconstruction results are object dependent. A cadaver head was scanned on a Varian Trilogy system at different dose levels. Several figures of merit including the relative root mean square error and a quality factor which accounts for the noise performance and the spatial resolution were introduced to objectively evaluate reconstruction performance. A comparison is presented between the three algorithms for a constant undersampling factor comparing different algorithms at several dose levels. To facilitate this comparison, the original CS method was formulated in the framework of the statistical image reconstruction algorithms. Important conclusions of the measurements from our studies are that (1) for realistic neuro-anatomy, over 100 projections are required to avoid streak artifacts in the reconstructed images even with CS reconstruction, (2) regardless of the algorithm employed, it is beneficial to distribute the total dose to more views as long as each view remains quantum noise limited and (3) the total variation-based CS method is not appropriate for very low dose levels because while it can mitigate streaking artifacts, the images exhibit patchy behavior, which is potentially harmful for medical diagnosis.},
  number = {19},
  journal = {Physics in medicine and biology},
  author = {Tang, Jie and Nett, Brian E and Chen, Guang-Hong},
  month = oct,
  year = {2009},
  keywords = {Algorithms,Humans,Image Processing; Computer-Assisted,Radiation Dosage,Tomography; X-Ray Computed},
  pages = {5781--5804},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/T394JIDH/Tang et al. - 2009 - Performance comparison between total variation (TV.pdf},
  pmid = {19741274}
}

@misc{majumdar_choice_,
  title = {{{ON THE CHOICE OF COMPRESSED SENSING PRIORS AND SPARSIFYING TRANSFORMS FOR MR IMAGE RECONSTRUCTION}}: {{AN EXPERIMENTAL STUDY}}},
  shorttitle = {{{ON THE CHOICE OF COMPRESSED SENSING PRIORS AND SPARSIFYING TRANSFORMS FOR MR IMAGE RECONSTRUCTION}}},
  abstract = {"Recently Compressed Sensing (CS) based techniques are being used for reconstructing MR images from partially sampled k-space data. CS based reconstruction techniques can be categorized into three categories based on the objective function \textendash{} i)},
  howpublished = {http://www.academia.edu/1990541/ON\_THE\_CHOICE\_OF\_COMPRESSED\_SENSING\_PRIORS\_AND\_SPARSIFYING\_TRANSFORMS\_FOR\_MR\_IMAGE\_RECONSTRUCTION\_AN\_EXPERIMENTAL\_STUDY},
  author = {Majumdar, Angshul},
  keywords = {Biology,Chemistry,Computer Science,Earth Sciences,Economics,English,Geography,History,Law,Math,Medicine,Philosophy,Physics,Political Science,Psychology,Religion,academia,academics,research,universities},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/4CNXSFSR/ON_THE_CHOICE_OF_COMPRESSED_SENSING_PRIORS_AND_SPARSIFYING_TRANSFORMS_FOR_MR_IMAGE_RECONSTRUCTI.html}
}

@article{gonzalez_compressive_,
  title = {Compressive {{Optical Deflectometric Tomography}}: {{A Constrained Total}}-{{Variation Minimization Approach}}},
  shorttitle = {Compressive {{Optical Deflectometric Tomography}}},
  abstract = {Optical Deflectometric Tomography (ODT) provides an accurate characterization of transparent materials whose complex surfaces present a real challenge for manufacture and control. In ODT, the refractive index map (RIM) of a transparent},
  journal = {Journal of Inverse Problems and Imaging},
  author = {Gonzalez, Adriana and Jacques, Laurent and De Vleeschouwer, Christophe and Antoine, Philippe},
  year = {In press},
  keywords = {Biology,Chemistry,Computer Science,Earth Sciences,Economics,English,Geography,History,Law,Math,Medicine,Philosophy,Physics,Political Science,Psychology,Religion,academia,academics,research,universities},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/5PXHFQBX/Jacques - Compressive Optical Deflectometric Tomography A C.pdf}
}

@article{lauritsch_towards_2006,
  title = {Towards Cardiac {{C}}-Arm Computed Tomography},
  volume = {25},
  issn = {0278-0062},
  abstract = {Cardiac interventional procedures would benefit tremendously from sophisticated three-dimensional image guidance. Such procedures are typically performed with C-arm angiography systems, and tomographic imaging is currently available only by using preprocedural computed tomography (CT) or magnetic resonance imaging (MRI) scans. Recent developments in C-arm CT (Angiographic CT) allow three-dimensional (3-D) imaging of low contrast details with angiography imaging systems for noncardiac applications. We propose a new approach for cardiac imaging that takes advantage of this improved contrast resolution and is based on intravenous contrast injection. The method is an analogue to multisegment reconstruction in cardiac CT adapted to the much slower rotational speed of C-arm CT. Motion of the heart is considered in the reconstruction process by retrospective electrocardiogram (ECG)-gating, using only projections acquired at a similar heart phase. A series of N almost identical rotational acquisitions is performed at different heart phases to obtain a complete data set at a minimum temporal resolution of 1/N of the heart cycle time. First results in simulation, using an experimental phantom, and in preclinical in vivo studies showed that excellent image quality can be achieved.},
  number = {7},
  journal = {IEEE Transactions on Medical Imaging},
  author = {Lauritsch and Boese, Jan and Wigstr{\"o}m, Lars and Kemeth, Herbert and Fahrig, Rebecca},
  month = jul,
  year = {2006},
  keywords = {Algorithms,Electrocardiography,Heart,Humans,Imaging; Three-Dimensional,Phantoms; Imaging,Radiographic Image Enhancement,Radiographic Image Interpretation; Computer-Assisted,Reproducibility of Results,Sensitivity and Specificity,Tomography; Spiral Computed},
  pages = {922--934},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/MU5R8RSB/Lauritsch et al. - 2006 - Towards cardiac C-arm computed tomography.pdf},
  pmid = {16827492}
}

@article{wang_x-ray_2002,
  title = {X-Ray Micro-{{CT}} with a Displaced Detector Array},
  volume = {29},
  issn = {00942405},
  doi = {10.1118/1.1489043},
  language = {en},
  number = {7},
  journal = {Medical Physics},
  author = {Wang, Ge},
  year = {2002},
  pages = {1634},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/HIZPV2KR/wang2002a.pdf}
}

@phdthesis{prummer_cardiac_2009,
  title = {Cardiac {{C}}-{{Arm Computed Tomography}}: {{Motion Estimation}} and {{Dynamic Reconstruction}}},
  school = {Erlangen},
  author = {Pr{\"u}mmer, Marcus},
  year = {2009},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/GC2FEGF2/PrÃ¼mmer - 2009 - Cardiac C-Arm Computed Tomography Motion Estimati.pdf}
}

@phdthesis{valton_reconstruction_2007,
  title = {Reconstruction Tomographique {{3D}} En G{\'e}om{\'e}trie Conique {\`a} Trajectoire Circulaire Pour Des Prototypes d'imageur Bimodal Pour Le Petit Animal},
  author = {Valton, Sol{\`e}ne},
  year = {2007},
  keywords = {doctorat,theses},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/Z4PE62D8/Valton - Reconstruction tomographique 3D en gÃ©omÃ©trie coniq.pdf}
}

@article{wesarg_parker_2002,
  title = {Parker Weights Revisited},
  volume = {29},
  issn = {0094-2405},
  abstract = {The short-scan case in fan-beam computed tomography requires the introduction of a weighting function to handle redundant data. Parker introduced such a weighting function for a scan over pi plus the opening angle of the fan. In this article we derive a general class of weighting functions for arbitrary scan angles between pi plus fan angle and 2pi (over-scan). These weighting functions lead to mathematically exact reconstructions in the continuous case. Parker weights are a special case of a weighting function that belongs to this class. It will be shown that Parker weights are not generally the best choice in terms of noise reduction, especially when there is considerable over-scan. We derive a new weighting function that has a value of 0.5 for most of the redundant data and is smooth at the boundaries.},
  number = {3},
  journal = {Medical physics},
  author = {Wesarg, Stefan and Ebert, Matthias and Bortfeld, Thomas},
  month = mar,
  year = {2002},
  keywords = {Algorithms,Humans,Models; Statistical,Models; Theoretical,Tomography; X-Ray Computed,X-Rays},
  pages = {372--378},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/B4K8HQVF/Wesarg et al. - 2002 - Parker weights revisited.pdf},
  pmid = {11929021}
}

@article{asif_motion-adaptive_2012,
  title = {Motion-Adaptive Spatio-Temporal Regularization for Accelerated Dynamic {{MRI}}},
  issn = {1522-2594},
  doi = {10.1002/mrm.24524},
  abstract = {Accelerated magnetic resonance imaging techniques reduce signal acquisition time by undersampling k-space. A fundamental problem in accelerated magnetic resonance imaging is the recovery of quality images from undersampled k-space data. Current state-of-the-art recovery algorithms exploit the spatial and temporal structures in underlying images to improve the reconstruction quality. In recent years, compressed sensing theory has helped formulate mathematical principles and conditions that ensure recovery of (structured) sparse signals from undersampled, incoherent measurements. In this article, a new recovery algorithm, motion-adaptive spatio-temporal regularization, is presented that uses spatial and temporal structured sparsity of MR images in the compressed sensing framework to recover dynamic MR images from highly undersampled k-space data. In contrast to existing algorithms, our proposed algorithm models temporal sparsity using motion-adaptive linear transformations between neighboring images. The efficiency of motion-adaptive spatio-temporal regularization is demonstrated with experiments on cardiac magnetic resonance imaging for a range of reduction factors. Results are also compared with k-t FOCUSS with motion estimation and compensation-another recently proposed recovery algorithm for dynamic magnetic resonance imaging. Magn Reson Med, 2012. \textcopyright{} 2012 Wiley Periodicals, Inc.},
  language = {ENG},
  journal = {Magnetic Resonance in Medicine: Official Journal of the Society of Magnetic Resonance in Medicine / Society of Magnetic Resonance in Medicine},
  author = {Asif, M. Salman and Hamilton, Lei and Brummer, Marijn and Romberg, Justin},
  month = nov,
  year = {2012},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/ESUCFEV7/Asif et al. - 2012 - Motion-adaptive spatio-temporal regularization for.pdf},
  pmid = {23132400}
}

@article{song_sparseness_2007,
  title = {Sparseness Prior Based Iterative Image Reconstruction for Retrospectively Gated Cardiac Micro-{{CT}}},
  volume = {34},
  issn = {00942405},
  doi = {10.1118/1.2795830},
  number = {11},
  journal = {Medical Physics},
  author = {Song, Jiayu and Liu, Qing H. and Johnson, G. Allan and Badea, Cristian T.},
  year = {2007},
  pages = {4476--4483},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/IJM8WVE5/Song et al. - 2007 - Sparseness prior based iterative image reconstruct.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/JH9BZJTK/p4476_s1.html}
}

@phdthesis{_methodes_,
  title = {M{\'e}thodes de Reconstruction d'images {\`a} Partir d'un Faible Nombre de Projections En Tomographie Par Rayons {{X}}},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/7TJTK9TK/MÃ©thodes de reconstruction dâ€™images Ã  partir dâ€™un .pdf}
}

@article{vembar_dynamic_2003,
  title = {A Dynamic Approach to Identifying Desired Physiological Phases for Cardiac Imaging Using Multislice Spiral {{CT}}},
  volume = {30},
  issn = {0094-2405},
  abstract = {In this investigation, we describe a quantitative technique to measure coronary motion, which can be correlated with cardiac image quality using multislice computed tomography (MSCT) scanners. MSCT scanners, with subsecond scanning, thin-slice imaging (sub-millimeter) and volume scanning capabilities have paved the way for new clinical applications like noninvasive cardiac imaging. ECG-gated spiral CT using MSCT scanners has made it possible to scan the entire heart in a single breath-hold. The continuous data acquisition makes it possible for multiple phases to be reconstructed from a cardiac cycle. We measure the position and three-dimensional velocities of well-known landmarks along the proximal, mid, and distal regions of the major coronary arteries [left main (LM), left anterior descending (LAD), right coronary artery (RCA), and left circumflex (LCX)] during the cardiac cycle. A dynamic model (called the "delay algorithm") is described which enables us to capture the same physiological phase or "state" of the anatomy during the cardiac cycle as the instantaneous heart rate varies during the spiral scan. The coronary arteries are reconstructed from data obtained during different physiological cardiac phases and we correlate image quality of different parts of the coronary anatomy with phases at which minimum velocities occur. The motion characteristics varied depending on the artery, with the highest motion being observed for RCA. The phases with the lowest mean velocities provided the best visualization. Though more than one phase of relative minimum velocity was observed for each artery, the most consistent image quality was observed during mid-diastole ("diastasis") of the cardiac cycle and was judged to be superior to other reconstructed phases in 92\% of the cases. In the process, we also investigated correlation between cardiac arterial states and other measures of motion, such as the left ventricular volume during a cardiac cycle, which earlier has been demonstrated as an example of how anatomic-specific information can be used in a knowledge-based cardiac CT algorithm. Using these estimates in characterizing cardiac motion also provides realistic simulation models for higher heart rates and also in optimizing volume reconstructions for individual segments of the cardiac anatomy.},
  number = {7},
  journal = {Medical physics},
  author = {Vembar, M and Garcia, M J and Heuscher, D J and Haberl, R and Matthews, D and B{\"o}hme, G E and Greenberg, N L},
  month = jul,
  year = {2003},
  keywords = {Aged,Algorithms,Coronary Angiography,Coronary Artery Disease,Coronary Vessels,Electrocardiography,Female,Heart,Humans,Imaging; Three-Dimensional,Male,Movement,Radiographic Image Interpretation; Computer-Assisted,Reproducibility of Results,Sensitivity and Specificity,Statistics as Topic,Subtraction Technique,Tomography; Spiral Computed},
  pages = {1683--1693},
  pmid = {12906185}
}

@article{hamelin_design_2010,
  title = {Design of Iterative {{ROI}} Transmission Tomography Reconstruction Procedures and Image Quality Analysis},
  volume = {37},
  issn = {0094-2405},
  doi = {10.1118/1.3447722},
  abstract = {Purpose: An iterative edge-preserving CTreconstruction algorithm for high-resolution imaging of small regions of the field of view is investigated. It belongs to a family of region-of-interest reconstruction techniques in which a low-cost pilot reconstruction of the whole field of view is first performed and then used to deduce the contribution of the region of interest to the projection data. These projections are used for a high-resolution reconstruction of the region of interest (ROI) using a regularized iterative algorithm, resulting in significant computational savings. This paper examines how the technique by which the pilot reconstruction of the full field of view is obtained affects the total runtime and the image quality in the region of interest. Methods: Previous contributions to the literature have each focused on a single approach for the pilot reconstruction. In this paper, two such approaches are compared: the filtered backprojection and a low-resolution regularized iterative reconstruction method. ROI reconstructions are compared in terms of image quality and computational cost over simulated and physical phantom (Catphan600\textcopyright) studies, in order to assess the compromises that most impact the quality of the ROI reconstruction. Results: With the simulated phantom, new artifacts that appear in the ROI images are caused by significant errors in the pilot reconstruction. These errors include excessive coarseness of the pilot image grid and beam-hardening artifacts. With the Catphan600 phantom, differences in the imaging model of the scanner and that of the iterative reconstruction algorithm cause dark border artifacts in the ROI images. Conclusions: Inexpensive pilot reconstruction techniques (analytical algorithms, very-coarse-grid penalized likelihood) are practical choices in many common cases. However, they may yield background images altered by edge degradation or beam hardening, inducing projection inconsistency in the data used for ROI reconstruction. The ROI images thus have significant streak and speckle artifacts, which adversely affect the resolution-to-noise compromise. In these cases, edge-preserving penalized-likelihood methods on not-too-coarse image grids prove to be more robust and provide the best ROI image quality.},
  number = {9},
  journal = {Medical Physics},
  author = {Hamelin, Benoit and Goussard, Yves and Dussault, Jean-Pierre and Cloutier, Guy and Beaudoin, Gilles and Soulez, Gilles},
  month = sep,
  year = {2010},
  keywords = {Image reconstruction,Medical image quality,Medical image reconstruction,Medical imaging,Modulation transfer functions},
  pages = {4577--4589},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/2BP9HEAA/1.html}
}

@article{guan_optical_2013,
  title = {Optical Tomography Reconstruction Algorithm Based on the Radiative Transfer Equation Considering Refractive Index: {{Part}} 2. {{Inverse}} Model},
  issn = {08956111},
  shorttitle = {Optical Tomography Reconstruction Algorithm Based on the Radiative Transfer Equation Considering Refractive Index},
  doi = {10.1016/j.compmedimag.2013.01.006},
  journal = {Computerized Medical Imaging and Graphics},
  author = {Guan, Jinlan and Fang, Shaomei and Guo, Changhong},
  month = feb,
  year = {2013},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/DNJV9AZ8/1-s2.0-S0895611113000086-main.pdf}
}

@inproceedings{mory_deconvolution_2012,
  title = {Deconvolution for Limited-View Streak Artifacts Removal: Improvements upon an Existing Approach},
  shorttitle = {Deconvolution for Limited-View Streak Artifacts Removal},
  doi = {10.1109/NSSMIC.2012.6551539},
  abstract = {Cardiac C-Arm computed tomography leads to a view-starved reconstruction problem because of electrocardiogram gating. Reconstruction with the Feldkamp, Davis and Kress method (FDK) generates large streak artifacts in the reconstructed volumes, hampering the medical interpretation. In order to remove these artifacts, deconvolution techniques have been proposed. In this paper, we start from a recent inverse filtering method developed for streak artifact removal. Two ways to improve upon this method are described. It is then proposed to replace inverse filtering with an iterative deconvolution scheme. Finally, we show that the iterative deconvolution method can itself be replaced by iterative filtered back projection (IFBP). The IFBP approach is flexible and could be used in a broad range of applications, while the improved inverse filtering approaches are computationally less demanding and better suited for time-critical applications.},
  booktitle = {2012 {{IEEE Nuclear Science Symposium}} and {{Medical Imaging Conference}} ({{NSS}}/{{MIC}})},
  author = {Mory, Cyril and Auvray, Vincent and Zhang, Bo and Grass, Michael and Schafer, Dirk and Peyrin, Francoise and Rit, Simon and Douek, Philippe and Boussel, Loic},
  year = {2012},
  pages = {2370--2373},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/GFNREAC3/Mory et al. - 2012 - Deconvolution for limited-view streak artifacts re.pdf}
}

@article{langet_compressed_2011,
  title = {Compressed Sensing Based {{3D}} Tomographic Reconstruction for Rotational Angiography},
  volume = {14},
  abstract = {In this paper, we address three-dimensional tomographic reconstruction of rotational angiography acquisitions. In clinical routine, angular subsampling commonly occurs, due to the technical limitations of C-arm systems or possible improper injection. Standard methods such as filtered backprojection yield a reconstruction that is deteriorated by sampling artifacts, which potentially hampers medical interpretation. Recent developments of compressed sensing have demonstrated that it is possible to significantly improve reconstruction of subsampled datasets by generating sparse approximations through l1-penalized minimization. Based on these results, we present an extension of the iterative filtered backprojection that includes a sparsity constraint called soft background subtraction. This approach is shown to provide sampling artifact reduction when reconstructing sparse objects, and more interestingly, when reconstructing sparse objects over a non-sparse background. The relevance of our approach is evaluated in cone-beam geometry on real clinical data.},
  language = {eng},
  number = {Pt 1},
  journal = {Medical image computing and computer-assisted intervention: MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention},
  author = {Langet, H{\'e}l{\`e}ne and Riddell, Cyril and Trousset, Yves and Tenenhaus, Arthur and Lahalle, Elisabeth and Fleury, Gilles and Paragios, Nikos},
  year = {2011},
  keywords = {Algorithms,Angiography,Artifacts,Brain,Humans,Image Processing; Computer-Assisted,Imaging; Three-Dimensional,Models; Statistical,Software,Surgery; Computer-Assisted,Tomography; X-Ray Computed},
  pages = {97--104},
  pmid = {22003605}
}

@article{wallace_three-dimensional_2008,
  title = {Three-{{Dimensional C}}-Arm {{Cone}}-Beam {{CT}}: {{Applications}} in the {{Interventional Suite}}},
  volume = {19},
  issn = {10510443},
  shorttitle = {Three-{{Dimensional C}}-Arm {{Cone}}-Beam {{CT}}},
  doi = {10.1016/j.jvir.2008.02.018},
  number = {6},
  journal = {Journal of Vascular and Interventional Radiology},
  author = {Wallace, Michael J. and Kuo, Michael D. and Glaiberman, Craig and Binkert, Christoph A. and Orth, Robert C. and Soulez, Gilles},
  month = jun,
  year = {2008},
  pages = {799--813},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/TD4CXKU4/Wallace et al. - 2008 - Three-Dimensional C-arm Cone-beam CT Applications.pdf}
}

@book{grangeat_tomography_2009,
  address = {London [u.a.},
  title = {{Tomography}},
  isbn = {978-1-84821-099-8 1-84821-099-X},
  language = {Translation of: La tomographie and La tomographie medicale, originally published by Hermes Science/Lavoisier, 2002},
  publisher = {{ISTE [u.a.]}},
  author = {Grangeat, Pierre},
  year = {2009}
}

@article{koken_aperture_2006,
  title = {Aperture Weighted Cardiac Reconstruction for Cone-Beam {{CT}}},
  volume = {51},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/51/14/011},
  abstract = {Multi-row detectors together with fast rotating gantries made cardiac imaging possible for CT. Due to the cardiac motion, ECG gating has to be integrated into the reconstruction of the data measured on a low pitch helical trajectory. Since the first multi-row scanners were introduced, it has been shown that approximative true cone-beam reconstruction methods are most suitable for the task of retrospectively gated cardiac volume CT. In this paper, we present the aperture weighted cardiac reconstruction (AWCR), which is a three-dimensional reconstruction algorithm of the filtered back-projection type. It is capable of handling all illumination intervals of an object point, which occur as a consequence of a low pitch helical cone-beam acquisition. Therefore, this method is able to use as much redundant data as possible, resulting in an improvement of the image homogeneity, the signal to noise ratio and the temporal resolution. Different optimization techniques like the heart rate adaptive cardiac weighting or the automatic phase determination can be adopted to AWCR. The excellent image quality achieved by AWCR is presented for medical datasets acquired with both a 40-slice and a 64-slice cone-beam CT scanner.},
  number = {14},
  journal = {Physics in medicine and biology},
  author = {Koken, P and Grass, M},
  month = jul,
  year = {2006},
  keywords = {Algorithms,Heart,Humans,Image Processing; Computer-Assisted,Models; Statistical,Myocardium,Phantoms; Imaging,Radiographic Image Interpretation; Computer-Assisted,Time Factors,Tomography; X-Ray Computed},
  pages = {3433--3448},
  pmid = {16825741}
}

@article{bouman_unified_1996,
  title = {A Unified Approach to Statistical Tomography Using Coordinate Descent Optimization},
  volume = {5},
  number = {3},
  journal = {Image Processing, IEEE Transactions on},
  author = {Bouman, Charles and Sauer, Ken and {others}},
  year = {1996},
  pages = {480--492},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/MZZ6ZA4G/00491321.pdf}
}

@article{neubauer_clinical_2010,
  title = {Clinical {{Feasibility}} of a {{Fully Automated 3D Reconstruction}} of {{Rotational Coronary X}}-{{Ray Angiograms}} / {{CLINICAL PERSPECTIVE}}},
  volume = {3},
  doi = {10.1161/CIRCINTERVENTIONS.109.897629},
  abstract = {Background\textemdash{} Although fixed view x-ray angiography remains the primary technique for anatomic imaging of coronary artery disease, the known shortcomings of 2D projection imaging may limit accurate 3D vessel and lesion definition and characterization. A recently developed method to create 3D images of the coronary arteries uses x-ray projection images acquired during a 180$^\circ$ C-arm rotation and continuous contrast injection followed by ECG-gated iterative reconstruction. This method shows promise for providing high-quality 3D reconstructions of the coronary arteries with no user interaction but requires clinical evaluation.Methods and Results\textemdash{} The reconstruction strategy was evaluated by comparing the reconstructed 3D volumetric images with the 2D angiographic projection images from the same 23 patients to ascertain overall image quality, lesion visibility, and a comparison of 3D quantitative coronary analysis with 2D quantitative coronary analysis. The majority of the resulting 3D volume images were rated as having high image quality (66\%) and provided the physician with additional clinical information such as complete visualization of bifurcations and unobtainable views of the coronary tree. True-positive lesion detection rates were high (90 to 100\%), whereas false-positive detection rates were low (0 to 8.1\%). Finally, 3D quantitative coronary analysis showed significant similarity with 2D quantitative coronary analysis in terms of lumen diameters and provided vessel segment length free from the errors of foreshortening.Conclusions\textemdash{} Fully automated reconstruction of rotational coronary x-ray angiograms is feasible, produces 3D volumetric images that overcome some of the limitations of standard 2D angiography, and is ready for further implementation and study in the clinical environment.},
  number = {1},
  journal = {Circulation: Cardiovascular Interventions},
  author = {Neubauer, Anne M. and Garcia, Joel A. and Messenger, John C. and Hansis, Eberhard and Kim, Michael S. and Klein, Andrew J.P. and Schoonenberg, Gert A.F. and Grass, Michael and Carroll, John D.},
  month = feb,
  year = {2010},
  keywords = {Automation; Laboratory,Coronary Angiography,Coronary Vessels,Diagnostic Errors,Humans,Imaging; Three-Dimensional,Rotation,X-Rays},
  pages = {71 --79},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/E8GIZATM/Neubauer et al. - 2010 - Clinical Feasibility of a Fully Automated 3D Recon.pdf}
}

@article{parker_optimal_1982,
  title = {Optimal Short Scan Convolution Reconstruction for Fanbeam {{CT}}},
  volume = {9},
  issn = {0094-2405},
  abstract = {The problem of using a divergent fan beam convolution reconstruction algorithm in conjunction with a minimal complete (180 degrees plus the fan angle) data set is reviewed. It is shown that by proper weighting of the initial data set, image quality essentially equivalent to the quality of reconstructions from 360 degrees data sets is obtained. The constraints on the weights are that the sum of the two weights corresponding to the same line-integral must equal one, in regions of no data the weights must equal zero, and the weights themselves as well as the gradient of the weights must be continuous over the full 360 degrees. After weighting the initial data with weights that satisfy these constraints, image reconstruction can be conveniently achieved by using the standard (hardwired if available) convolver and backprojector of the specific scanner.},
  number = {2},
  journal = {Medical physics},
  author = {Parker, D L},
  year = {1982 Mar-Apr},
  keywords = {Technology; Radiologic,Tomography; X-Ray Computed},
  pages = {254--257},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/MFAJMT9A/Parker - 1982 - Optimal short scan convolution reconstruction for .pdf},
  pmid = {7087912}
}

@article{wu_weighted_2016,
  title = {A Weighted Polynomial Based Material Decomposition Method for Spectral X-Ray {{CT}} Imaging},
  volume = {61},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/61/10/3749},
  abstract = {Currently in photon counting based spectral x-ray computed tomography (CT) imaging, pre-reconstruction basis materials decomposition is an effective way to reconstruct densities of various materials. The iterative maximum-likelihood method requires precise spectrum information and is time-costly. In this paper, a novel non-iterative decomposition method based on polynomials is proposed for spectral CT, whose aim was to optimize the noise performance when there is more energy bins than the number of basis materials. Several subsets were taken from all the energy bins and conventional polynomials were established for each of them. The decomposition results from each polynomial were summed with pre-calculated weighting factors, which were designed to minimize the overall noises. Numerical studies showed that the decomposition noise of the proposed method was close to the Cramer\textendash{}Rao lower bound under Poisson noises. Furthermore, experiments were carried out with an XCounter Filte X1 photon counting detector for two-material decomposition and three-material decomposition for validation.},
  language = {en},
  number = {10},
  journal = {Physics in Medicine and Biology},
  author = {Wu, Dufan and Zhang, Li and Zhu, Xiaohua and Xu, Xiaofei and Wang, Sen},
  year = {2016},
  pages = {3749}
}

@article{leng_high_2008,
  title = {High Temporal Resolution and Streak-Free Four-Dimensional Cone-Beam Computed Tomography},
  volume = {53},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/53/20/006},
  abstract = {Cone-beam computed tomography (CBCT) has been clinically used to verify patient position and to localize the target of treatment in image-guided radiation therapy (IGRT). However, when the chest and the upper abdomen are scanned, respiratory-induced motion blurring limits the utility of CBCT. In order to mitigate this blurring, respiratory-gated CBCT, i.e. 4D CBCT, was introduced. In 4D CBCT, the cone-beam projection data sets acquired during a gantry rotation are sorted into several respiratory phases. In these gated reconstructions, the number of projections for each respiratory phase is significantly reduced. Consequently, undersampling streaking artifacts are present in the reconstructed images, and the image contrast resolution is also significantly compromised. In this paper, we present a new method to simultaneously achieve both high temporal resolution ( approximately 100 ms) and streaking artifact-free image volumes in 4D CBCT. The enabling technique is a newly proposed image reconstruction method, i.e. prior image constrained compressed sensing (PICCS), which enables accurate image reconstruction using vastly undersampled cone-beam projections and a fully sampled prior image. Using PICCS, a streak-free image can be reconstructed from 10-20 cone-beam projections while the signal-to-noise ratio is determined by a denoising feature of the selected objective function and by the prior image, which is reconstructed using all of the acquired cone-beam projections. This feature of PICCS breaks the connection between the temporal resolution and streaking artifacts' level in 4D CBCT. Numerical simulations and experimental phantom studies have been conducted to validate the method.},
  number = {20},
  journal = {Physics in medicine and biology},
  author = {Leng, Shuai and Tang, Jie and Zambelli, Joseph and Nett, Brian and Tolakanahalli, Ranjini and Chen, Guang-Hong},
  month = oct,
  year = {2008},
  keywords = {Algorithms,Cone-Beam Computed Tomography,Humans,Imaging; Three-Dimensional,Phantoms; Imaging,Radiographic Image Enhancement,Radiographic Image Interpretation; Computer-Assisted,Reproducibility of Results,Respiratory Mechanics,Sensitivity and Specificity},
  pages = {5653--5673},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/67EIVNJ6/Leng et al. - 2008 - High temporal resolution and streak-free four-dime.pdf},
  pmid = {18812650}
}

@phdthesis{mory_cardiac_2014-1,
  title = {Cardiac {{C}}-{{Arm Computed Tomography}}},
  school = {Universit{\'e} Lyon 1},
  author = {Mory, Cyril},
  month = feb,
  year = {2014},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/BR8HXN4C/Mory - 2014 - Cardiac C-Arm Computed Tomography.pdf}
}

@article{ritschl_improved_2011,
  title = {Improved Total Variation-Based {{CT}} Image Reconstruction Applied to Clinical Data},
  volume = {56},
  issn = {0031-9155, 1361-6560},
  doi = {10.1088/0031-9155/56/6/003},
  number = {6},
  journal = {Physics in Medicine and Biology},
  author = {Ritschl, Ludwig and Bergner, Frank and Fleischmann, Christof and Kachelrie{\ss}, Marc},
  month = mar,
  year = {2011},
  pages = {1545--1561},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/M34URBWH/improvedTV.pdf}
}

@article{isola_fully_2010,
  title = {Fully Automatic Nonrigid Registration-Based Local Motion Estimation for Motion-Corrected Iterative Cardiac {{CT}} Reconstruction},
  volume = {37},
  issn = {0094-2405},
  abstract = {PURPOSE Cardiac computed tomography is a rapidly emerging technique for noninvasive diagnosis of cardiovascular diseases. Nevertheless, the cardiac motion continues to be a limiting factor. Electrocardiogram-gated cardiac computed tomography reconstruction methods yield excellent results, but these are limited in their temporal resolution due to the mechanical movement of the gantry, and lead to residual motion blurring artifacts. If the motion of the cardiac region of interest is determined, motion compensated gated reconstructions can be applied to reduce motion artifacts. In this paper it is shown that elastic image registration methods can be an accurate solution to determine the cardiac motion. A method, which combines elastic registration and iterative computed tomography reconstruction methods delivering motion-corrected images of a chosen cardiac region of interest, is introduced. METHODS Using a gated four-dimensional region of interest image data set, a fully automatic elastic image registration is applied to recover a cardiac displacement field from a reference phase to a number of phases within the RR interval. Here, a stochastic optimizer and multiresolution approach are adopted to speed up the registration process. Subsequently, motion-compensated iterative reconstruction using the determined motion field is carried out. For the image representation volume-adapted spherical basis functions (blobs) are used to take the volume change caused by a divergent motion vector field into account. RESULTS The method is evaluated on phantom data and on four clinical data sets at a strong cardiac motion phase. Comparing the method to standard gated iterative reconstruction results shows that motion compensation strongly improves the image quality in these phases. A qualitative and quantitative accuracy study is presented for the estimated cardiac motion field. For the first time a blob-volume adaptation is applied on clinical data, and in the case of divergent motion it yields improved image quality. CONCLUSIONS A fully automatic local cardiac motion compensated gated iterative method with volume-adapted blobs is proposed. The method leads to excellent motion-corrected images which outperform nonmotion corrected results in phases of strong cardiac motion. In clinical cases, a volume-dependent blob-footprint adaptation proves to be a good solution to take care of the change in the blob volume caused by a divergent motion field.},
  number = {3},
  journal = {Medical Physics},
  author = {Isola, Alfonso A and Grass, Michael and Niessen, Wiro J},
  month = mar,
  year = {2010},
  keywords = {Algorithms,Artifacts,Artificial Intelligence,Cardiac-Gated Imaging Techniques,Heart,Humans,Motion,Pattern Recognition; Automated,Radiographic Image Enhancement,Radiographic Image Interpretation; Computer-Assisted,Reproducibility of Results,Sensitivity and Specificity,Signal Processing; Computer-Assisted,Subtraction Technique,Tomography; X-Ray Computed},
  pages = {1093--1109},
  pmid = {20384245}
}

@article{lee_improved_2012,
  title = {Improved Compressed Sensing-Based Cone-Beam {{CT}} Reconstruction Using Adaptive Prior Image Constraints},
  volume = {57},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/57/8/2287},
  abstract = {Volumetric cone-beam CT (CBCT) images are acquired repeatedly during a course of radiation therapy and a natural question to ask is whether CBCT images obtained earlier in the process can be utilized as prior knowledge to reduce patient imaging dose in subsequent scans. The purpose of this work is to develop an adaptive prior image constrained compressed sensing (APICCS) method to solve this problem. Reconstructed images using full projections are taken on the first day of radiation therapy treatment and are used as prior images. The subsequent scans are acquired using a protocol of sparse projections. In the proposed APICCS algorithm, the prior images are utilized as an initial guess and are incorporated into the objective function in the compressed sensing (CS)-based iterative reconstruction process. Furthermore, the prior information is employed to detect any possible mismatched regions between the prior and current images for improved reconstruction. For this purpose, the prior images and the reconstructed images are classified into three anatomical regions: air, soft tissue and bone. Mismatched regions are identified by local differences of the corresponding groups in the two classified sets of images. A distance transformation is then introduced to convert the information into an adaptive voxel-dependent relaxation map. In constructing the relaxation map, the matched regions (unchanged anatomy) between the prior and current images are assigned with smaller weight values, which are translated into less influence on the CS iterative reconstruction process. On the other hand, the mismatched regions (changed anatomy) are associated with larger values and the regions are updated more by the new projection data, thus avoiding any possible adverse effects of prior images. The APICCS approach was systematically assessed by using patient data acquired under standard and low-dose protocols for qualitative and quantitative comparisons. The APICCS method provides an effective way for us to enhance the image quality at the matched regions between the prior and current images compared to the existing PICCS algorithm. Compared to the current CBCT imaging protocols, the APICCS algorithm allows an imaging dose reduction of 10\textendash{}40 times due to the greatly reduced number of projections and lower x-ray tube current level coming from the low-dose protocol.},
  language = {en},
  number = {8},
  journal = {Physics in Medicine and Biology},
  author = {Lee, Ho and Xing, Lei and Davidi, Ran and Li, Ruijiang and Qian, Jianguo and Lee, Rena},
  month = apr,
  year = {2012},
  pages = {2287},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/V6AQNFBS/2287.html}
}

@inproceedings{mory_modified_2014,
  address = {Salt Lake City, USA},
  title = {A Modified {{4D ROOSTER}} Method Using the {{Chambolle}}-{{Pock}} Algorithm},
  booktitle = {Proceedings of the {{Third International Conference}} on {{Image Formation}} in {{X}}-{{Ray Computed Tomography}}},
  author = {Mory, Cyril and Jacques, Laurent},
  month = jun,
  year = {2014},
  pages = {191--193}
}

@article{li_accurate_2002,
  title = {An Accurate Iterative Reconstruction Algorithm for Sparse Objects: Application to {{3D}} Blood Vessel Reconstruction from a Limited Number of Projections},
  volume = {47},
  issn = {0031-9155},
  shorttitle = {An Accurate Iterative Reconstruction Algorithm for Sparse Objects},
  abstract = {Based on the duality of nonlinear programming, this paper proposes an accurate row-action type iterative algorithm which is appropriate to reconstruct sparse objects from a limited number of projections. The cost function we use is the Lp norm with p approximately 1.1. This norm allows us to pick up a sparse solution from a set of feasible solutions to the measurement equation. Furthermore, since it is both strictly convex and differentiable, we can use the duality of nonlinear programming to construct a row-action type iterative algorithm to find a solution. We also impose the bound constraint on pixel values to pick up a better solution. We demonstrate that this method works well in three-dimensional blood vessel reconstruction from a limited number of cone beam projections.},
  language = {eng},
  number = {15},
  journal = {Physics in medicine and biology},
  author = {Li, Meihua and Yang, Haiquan and Kudo, Hiroyuki},
  month = aug,
  year = {2002},
  keywords = {Algorithms,Angiography,Humans,Imaging; Three-Dimensional,Phantoms; Imaging,Radiographic Image Enhancement,Tomography,Tomography; X-Ray Computed},
  pages = {2599--2609},
  pmid = {12200927}
}

@article{cormack_representation_1963,
  title = {Representation of a {{Function}} by {{Its Line Integrals}}, with {{Some Radiological Applications}}},
  volume = {34},
  issn = {00218979},
  doi = {doi:10.1063/1.1729798},
  abstract = {A method is given of finding a real function in a finite region of a plane given its line integrals along all straight lines intersecting the region. The solution found is applicable to three problems of interest for precise radiology and radiotherapy: (1) the determination of a variable x-ray absorption coefficient in two dimensions; (2) the determination of the distribution of positron annihilations when there is an inhomogeneous distribution of the positron emitter in matter; and (3) the determination of a variable density of matter with constant chemical composition, using the energy loss of charged particles in the matter.},
  number = {9},
  journal = {Journal of Applied Physics},
  author = {Cormack, A. M.},
  month = sep,
  year = {1963},
  pages = {2722--2727},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/SW9STMWB/p2722_s1.html}
}

@article{lauzier_time-resolved_2012,
  title = {Time-Resolved Cardiac Interventional Cone-Beam {{CT}} Reconstruction from Fully Truncated Projections Using the Prior Image Constrained Compressed Sensing ({{PICCS}}) Algorithm},
  volume = {57},
  issn = {1361-6560},
  doi = {10.1088/0031-9155/57/9/2461},
  abstract = {C-arm cone-beam CT could replace preoperative multi-detector CT scans in the cardiac interventional setting. However, cardiac gating results in view angle undersampling and the small size of the detector results in projection data truncation. These problems are incompatible with conventional tomographic reconstruction algorithms. In this paper, the prior image constrained compressed sensing (PICCS) reconstruction method was adapted to solve these issues. The performance of the proposed method was compared to that of FDK, FDK with extrapolated projection data (E-FDK), and total variation-based compressed sensing. A canine projection dataset acquired using a clinical C-arm imaging system supplied realistic cardiac motion and anatomy for this evaluation. Three different levels of truncation were simulated. The relative root mean squared error and the universal image quality index were used to quantify the reconstruction accuracy. Three main conclusions were reached. (1) The adapted version of the PICCS algorithm offered the highest image quality and reconstruction accuracy. (2) No meaningful variation in performance was observed when the amount of truncation was changed. (3) This study showed evidence that accurate interior tomography with an undersampled acquisition is possible for realistic objects if a prior image with minimal artifacts is available.},
  language = {eng},
  number = {9},
  journal = {Physics in medicine and biology},
  author = {Lauzier, Pascal Th{\'e}riault and Tang, Jie and Chen, Guang-Hong},
  month = may,
  year = {2012},
  keywords = {Algorithms,Animals,Cone-Beam Computed Tomography,Dogs,Heart,Image Processing; Computer-Assisted,Time Factors},
  pages = {2461--2476},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/98KNCEFH/Lauzier et al. - 2012 - Time-resolved cardiac interventional cone-beam CT .pdf},
  pmid = {22481501}
}

@article{luo_reconstruction_2010,
  title = {Reconstruction {{From Limited}}-{{Angle Projections Based}} on Delta - Mu   {{Spectrum Analysis}}},
  volume = {19},
  issn = {1057-7149},
  doi = {10.1109/TIP.2009.2032893},
  abstract = {This paper proposes a sparse representation of an image using discrete   delta - u functions. A delta - u function is defined as the product of a   Kronecker delta function and a step function. Based on the sparse   representation, we have developed a novel and effective method for   reconstructing an image from limited-angle projections. The method first   estimates the parameters of the sparse representation from the   incomplete projection data, and then directly calculates the image to be   reconstructed. Experiments have shown that the proposed method can   effectively recover the missing data and reconstruct images more   accurately than the total-variation (TV) regularized reconstruction   method.},
  language = {English},
  number = {1},
  journal = {Ieee Transactions on Image Processing},
  author = {Luo, Jianhua and Li, Wanqing and Zhu, Yuemin},
  month = jan,
  year = {2010},
  keywords = {Limited angle,Tomography,backprojection,compton camera data,ct,delta - u function,delta - u spectrum analysis,image-reconstruction,sparse representation,tomosynthesis},
  pages = {131--140},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/EAD76BVK/Luo et al. - 2010 - Reconstruction From Limited-Angle Projections Base.pdf},
  note = {WOS:000272844000012}
}

@article{auvray_joint_2009,
  title = {Joint {{Motion Estimation}} and {{Layer Segmentation}} in {{Transparent Image Sequences}}\textemdash{}{{Application}} to {{Noise Reduction}} in {{X}}-{{Ray Image Sequences}}},
  volume = {2009},
  issn = {1687-6180},
  doi = {10.1155/2009/647262},
  number = {1},
  journal = {EURASIP Journal on Advances in Signal Processing},
  author = {Auvray, Vincent and Bouthemy, Patrick and Li{\'e}nard, Jean},
  year = {2009},
  pages = {647262},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/U3HZM2QB/Auvray et al. - 2009 - Joint Motion Estimation and Layer Segmentation in .pdf}
}

@article{zhang_effect_2014,
  title = {Effect of Compressed Sensing Reconstruction on Target and Organ Delineation in Cone-Beam {{CT}} of Head-and-Neck and Breast Cancer Patients},
  issn = {01678140},
  doi = {10.1016/j.radonc.2014.07.002},
  language = {en},
  journal = {Radiotherapy and Oncology},
  author = {Zhang, Hua and Tan, Wenyong and Sonke, Jan-Jakob},
  month = aug,
  year = {2014},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/I4XAR4QD/Zhang et al. - 2014 - Effect of compressed sensing reconstruction on tar.pdf}
}

@article{chambolle_first-order_2011,
  title = {A {{First}}-{{Order Primal}}-{{Dual Algorithm}} for {{Convex Problems}} with {{Applications}} to {{Imaging}}},
  volume = {40},
  issn = {0924-9907, 1573-7683},
  doi = {10.1007/s10851-010-0251-1},
  abstract = {In this paper we study a first-order primal-dual algorithm for non-smooth convex optimization problems with known saddle-point structure. We prove convergence to a saddle-point with rate O(1/N) in finite dimensions for the complete class of problems. We further show accelerations of the proposed algorithm to yield improved rates on problems with some degree of smoothness. In particular we show that we can achieve O(1/N 2) convergence on problems, where the primal or the dual objective is uniformly convex, and we can show linear convergence, i.e. O($\omega$ N ) for some $\omega\in$(0,1), on smooth problems. The wide applicability of the proposed algorithm is demonstrated on several imaging problems such as image denoising, image deconvolution, image inpainting, motion estimation and multi-label image segmentation.},
  language = {en},
  number = {1},
  journal = {Journal of Mathematical Imaging and Vision},
  author = {Chambolle, Antonin and Pock, Thomas},
  month = may,
  year = {2011},
  keywords = {Artificial Intelligence (incl. Robotics),Computer Imaging; Vision; Pattern Recognition and Graphics,Control ; Robotics; Mechatronics,Dual approaches,Image Processing and Computer Vision,Inverse problems,Reconstruction,Total variation,convex optimization,image},
  pages = {120--145},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/MBK32XEU/Chambolle et Pock - 2011 - A First-Order Primal-Dual Algorithm for Convex Pro.pdf}
}

@article{sidky_image_2008,
  title = {Image Reconstruction in Circular Cone-Beam Computed Tomography by Constrained, Total-Variation Minimization},
  volume = {53},
  issn = {0031-9155, 1361-6560},
  doi = {10.1088/0031-9155/53/17/021},
  number = {17},
  journal = {Physics in Medicine and Biology},
  author = {Sidky, Emil Y and Pan, Xiaochuan},
  month = sep,
  year = {2008},
  pages = {4777--4807},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/UC5Q8CP7/Sidky and Pan - 2008 - Image reconstruction in circular cone-beam compute.pdf}
}

@book{kak_principles_1988,
  title = {Principles of {{Computerized Tomographic Imaging}}},
  publisher = {{IEEE Press}},
  author = {Kak, AC and Slaney, M},
  year = {1988},
  keywords = {bibtex-import}
}

@article{guan_projection_1994,
  title = {A Projection Access Order for Speedy Convergence of {{ART}} (Algebraic Reconstruction Technique): A Multilevel Scheme for Computed Tomography},
  volume = {39},
  issn = {0031-9155, 1361-6560},
  shorttitle = {A Projection Access Order for Speedy Convergence of {{ART}} (Algebraic Reconstruction Technique)},
  doi = {10.1088/0031-9155/39/11/013},
  number = {11},
  journal = {Physics in Medicine and Biology},
  author = {Guan, H and Gordon, R},
  month = nov,
  year = {1994},
  pages = {2005--2022},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/UJFJ25JK/Guan and Gordon - 1994 - A projection access order for speedy convergence o.pdf}
}

@article{candes_stable_2006,
  title = {Stable Signal Recovery from Incomplete and Inaccurate Measurements},
  volume = {59},
  number = {8},
  journal = {Communications on pure and applied mathematics},
  author = {Candes, Emmanuel J. and Romberg, Justin K. and Tao, Terence},
  year = {2006},
  pages = {1207--1223},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/UFX9GKTV/Candes et al. - 2006 - Stable signal recovery from incomplete and inaccur.pdf}
}

@inproceedings{schafer_low_2012,
  title = {Low {{kV}} Rotational {{3D}} X-Ray Imaging for Improved {{CNR}} of Iodine Contrast Agent},
  doi = {10.1117/12.909909},
  author = {Sch{\"a}fer, Dirk and Ahrens, Martin and Eshuis, Peter and Grass, Michael},
  editor = {Pelc, Norbert J. and Nishikawa, Robert M. and Whiting, Bruce R.},
  month = feb,
  year = {2012},
  pages = {83132V--83132V--6},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/A7EAR7FG/SchÃ¤fer et al. - 2012 - Low kV rotational 3D x-ray imaging for improved CN.pdf}
}

@article{man_distance-driven_2004,
  title = {Distance-Driven Projection and Backprojection in Three Dimensions},
  volume = {49},
  issn = {0031-9155, 1361-6560},
  doi = {10.1088/0031-9155/49/11/024},
  number = {11},
  journal = {Physics in Medicine and Biology},
  author = {Man, Bruno De and Basu, Samit},
  month = jun,
  year = {2004},
  pages = {2463--2475},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/X4A6V5CF/deman2004.pdf}
}

@inproceedings{mory_iterative_2016,
  address = {London},
  title = {Iterative Cone Beam Computed Tomography in {{RTK}}, the {{Reconstruction ToolKit}}},
  booktitle = {Proceedings of the 18th {{International Conference}} on the Use of {{Computers}} in {{Radiation Therapy}}},
  author = {Mory, Cyril and Rit, Simon},
  month = jun,
  year = {2016},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/9WPI4F2I/Mory and Rit - 2016 - Iterative cone beam computed tomography in RTK, th.pdf}
}

@article{zheng_fast_2011,
  title = {Fast {{4D}} Cone-Beam Reconstruction Using the {{McKinnon}}-{{Bates}} Algorithm with Truncation Correction and Nonlinear Filtering},
  doi = {10.1117/12.878226},
  abstract = {A challenge in using on-board cone beam computed tomography (CBCT) to image lung tumor motion prior to radiation therapy treatment is acquiring and reconstructing high quality 4D images in a sufficiently short time for practical use. For the 1 minute rotation times typical of Linacs, severe view aliasing artifacts, including streaks, are created if a conventional phase-correlated FDK reconstruction is performed. The McKinnon-Bates (MKB) algorithm provides an efficient means of reducing streaks from static tissue but can suffer from low SNR and other artifacts due to data truncation and noise. We have added truncation correction and bilateral nonlinear filtering to the MKB algorithm to reduce streaking and improve image quality. The modified MKB algorithm was implemented on a graphical processing unit (GPU) to maximize efficiency. Results show that a nearly 4x improvement in SNR is obtained compared to the conventional FDK phase-correlated reconstruction and that high quality 4D images with 0.4 second temporal resolution and 1 mm3 isotropic spatial resolution can be reconstructed in less than 20 seconds after data acquisition completes.},
  author = {Zheng, Ziyi and Sun, Mingshan and Pavkovich, John and Star-Lack, Josh},
  month = mar,
  year = {2011},
  pages = {79612U--79612U}
}

@incollection{combettes_proximal_2011,
  series = {Springer Optimization and Its Applications},
  title = {Proximal {{Splitting Methods}} in {{Signal Processing}}},
  copyright = {\textcopyright{}2011 Springer Science+Business Media, LLC},
  isbn = {978-1-4419-9568-1 978-1-4419-9569-8},
  abstract = {The proximity operator of a convex function is a natural extension of the notion of a projection operator onto a convex set. This tool, which plays a central role in the analysis and the numerical solution of convex optimization problems, has recently been introduced in the arena of inverse problems and, especially, in signal processing, where it has become increasingly important. In this paper, we review the basic properties of proximity operators which are relevant to signal processing and present optimization methods based on these operators. These proximal splitting methods are shown to capture and extend several well-known algorithms in a unifying framework. Applications of proximal methods in signal recovery and synthesis are discussed.},
  language = {en},
  booktitle = {Fixed-{{Point Algorithms}} for {{Inverse Problems}} in {{Science}} and {{Engineering}}},
  publisher = {{Springer New York}},
  author = {Combettes, Patrick L. and Pesquet, Jean-Christophe},
  editor = {Bauschke, Heinz H. and Burachik, Regina S. and Combettes, Patrick L. and Elser, Veit and Luke, D. Russell and Wolkowicz, Henry},
  month = jan,
  year = {2011},
  keywords = {Algorithm Analysis and Problem Complexity,Alternating-direction method of multipliers,Backwardâ€“backward         algorithm,Calculus of Variations and Optimal Control; Optimization,Computational Mathematics and Numerical Analysis,Denoising,Douglasâ€“Rachford algorithm,Forwardâ€“backward algorithm,Frame,Iterative thresholding,Landweber method,Mathematical Modeling and Industrial Mathematics,Parallel computing,Peacemanâ€“Rachford algorithm,Proximal algorithm,Restoration         and reconstruction,Sparsity,Splitting,Theoretical; Mathematical and Computational Physics,convex optimization},
  pages = {185--212},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/ZZ7WMCUM/Combettes et Pesquet - 2011 - Proximal Splitting Methods in Signal Processing.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/K8EVXAQD/10.html}
}

@article{kuntz_real-time_2013,
  title = {Real-Time {{X}}-Ray-Based {{4D}} Image Guidance of Minimally Invasive Interventions},
  volume = {23},
  issn = {0938-7994, 1432-1084},
  doi = {10.1007/s00330-012-2761-2},
  number = {6},
  journal = {European Radiology},
  author = {Kuntz, Jan and Gupta, Rajiv and Sch{\"o}nberg, Stefan O. and Semmler, Wolfhard and Kachelrie{\ss}, Marc and Bartling, S{\"o}nke},
  month = jan,
  year = {2013},
  pages = {1669--1677},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/TGM893G6/Kuntz et al. - 2013 - Real-time X-ray-based 4D image guidance of minimal.pdf}
}

@article{kriminski_respiratory_2005,
  title = {Respiratory Correlated Cone-Beam Computed Tomography on an Isocentric {{C}}-Arm},
  volume = {50},
  issn = {0031-9155, 1361-6560},
  doi = {10.1088/0031-9155/50/22/004},
  number = {22},
  journal = {Physics in Medicine and Biology},
  author = {Kriminski, Sergey and Mitschke, Matthias and Sorensen, Stephen and Wink, Nicole M and Chow, Phillip E and Tenn, Steven and Solberg, Timothy D},
  month = nov,
  year = {2005},
  pages = {5263--5280},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/BQSGFSZZ/Kriminski et al. - 2005 - Respiratory correlated cone-beam computed tomograp.pdf}
}

@article{clackdoyle_data_2015,
  title = {Data Consistency Conditions for Truncated Fanbeam and Parallel Projections},
  volume = {42},
  issn = {0094-2405},
  doi = {10.1118/1.4905161},
  abstract = {PURPOSE: In image reconstruction from projections, data consistency conditions (DCCs) are mathematical relationships that express the overlap of information between ideal projections. DCCs have been incorporated in image reconstruction procedures for positron emission tomography, single photon emission computed tomography, and x-ray computed tomography (CT). Building on published fanbeam DCCs for nontruncated projections along a line, the authors recently announced new DCCs that can be applied to truncated parallel projections in classical (two-dimensional) image reconstruction. These DCCs take the form of polynomial expressions for a weighted backprojection of the projections. The purpose of this work was to present the new DCCs for truncated parallel projections, to extend these conditions to truncated fanbeam projections on a circular trajectory, to verify the conditions with numerical examples, and to present a model of how DCCs could be applied with a toy problem in patient motion estimation with truncated projections.
METHODS: A mathematical derivation of the new parallel DCCs was performed by substituting the underlying imaging equation into the mathematical expression for the weighted backprojection and demonstrating the resulting polynomial form. This DCC result was extended to fanbeam projections by a substitution of parallel to fanbeam variables. Ideal fanbeam projections of a simple mathematical phantom were simulated and the DCCs for these projections were evaluated by fitting polynomials to the weighted backprojection. For the motion estimation problem, a parametrized motion was simulated using a dynamic version of the mathematical phantom, and both noiseless and noisy fanbeam projections were simulated for a full circular trajectory. The fanbeam DCCs were applied to extract the motion parameters, which allowed the motion contamination to be removed from the projections. A reconstruction was performed from the corrected projections.
RESULTS: The mathematical derivation revealed the anticipated polynomial behavior. The conversion to fanbeam variables led to a straight-forward weighted fanbeam backprojection which yielded the same function and therefore the same polynomial behavior as occurred in the parallel case. Plots of the numerically calculated DCCs showed polynomial behavior visually indistinguishable from the fitted polynomials. For the motion estimation problem, the motion parameters were satisfactorily recovered and ten times more accurately for the noise-free case. The reconstructed images showed that only a faint trace of the motion blur was still visible after correction from the noisy motion-contaminated projections.
CONCLUSIONS: New DCCs have been established for fanbeam and parallel projections, and these conditions have been validated using numerical experiments with truncated projections. It has been shown how these DCCs could be applied to extract parameters of unwanted physical effects in tomographic imaging, even with truncated projections.},
  language = {eng},
  number = {2},
  journal = {Medical Physics},
  author = {Clackdoyle, Rolf and Desbat, Laurent},
  month = feb,
  year = {2015},
  keywords = {Algorithms,Image Processing; Computer-Assisted,Tomography; X-Ray Computed},
  pages = {831--845},
  pmid = {25652496}
}

@article{chartrand_restricted_2008,
  title = {Restricted Isometry Properties and Nonconvex Compressive Sensing},
  volume = {24},
  issn = {0266-5611},
  doi = {10.1088/0266-5611/24/3/035020},
  abstract = {The recently emerged field known as compressive sensing has produced powerful results showing the ability to recover sparse signals from surprisingly few linear measurements, using $\mathscr{l}$1 minimization. In previous work, numerical experiments showed that $\mathscr{l}$p minimization with 0 $<$ p $<$ 1 recovers sparse signals from fewer linear measurements than does $\mathscr{l}$1 minimization. It was also shown that a weaker restricted isometry property is sufficient to guarantee perfect recovery in the $\mathscr{l}$p case. In this work, we generalize this result to an $\mathscr{l}$p variant of the restricted isometry property, and then determine how many random, Gaussian measurements are sufficient for the condition to hold with high probability. The resulting sufficient condition is met by fewer measurements for smaller p. This adds to the theoretical justification for the methods already being applied to replacing high-dose CT scans with a small number of x-rays and reducing MRI scanning time. The potential benefits extend to any application of compressive sensing.},
  language = {en},
  number = {3},
  journal = {Inverse Problems},
  author = {Chartrand, Rick and Staneva, Valentina},
  month = jun,
  year = {2008},
  pages = {035020},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/WND5KFHF/035020.html}
}

@article{medoff_iterative_1983,
  title = {Iterative Convolution Backprojection Algorithms for Image Reconstruction from Limited Data},
  volume = {73},
  doi = {10.1364/JOSA.73.001493},
  abstract = {Image-reconstruction algorithms implemented on existing computerized tomography (CT) scanners require the collection of line integrals that are evenly spaced over 360 deg. In many practical situations, some of the line integrals are inaccurately measured or are not measured at all. In these limited-data situations, conventional algorithms produce images with severe streak artifacts. Recently, several other image-reconstruction algorithms were suggested, each tailored to a specific type of limited-data problem. These algorithms make minimal use of a priori knowledge about the image; only one has been demonstrated with real x-ray data. We present a new operator framework that treats all types of limited-data image-reconstruction problems in a unified way. From this framework we derive iterative convolution backprojection algorithms that make no restrictions on the location of missing line integrals. All available a priori information is incorporated by constraint operators. The algorithm has been implemented on a commercial CT scanner. We present examples of images reconstructed from real x-ray data in two limited-data situations and demonstrate the use of additional a priori information to reduce streak artifacts further.},
  number = {11},
  journal = {Journal of the Optical Society of America},
  author = {Medoff, Barry P. and Brody, William R. and Nassi, Menahem and Macovski, Albert},
  month = nov,
  year = {1983},
  pages = {1493--1500},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/XI3RCPT8/Medoff et al. - 1983 - Iterative convolution backprojection algorithms fo.pdf}
}

@article{ayasso_joint_2010,
  title = {Joint {{NDT Image Restoration}} and {{Segmentation Using Gauss}}\&\#x2013;{{Markov}}\&\#x2013;{{Potts Prior Models}} and {{Variational Bayesian Computation}}},
  volume = {19},
  issn = {1057-7149, 1941-0042},
  doi = {10.1109/TIP.2010.2047902},
  number = {9},
  journal = {IEEE Transactions on Image Processing},
  author = {Ayasso, H and Mohammad-Djafari, A},
  month = sep,
  year = {2010},
  pages = {2265--2277},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/4CMJEKTW/05445046.pdf}
}

@article{combettes_stochastic_2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1404.7536},
  primaryClass = {math},
  title = {Stochastic {{Quasi}}-{{Fej{\'e}r Block}}-{{Coordinate Fixed Point Iterations}} with {{Random Sweeping}}},
  abstract = {This work proposes block-coordinate fixed point algorithms with applications to nonlinear analysis and optimization in Hilbert spaces. The asymptotic analysis relies on a notion of stochastic quasi-Fej$\backslash$'er monotonicity, which is thoroughly investigated. The iterative methods under consideration feature random sweeping rules to select arbitrarily the blocks of variables that are activated over the course of the iterations and they allow for stochastic errors in the evaluation of the operators. Algorithms using quasinonexpansive operators or compositions of averaged nonexpansive operators are constructed, and weak and strong convergence results are established for the sequences they generate. As a by-product, novel block-coordinate operator splitting methods are obtained for solving structured monotone inclusion and convex minimization problems. In particular, the proposed framework leads to random block-coordinate versions of the Douglas-Rachford and forward-backward algorithms and of some of their variants. In the standard case of \$m=1\$ block, our results remain new as they incorporate stochastic perturbations.},
  journal = {arXiv:1404.7536 [math]},
  author = {Combettes, Patrick L. and Pesquet, Jean-Christophe},
  month = apr,
  year = {2014},
  keywords = {Mathematics - Optimization and Control,Primary 47H05; Secondary 65K05; 90C25},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/KEJR5PZ8/Combettes and Pesquet - 2014 - Stochastic Quasi-FejÃ©r Block-Coordinate Fixed Poin.pdf}
}

@article{candes_introduction_2008,
  title = {An {{Introduction To Compressive Sampling}}},
  volume = {25},
  issn = {1053-5888},
  doi = {10.1109/MSP.2007.914731},
  number = {2},
  journal = {IEEE Signal Processing Magazine},
  author = {Candes, E.J. and Wakin, M.B.},
  month = mar,
  year = {2008},
  pages = {21--30},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/9ARVTKHM/Candes and Wakin - 2008 - An Introduction To Compressive Sampling.pdf}
}

@article{jia_four-dimensional_2012,
  title = {Four-Dimensional Cone Beam {{CT}} Reconstruction and Enhancement Using a Temporal Nonlocal Means Method},
  volume = {39},
  issn = {0094-2405},
  doi = {10.1118/1.4745559},
  abstract = {Purpose: Four-dimensional cone beam computed tomography (4D-CBCT) has been developed to provide respiratory phase-resolved volumetric imaging in image guided radiation therapy. Conventionally, it is reconstructed by first sorting the x-ray projections into multiple respiratory phase bins according to a breathing signal extracted either from the projection images or some external surrogates, and then reconstructing a 3D CBCT image in each phase bin independently using FDK algorithm. This method requires adequate number of projections for each phase, which can be achieved using a low gantry rotation or multiple gantry rotations. Inadequate number of projections in each phase bin results in low quality 4D-CBCT images with obvious streaking artifacts. 4D-CBCT images at different breathing phases share a lot of redundant information, because they represent the same anatomy captured at slightly different temporal points. Taking this redundancy along the temporal dimension into account can in principle facilitate the reconstruction in the situation of inadequate number of projection images. In this work, the authors propose two novel 4D-CBCT algorithms: an iterative reconstruction algorithm and an enhancement algorithm, utilizing a temporal nonlocal means (TNLM) method., Methods: The authors define a TNLM energy term for a given set of 4D-CBCT images. Minimization of this term favors those 4D-CBCT images such that any anatomical features at one spatial point at one phase can be found in a nearby spatial point at neighboring phases. 4D-CBCT reconstruction is achieved by minimizing a total energy containing a data fidelity term and the TNLM energy term. As for the image enhancement, 4D-CBCT images generated by the FDK algorithm are enhanced by minimizing the TNLM function while keeping the enhanced images close to the FDK results. A forward\textendash{}backward splitting algorithm and a Gauss\textendash{}Jacobi iteration method are employed to solve the problems. The algorithms implementation on GPU is designed to avoid redundant and uncoalesced memory access, in order to ensure a high computational efficiency. Our algorithms have been tested on a digital NURBS-based cardiac-torso phantom and a clinical patient case., Results: The reconstruction algorithm and the enhancement algorithm generate visually similar 4D-CBCT images, both better than the FDK results. Quantitative evaluations indicate that, compared with the FDK results, our reconstruction method improves contrast-to-noise-ratio (CNR) by a factor of 2.56\textendash{}3.13 and our enhancement method increases the CNR by 2.75\textendash{}3.33 times. The enhancement method also removes over 80\% of the streak artifacts from the FDK results. The total computation time is 509\textendash{}683 s for the reconstruction algorithm and 524\textendash{}540 s for the enhancement algorithm on an NVIDIA Tesla C1060 GPU card., Conclusions: By innovatively taking the temporal redundancy among 4D-CBCT images into consideration, the proposed algorithms can produce high quality 4D-CBCT images with much less streak artifacts than the FDK results, in the situation of inadequate number of projections.},
  number = {9},
  journal = {Medical Physics},
  author = {Jia, Xun and Tian, Zhen and Lou, Yifei and Sonke, Jan-Jakob and Jiang, Steve B.},
  month = sep,
  year = {2012},
  pages = {5592--5602},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/48D5FTR8/Jia et al. - 2012 - Four-dimensional cone beam CT reconstruction and e.pdf},
  pmid = {22957625},
  pmcid = {PMC3436920}
}

@incollection{arsigny_log-euclidean_2006,
  series = {Lecture Notes in Computer Science},
  title = {A {{Log}}-{{Euclidean Framework}} for {{Statistics}} on {{Diffeomorphisms}}},
  copyright = {\textcopyright{}2006 Springer-Verlag Berlin Heidelberg},
  isbn = {978-3-540-44707-8 978-3-540-44708-5},
  abstract = {In this article, we focus on the computation of statistics of invertible geometrical deformations (i.e., diffeomorphisms), based on the generalization to this type of data of the notion of principal logarithm. Remarkably, this logarithm is a simple 3D vector field, and is well-defined for diffeomorphisms close enough to the identity. This allows to perform vectorial statistics on diffeomorphisms, while preserving the invertibility constraint, contrary to Euclidean statistics on displacement fields. We also present here two efficient algorithms to compute logarithms of diffeomorphisms and exponentials of vector fields, whose accuracy is studied on synthetic data. Finally, we apply these tools to compute the mean of a set of diffeomorphisms, in the context of a registration experiment between an atlas an a database of 9 T1 MR images of the human brain.},
  language = {en},
  number = {4190},
  booktitle = {Medical {{Image Computing}} and {{Computer}}-{{Assisted Intervention}} \textendash{} {{MICCAI}} 2006},
  publisher = {{Springer Berlin Heidelberg}},
  author = {Arsigny, Vincent and Commowick, Olivier and Pennec, Xavier and Ayache, Nicholas},
  editor = {Larsen, Rasmus and Nielsen, Mads and Sporring, Jon},
  month = oct,
  year = {2006},
  keywords = {Artificial Intelligence (incl. Robotics),Computer Graphics,Health Informatics,Image Processing and Computer Vision,Imaging / Radiology,Pattern Recognition},
  pages = {924--931},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/47HT44VR/Arsigny et al. - 2006 - A Log-Euclidean Framework for Statistics on Diffeo.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/K4QRZKC6/10.html},
  doi = {10.1007/11866565_113}
}

@article{lu_four-dimensional_2007,
  title = {Four-Dimensional Cone Beam {{CT}} with Adaptive Gantry Rotation and Adaptive Data Sampling},
  volume = {34},
  issn = {00942405},
  doi = {10.1118/1.2767145},
  language = {en},
  number = {9},
  journal = {Medical Physics},
  author = {Lu, Jun and Guerrero, Thomas M. and Munro, Peter and Jeung, Andrew and Chi, Pai-Chun M. and Balter, Peter and Zhu, X. Ronald and Mohan, Radhe and Pan, Tinsu},
  year = {2007},
  pages = {3520},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/F56NI6XW/Lu et al. - 2007 - Four-dimensional cone beam CT with adaptive gantry.pdf}
}

@article{clackdoyle_full_2013,
  title = {Full Data Consistency Conditions for Cone-Beam Projections with Sources on a Plane},
  volume = {58},
  issn = {0031-9155, 1361-6560},
  doi = {10.1088/0031-9155/58/23/8437},
  number = {23},
  journal = {Physics in Medicine and Biology},
  author = {Clackdoyle, Rolf and Desbat, Laurent},
  month = dec,
  year = {2013},
  pages = {8437--8456},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/NJH95EIH/clackdoyle2013a.pdf}
}

@article{mory_motion-aware_2016,
  title = {Motion-Aware Temporal Regularization for Improved {{4D}} Cone-Beam Computed Tomography},
  volume = {61},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/61/18/6856},
  abstract = {Four-dimensional cone-beam computed tomography (4D-CBCT) of the free-breathing thorax is a valuable tool in image-guided radiation therapy of the thorax and the upper abdomen. It allows the determination of the position of a tumor throughout the breathing cycle, while only its mean position can be extracted from three-dimensional CBCT. The classical approaches are not fully satisfactory: respiration-correlated methods allow one to accurately locate high-contrast structures in any frame, but contain strong streak artifacts unless the acquisition is significantly slowed down. Motion-compensated methods can yield streak-free, but static, reconstructions. This work proposes a 4D-CBCT method that can be seen as a trade-off between respiration-correlated and motion-compensated reconstruction. It builds upon the existing reconstruction using spatial and temporal regularization (ROOSTER) and is called motion-aware ROOSTER (MA-ROOSTER). It performs temporal regularization along curved trajectories, following the motion estimated on a prior 4D CT scan. MA-ROOSTER does not involve motion-compensated forward and back projections: the input motion is used only during temporal regularization. MA-ROOSTER is compared to ROOSTER, motion-compensated Feldkamp\textendash{}Davis\textendash{}Kress (MC-FDK), and two respiration-correlated methods, on CBCT acquisitions of one physical phantom and two patients. It yields streak-free reconstructions, visually similar to MC-FDK, and robust information on tumor location throughout the breathing cycle. MA-ROOSTER also allows a variation of the lung tissue density during the breathing cycle, similar to that of planning CT, which is required for quantitative post-processing.},
  language = {en},
  number = {18},
  journal = {Physics in Medicine and Biology},
  author = {Mory, Cyril and Janssens, Guillaume and Rit, Simon},
  year = {2016},
  pages = {6856},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/UIMKPHU5/Mory et al. - 2016 - Motion-aware temporal regularization for improved .pdf}
}

@article{charbonnier_deterministic_1997,
  title = {Deterministic Edge-Preserving Regularization in Computed Imaging},
  volume = {6},
  issn = {1057-7149},
  doi = {10.1109/83.551699},
  abstract = {Many image processing problems are ill-posed and must be regularized. Usually, a roughness penalty is imposed on the solution. The difficulty is to avoid the smoothing of edges, which are very important attributes of the image. In this paper, we first give conditions for the design of such an edge-preserving regularization. Under these conditions, we show that it is possible to introduce an auxiliary variable whose role is twofold. First, it marks the discontinuities and ensures their preservation from smoothing. Second, it makes the criterion half-quadratic. The optimization is then easier. We propose a deterministic strategy, based on alternate minimizations on the image and the auxiliary variable. This leads to the definition of an original reconstruction algorithm, called ARTUR. Some theoretical properties of ARTUR are discussed. Experimental results illustrate the behavior of the algorithm. These results are shown in the field of 2D single photon emission tomography, but this method can be applied in a large number of applications in image processing},
  language = {English},
  number = {2},
  journal = {IEEE Transactions on Image Processing},
  author = {Charbonnier, P. and Blanc-Feraud, L. and Aubert, G. and Barlaud, M.},
  month = feb,
  year = {1997},
  keywords = {ARTUR,Computer applications,Image processing,Image reconstruction,Markov random fields,Minimization methods,Nonlinear equations,Reconstruction algorithms,Smoothing methods,Tomography,Transforms,alternate minimizations,auxiliary variable,computed imaging,computerised tomography,deterministic edge-preserving regularization,discontinuities,edge detection,half-quadratic criterion,image processing problems,medical image processing,minimisation,optimization,reconstruction algorithm,single photon emission computed tomography},
  pages = {298--311},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/M8AJBZ4T/Charbonnier et al. - 1997 - Deterministic edge-preserving regularization in co.pdf}
}

@article{andersen_simultaneous_1984,
  title = {Simultaneous Algebraic Reconstruction Technique ({{SART}}): A Superior Implementation of the Art Algorithm},
  volume = {6},
  issn = {0161-7346},
  shorttitle = {Simultaneous Algebraic Reconstruction Technique ({{SART}})},
  abstract = {In this paper we have discussed what appears to be a superior implementation of the Algebraic Reconstruction Technique (ART). The method is based on 1) simultaneous application of the error correction terms as computed by ART for all rays in a given projection; 2) longitudinal weighting of the correction terms back-distributed along the rays; and 3) using bilinear elements for discrete approximation to the ray integrals of a continuous image. Since this implementation generates a good reconstruction in only one iteration, it also appears to have a computational advantage over the more traditional implementation of ART. Potential applications of this implementation include image reconstruction in conjunction with ray tracing for ultrasound and microwave tomography in which the curved nature of the rays leads to a non-uniform ray density across the image.},
  number = {1},
  journal = {Ultrasonic Imaging},
  author = {Andersen, A H and Kak, A C},
  month = jan,
  year = {1984},
  keywords = {Humans,Image Enhancement,Mathematics,Microwaves,Models; Biological,Software,Tomography,Ultrasonography},
  pages = {81--94},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/GWBP3VXW/Andersen et Kak - 1984 - Simultaneous algebraic reconstruction technique (S.pdf},
  pmid = {6548059}
}

@inproceedings{candes_compressive_2006,
  title = {Compressive Sampling},
  booktitle = {Proceedings Oh the {{International Congress}} of {{Mathematicians}}: {{Madrid}}, {{August}} 22-30, 2006: Invited Lectures},
  author = {Cand{\`e}s, Emmanuel J.},
  year = {2006},
  pages = {1433--1452},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/Q2URB9JI/CandÃ¨s - 2006 - Compressive sampling.pdf}
}

@phdthesis{champley_spect_2004,
  title = {{{SPECT}} Reconstruction Using the Expectation Maximization Algorithm and an Exact Inversion Formula},
  school = {Citeseer},
  author = {Champley, Kyle},
  year = {2004},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/4VUTF3WS/champley_masters_2004.pdf}
}

@article{rit_comparative_2011,
  title = {Comparative Study of Respiratory Motion Correction Techniques in Cone-Beam Computed Tomography},
  volume = {100},
  issn = {1879-0887},
  doi = {10.1016/j.radonc.2011.08.018},
  abstract = {BACKGROUND AND PURPOSE: To validate the clinical usefulness of motion-compensated (MC) cone-beam (CB) computed tomography (CT) for image-guided radiotherapy (IGRT) in comparison to four-dimensional (4D) CBCT and three-dimensional (3D) CBCT.
MATERIAL AND METHODS: Forty-eight stereotactic body radiation therapy (SBRT) patients were selected. Each patient had 5-12 long CB acquisitions (4 min) and 1-7 short CB acquisitions (1 min), with a total of 349 and 150 acquisitions, respectively. 3D, 4D and MC CBCT images of every acquisition were reconstructed. Image quality, tumor positioning accuracy and tumor motion amplitude were quantified.
RESULTS: The mean image quality of long short acquisitions, measured using the correlation ratio with the planning CT, was 74\%/70\%, 67\%/47\% and 79\%/74\% for 3D, 4D and MC CBCT, respectively; both 4D and MC CBCT were corrected for respiratory motion artifacts but 4D CBCTs suffered from streak artifacts. Tumor positioning with MC CBCT was significantly closer to 4D CBCT than 3D CBCT (p$<$0.0001). Detailed patient analysis showed that motion correction was not required for tumors with less than 1cm motion amplitude.
CONCLUSIONS: 4D and MC CBCT both allow accurate tumor position analysis under respiratory motion but 4D CBCT requires longer acquisition time than MC CBCT for adequate image quality. MC CBCT can therefore advantageously replace 4D CBCT in clinical protocols for patients with large motion to improve image quality and reduce acquisition time.},
  language = {eng},
  number = {3},
  journal = {Radiotherapy and Oncology},
  author = {Rit, Simon and Nijkamp, Jasper and {van Herk}, Marcel and Sonke, Jan-Jakob},
  month = sep,
  year = {2011},
  keywords = {Artifacts,Cone-Beam Computed Tomography,Female,Humans,Imaging; Three-Dimensional,Lung Neoplasms,Male,Radiographic Image Interpretation; Computer-Assisted,Radiosurgery,Radiotherapy; Image-Guided,Respiration,Retrospective Studies,Tomography; Spiral Computed},
  pages = {356--359},
  pmid = {21924782}
}

@inproceedings{starck_very_2001,
  title = {Very High Quality Image Restoration by Combining Wavelets and Curvelets},
  volume = {4478},
  doi = {10.1117/12.449693},
  abstract = {We outline digital implementations of two newly developed multiscale representation systems, namely, the ridgelet and curvelet transforms. We apply these digital transforms to the problem of restoring an image from noisy data and compare our results with those obtained via well established methods based on the thresholding of wavelet coefficients. We develop a methodology to combine wavelets together these new systems to perform noise removal by exploiting all these systems simultaneously. The results of the combined reconstruction exhibits clear advantages over any individual system alone. For example, the residual error contains essentially no visually intelligible structure: no structure is lost in the reconstruction.},
  author = {Starck, Jean-Luc and Donoho, David L. and Candes, Emmanuel J.},
  year = {2001},
  pages = {9--19},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/ADCQT4E5/spie01_vhqr.pdf}
}

@article{galigekere_techniques_1999,
  title = {Techniques to Alleviate the Effects of View Aliasing Artifacts in Computed Tomography},
  volume = {26},
  issn = {00942405},
  doi = {10.1118/1.598606},
  journal = {Medical Physics},
  author = {Galigekere, R. R. and Wiesent, K. and Holdsworth, D. W.},
  year = {1999},
  pages = {896},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/D4EQHSQN/Galigekere et al. - 1999 - Techniques to alleviate the effects of view aliasi.pdf}
}

@phdthesis{momey_reconstruction_2013,
  title = {Reconstruction En Tomographie Dynamique Par Approche Inverse sans Compensation de Mouvement},
  abstract = {La tomographie est la discipline qui cherche {\`a} reconstruire une donn{\'e}e physique dans son volume, {\`a} partir de l'information indirecte de projections int{\'e}gr{\'e}es de l'objet, {\`a} diff{\'e}rents angles de vue. L'une de ses applications les plus r{\'e}pandues, et qui constitue le cadre de cette th{\`e}se, est l'imagerie scanner par rayons X pour le m{\'e}dical. Or, les mouvements inh{\'e}rents {\`a} tout {\^e}tre vivant, typiquement le mouvement respiratoire et les battements cardiaques, posent de s{\'e}rieux probl{\`e}mes dans une reconstruction classique. Il est donc imp{\'e}ratif d'en tenir compte, i.e. de reconstruire le sujet imag{\'e} comme une s{\'e}quence spatio-temporelle traduisant son '{\'e}volution anatomique' au cours du temps : c'est la tomographie dynamique. {\'E}laborer une m{\'e}thode de reconstruction sp{\'e}cifique {\`a} ce probl{\`e}me est un enjeu majeur en radioth{\'e}rapie, o{\`u} la localisation pr{\'e}cise de la tumeur dans le temps est un pr{\'e}requis afin d'irradier les cellules canc{\'e}reuses en prot{\'e}geant au mieux les tissus sains environnants. Des m{\'e}thodes usuelles de reconstruction augmentent le nombre de projections acquises, permettant des reconstructions ind{\'e}pendantes de plusieurs phases de la s{\'e}quence {\'e}chantillonn{\'e}e en temps. D'autres compensent directement le mouvement dans la reconstruction, en mod{\'e}lisant ce dernier comme un champ de d{\'e}formation, estim{\'e} {\`a} partir d'un jeu de donn{\'e}es d'acquisition ant{\'e}rieur. Nous proposons dans ce travail de th{\`e}se une approche nouvelle ; se basant sur la th{\'e}orie des probl{\`e}mes inverses, nous affranchissons la reconstruction dynamique du besoin d'accroissement de la quantit{\'e} de donn{\'e}es, ainsi que de la recherche explicite du mouvement, elle aussi consommatrice d'un surplus d'information. Nous reconstruisons la s{\'e}quence dynamique {\`a} partir du seul jeu de projections courant, avec pour seules hypoth{\`e}ses a priori la continuit{\'e} et la p{\'e}riodicit{\'e} du mouvement. Le probl{\`e}me inverse est alors trait{\'e} rigoureusement comme la minimisation d'un terme d'attache aux donn{\'e}es et d'une r{\'e}gularisation. Nos contributions portent sur la mise au point d'une m{\'e}thode de reconstruction adapt{\'e}e {\`a} l'extraction optimale de l'information compte tenu de la parcimonie des donn{\'e}es - un aspect typique du probl{\`e}me dynamique - en utilisant notamment la variation totale (TV) comme r{\'e}gularisation. Nous {\'e}laborons un nouveau mod{\`e}le de projection tomographique pr{\'e}cis et comp{\'e}titif en temps de calcul, bas{\'e} sur des fonctions B-splines s{\'e}parables, permettant de repousser encore la limite de reconstruction impos{\'e}e par la parcimonie. Ces d{\'e}veloppements sont ensuite ins{\'e}r{\'e}s dans un sch{\'e}ma de reconstruction dynamique coh{\'e}rent, appliquant notamment une r{\'e}gularisation TV spatio-temporelle efficace. Notre m{\'e}thode exploite ainsi de fa{\c c}on optimale la seule information courante {\`a} disposition ; de plus sa mise en {\oe}uvre fait preuve d'une grande simplicit{\'e}. Nous faisons premi{\`e}rement la d{\'e}monstration de la force de notre approche sur des reconstructions 2-D+t {\`a} partir de donn{\'e}es simul{\'e}es num{\'e}riquement. La faisabilit{\'e} pratique de notre m{\'e}thode est ensuite {\'e}tablie sur des reconstructions 2-D et 3-D+t {\`a} partir de donn{\'e}es physiques 'r{\'e}elles', acquises sur un fant{\^o}me m{\'e}canique et sur un patient.},
  school = {Universit{\'e} Jean Monnet - Saint-Etienne},
  author = {Momey, Fabien},
  month = jun,
  year = {2013},
  keywords = {B-splines.,Imagerie mÃ©dicale,Reconstruction; ProblÃ¨mes inverses,Tomographie,Tomographie dynamique,Traitement du signal},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/88HDRSGH/Momey - 2013 - Reconstruction en tomographie dynamique par approc.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/GXZW73RW/tel-00842572.html}
}

@article{mccollough_achieving_2012,
  title = {Achieving {{Routine Submillisievert CT Scanning}}: {{Report}} from the {{Summit}} on {{Management}} of {{Radiation Dose}} in {{CT}}},
  volume = {264},
  issn = {0033-8419},
  shorttitle = {Achieving {{Routine Submillisievert CT Scanning}}},
  doi = {10.1148/radiol.12112265},
  abstract = {This report summarizes the advances in data acquisition, image reconstruction, and optimization processes that were identified by consensus as being necessary to achieve effective dose levels for routine CT that are well below background levels., This Special Report presents the consensus of the Summit on Management of Radiation Dose in Computed Tomography (CT) (held in February 2011), which brought together participants from academia, clinical practice, industry, and regulatory and funding agencies to identify the steps required to reduce the effective dose from routine CT examinations to less than 1 mSv. The most promising technologies and methods discussed at the summit include innovations and developments in x-ray sources; detectors; and image reconstruction, noise reduction, and postprocessing algorithms. Access to raw projection data and standard data sets for algorithm validation and optimization is a clear need, as is the need for new, clinically relevant metrics of image quality and diagnostic performance. Current commercially available techniques such as automatic exposure control, optimization of tube potential, beam-shaping filters, and dynamic z-axis collimators are important, and education to successfully implement these methods routinely is critically needed. Other methods that are just becoming widely available, such as iterative reconstruction, noise reduction, and postprocessing algorithms, will also have an important role. Together, these existing techniques can reduce dose by a factor of two to four. Technical advances that show considerable promise for additional dose reduction but are several years or more from commercial availability include compressed sensing, volume of interest and interior tomography techniques, and photon-counting detectors. This report offers a strategic roadmap for the CT user and research and manufacturer communities toward routinely achieving effective doses of less than 1 mSv, which is well below the average annual dose from naturally occurring sources of radiation., \textcopyright{} RSNA, 2012},
  number = {2},
  journal = {Radiology},
  author = {McCollough, Cynthia H. and Chen, Guang Hong and Kalender, Willi and Leng, Shuai and Samei, Ehsan and Taguchi, Katsuyuki and Wang, Ge and Yu, Lifeng and Pettigrew, Roderic I.},
  month = aug,
  year = {2012},
  pages = {567--580},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/QTEKM6NE/McCollough et al. - 2012 - Achieving Routine Submillisievert CT Scanning Rep.pdf},
  pmid = {22692035},
  pmcid = {PMC3401354}
}

@article{shepp_fourier_1974,
  title = {The {{Fourier}} Reconstruction of a Head Section},
  volume = {21},
  issn = {0018-9499},
  doi = {10.1109/TNS.1974.6499235},
  abstract = {The Fourier reconstruction may be viewed simply in the spatial domain as the sum of each line integral times a weighting function of the distance from the line to the point of reconstruction. A modified weighting function simultaneously achieves accuracy, simplicity, low computation time, as well as low sensitivity to noise. Using a simulated phantom, the authors compare the Fourier algorithm and a search algorithm. The search algorithm required 12 iterations to obtain a reconstruction of accuracy and resolution comparable to that of the Fourier reconstruction, and was more sensitive to noise. To speed the search algorithm by using fewer interactions leaves decreased resolution in the region just inside the skull which could mask a subdural hematoma.},
  number = {3},
  journal = {IEEE Transactions on Nuclear Science},
  author = {Shepp, L.A. and Logan, B.F.},
  year = {1974},
  keywords = {Brain,Fourier analysis,Fourier reconstruction,biomedical applications of computers,head section,line integral,radiography,search algorithm,simulated phantom,subdural hematoma,weighting function},
  pages = {21--43},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/8A7PMSIG/Shepp and Logan - 1974 - The Fourier reconstruction of a head section.pdf}
}

@article{bertram_scatter_2007,
  title = {Scatter {{Correction}} for {{Flat Detector Cone}}-{{Beam CT Based On Simulated Sphere Models}}},
  volume = {34},
  shorttitle = {{{SU}}-{{FF}}-{{I}}-22},
  journal = {Medical Physics},
  author = {Bertram, M. and Hohmann, S. and Wiegert, J.},
  year = {2007},
  pages = {2342},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/AM3PQBGJ/Bertram et al. - 2007 - Scatter Correction for Flat Detector Cone-Beam CT .pdf}
}

@article{iiduka_hybrid_2008,
  title = {Hybrid {{Conjugate Gradient Method}} for a {{Convex Optimization Problem}} over the {{Fixed}}-{{Point Set}} of a {{Nonexpansive Mapping}}},
  volume = {140},
  issn = {0022-3239, 1573-2878},
  doi = {10.1007/s10957-008-9463-6},
  number = {3},
  journal = {Journal of Optimization Theory and Applications},
  author = {Iiduka, H.},
  month = oct,
  year = {2008},
  pages = {463--475},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/5UIZXKJC/Iiduka - 2008 - Hybrid Conjugate Gradient Method for a Convex Opti.pdf}
}

@article{yamada_hybrid_2001,
  title = {The Hybrid Steepest Descent Method for the Variational Inequality Problem over the Intersection of Fixed Point Sets of Nonexpansive Mappings},
  volume = {8},
  journal = {Studies in Computational Mathematics},
  author = {Yamada, Isao},
  year = {2001},
  pages = {473--504},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/J8GJFC8G/iyamada2001.pdf}
}

@inproceedings{wu_spatial-temporal_2012,
  address = {San Diego, CA, USA},
  title = {Spatial-Temporal Total Variation Regularization ({{STTVR}}) for {{4D}}-{{CT}} Reconstruction},
  doi = {10.1117/12.911162},
  booktitle = {Proceedings of {{SPIE Medical Imaging}} 2012},
  author = {Wu, Haibo and Maier, Andreas and Fahrig, Rebecca and Hornegger, Joachim},
  editor = {Pelc, Norbert J. and Nishikawa, Robert M. and Whiting, Bruce R.},
  month = feb,
  year = {2012},
  pages = {83133J},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/UP73A85P/Wu et al. - 2012 - Spatial-temporal total variation regularization (S.pdf}
}

@inproceedings{de_buck_new_2013,
  title = {A New Approach for Prospectively Gated Cardiac Rotational Angiography},
  doi = {10.1117/12.2007904},
  author = {De Buck, Stijn and Dauwe, Dieter and Wielandts, Jean-Yves and Claus, Piet and Janssens, Stefan and Heidbuchel, Hein and Nuyens, Dieter},
  editor = {Nishikawa, Robert M. and Whiting, Bruce R.},
  month = mar,
  year = {2013},
  pages = {86682W},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/XI97JA8E/De Buck et al. - 2013 - A new approach for prospectively gated cardiac rot.pdf}
}

@article{thiagalingam_intraprocedural_2008,
  title = {Intraprocedural {{Volume Imaging}} of the {{Left Atrium}} and {{Pulmonary Veins}} with {{Rotational X}}-{{Ray Angiography}}: {{Implications}} for {{Catheter Ablation}} of {{Atrial Fibrillation}}},
  volume = {19},
  issn = {1045-3873, 1540-8167},
  shorttitle = {Intraprocedural {{Volume Imaging}} of the {{Left Atrium}} and {{Pulmonary Veins}} with {{Rotational X}}-{{Ray Angiography}}},
  doi = {10.1111/j.1540-8167.2007.01013.x},
  number = {3},
  journal = {Journal of Cardiovascular Electrophysiology},
  author = {Thiagalingam, Aravinda and Manzke, Robert and D'Avila, Andre and Ho, Ivan and Locke, Andrew H. and Ruskin, Jeremy N. and Chan, Raymond C. and Reddy, Vivek Y.},
  month = mar,
  year = {2008},
  pages = {293--300},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/KVZHW47T/Thiagalingam et al. - 2008 - Intraprocedural Volume Imaging of the Left Atrium .pdf}
}

@article{rangayyan_algorithms_1985,
  title = {Algorithms for Limited-View Computed Tomography: An Annotated Bibliography and a Challenge},
  volume = {24},
  shorttitle = {Algorithms for Limited-View Computed Tomography},
  doi = {10.1364/AO.24.004000},
  abstract = {In many applications of computed tomography, it may not be possible to acquire projection data at all angles, as required by the most commonly used algorithm of convolution backprojection. In such a limited-data situation, we face an ill-posed problem in attempting to reconstruct an image from an incomplete set of projections. Many techniques have been proposed to tackle this situation, employing diverse theories such as signal recovery, image restoration, constrained deconvolution, and constrained optimization, as well as novel schemes such as iterative object-dependent algorithms incorporating a priori knowledge and use of multi-spectral radiation. We present an overview of such techniques and offer a challenge to all readers to reconstruct images from a set of limited-view data provided here.},
  number = {23},
  journal = {Applied Optics},
  author = {Rangayyan, Rangaraj and Dhawan, Atam Prakash and Gordon, Richard},
  month = dec,
  year = {1985},
  pages = {4000--4012},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/XMT4GT6U/Rangayyan et al. - 1985 - Algorithms for limited-view computed tomography a.pdf}
}

@article{modat_fast_2010,
  title = {Fast Free-Form Deformation Using Graphics Processing Units},
  volume = {98},
  issn = {1872-7565},
  doi = {10.1016/j.cmpb.2009.09.002},
  abstract = {A large number of algorithms have been developed to perform non-rigid registration and it is a tool commonly used in medical image analysis. The free-form deformation algorithm is a well-established technique, but is extremely time consuming. In this paper we present a parallel-friendly formulation of the algorithm suitable for graphics processing unit execution. Using our approach we perform registration of T1-weighted MR images in less than 1 min and show the same level of accuracy as a classical serial implementation when performing segmentation propagation. This technology could be of significant utility in time-critical applications such as image-guided interventions, or in the processing of large data sets.},
  language = {eng},
  number = {3},
  journal = {Computer Methods and Programs in Biomedicine},
  author = {Modat, Marc and Ridgway, Gerard R. and Taylor, Zeike A. and Lehmann, Manja and Barnes, Josephine and Hawkes, David J. and Fox, Nick C. and Ourselin, S{\'e}bastien},
  month = jun,
  year = {2010},
  keywords = {Algorithms,Computer Graphics,Diagnostic Imaging,Image Processing; Computer-Assisted,Software},
  pages = {278--284},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/CZ9JETBI/Modat et al. - 2010 - Fast free-form deformation using graphics processi.pdf},
  pmid = {19818524}
}

@article{badea_4d_2011,
  title = {{{4D}} Micro-{{CT}} for Cardiac and Perfusion Applications with View under Sampling},
  volume = {56},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/56/11/011},
  number = {11},
  journal = {Physics in Medicine and Biology},
  author = {Badea, Cristian T and Johnston, Samuel M and Qi, Yi and Johnson, G Allan},
  month = jun,
  year = {2011},
  pages = {3351--3369},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/SGXUW9ID/Badea et al. - 2011 - 4D micro-CT for cardiac and perfusion applications.pdf}
}

@article{dhawan_image_1985,
  title = {Image Restoration by {{Wiener}} Deconvolution in Limited-View Computed Tomography},
  volume = {24},
  doi = {10.1364/AO.24.004013},
  abstract = {In many applications of computed tomography, we cannot acquire the projection data at all angles evenly spaced over 360$^\circ$. In such cases, the computed tomography images reconstructed using a limited number of projections, measured over a narrow angle range, are characterized by approximately elliptical distortion along the view angles used and poor contrast at angles not used (anisotropic resolution). This systematic geometric distortion is caused by the 2-D point spread function of the reconstruction process. In this paper, we show that such geometric distortion and other artifacts introduced in the reconstruction process can be reduced substantially by deconvolution performed via Wiener filtering using a priori knowledge derived from the given projections. The 2-D system transfer function used in the deconvolution is obtained from the reconstruction of a test image by the same reconstruction algorithm which has been used for reconstructing the unknown object.},
  number = {23},
  journal = {Applied Optics},
  author = {Dhawan, Atam Prakash and Rangayyan, Rangaraj M. and Gordon, Richard},
  month = dec,
  year = {1985},
  pages = {4013--4020},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/DEEU8WBZ/Dhawan et al. - 1985 - Image restoration by Wiener deconvolution in limit.pdf}
}

@article{becker_nesta_2011,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0904.3367},
  title = {{{NESTA}}: {{A Fast}} and {{Accurate First}}-Order {{Method}} for {{Sparse Recovery}}},
  volume = {4},
  issn = {1936-4954},
  shorttitle = {{{NESTA}}},
  doi = {10.1137/090756855},
  abstract = {Accurate signal recovery or image reconstruction from indirect and possibly undersampled data is a topic of considerable interest; for example, the literature in the recent field of compressed sensing is already quite immense. Inspired by recent breakthroughs in the development of novel first-order methods in convex optimization, most notably Nesterov's smoothing technique, this paper introduces a fast and accurate algorithm for solving common recovery problems in signal processing. In the spirit of Nesterov's work, one of the key ideas of this algorithm is a subtle averaging of sequences of iterates, which has been shown to improve the convergence properties of standard gradient-descent algorithms. This paper demonstrates that this approach is ideally suited for solving large-scale compressed sensing reconstruction problems as 1) it is computationally efficient, 2) it is accurate and returns solutions with several correct digits, 3) it is flexible and amenable to many kinds of reconstruction problems, and 4) it is robust in the sense that its excellent performance across a wide range of problems does not depend on the fine tuning of several parameters. Comprehensive numerical experiments on realistic signals exhibiting a large dynamic range show that this algorithm compares favorably with recently proposed state-of-the-art methods. We also apply the algorithm to solve other problems for which there are fewer alternatives, such as total-variation minimization, and convex programs seeking to minimize the l1 norm of Wx under constraints, in which W is not diagonal.},
  number = {1},
  journal = {SIAM Journal on Imaging Sciences},
  author = {Becker, Stephen and Bobin, Jerome and Candes, Emmanuel},
  month = jan,
  year = {2011},
  keywords = {Mathematics - Optimization and Control},
  pages = {1--39},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/S5MMKPNZ/Becker et al. - 2011 - NESTA A Fast and Accurate First-order Method for .pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/XU9Q7GIM/0904.html}
}

@article{vaishnav_objective_2014,
  title = {Objective Assessment of Image Quality and Dose Reduction in {{CT}} Iterative Reconstruction},
  volume = {41},
  issn = {0094-2405},
  doi = {10.1118/1.4881148},
  abstract = {Purpose: Iterative reconstruction (IR) algorithms have the potential to reduce radiation dose in CT diagnostic imaging. As these algorithms become available on the market, a standardizable method of quantifying the dose reduction that a particular IR method can achieve would be valuable. Such a method would assist manufacturers in making promotional claims about dose reduction, buyers in comparing different devices, physicists in independently validating the claims, and the United States Food and Drug Administration in regulating the labeling of CT devices. However, the nonlinear nature of commercially available IR algorithms poses challenges to objectively assessing image quality, a necessary step in establishing the amount of dose reduction that a given IR algorithm can achieve without compromising that image quality. This review paper seeks to consolidate information relevant to objectively assessing the quality of CT IR images, and thereby measuring the level of dose reduction that a given IR algorithm can achieve. Methods: The authors discuss task-based methods for assessing the quality of CT IR images and evaluating dose reduction. Results: The authors explain and review recent literature on signal detection and localization tasks in CT IR image quality assessment, the design of an appropriate phantom for these tasks, possible choices of observers (including human and model observers), and methods of evaluating observer performance. Conclusions: Standardizing the measurement of dose reduction is a problem of broad interest to the CT community and to public health. A necessary step in the process is the objective assessment of CT image quality, for which various task-based methods may be suitable. This paper attempts to consolidate recent literature that is relevant to the development and implementation of task-based methods for the assessment of CT IR image quality.},
  number = {7},
  journal = {Medical Physics},
  author = {Vaishnav, J. Y. and Jung, W. C. and Popescu, L. M. and Zeng, R. and Myers, K. J.},
  month = jul,
  year = {2014},
  keywords = {Computed tomography,Image quality assessment,Medical image noise,Medical image reconstruction,dosimetry},
  pages = {071904},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/89TV3M56/Vaishnav et al. - 2014 - Objective assessment of image quality and dose red.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/CXPKE49P/1.html}
}

@inproceedings{mory_improving_2015,
  address = {Newport, Rhode Island, USA},
  title = {Improving Iterative {{4D CBCT}} through the Use of Motion Information},
  booktitle = {Proceedings of {{Fully 3D}} 2015},
  author = {Mory, Cyril and Rit, Simon},
  month = jun,
  year = {2015}
}

@article{muller_image_2014,
  title = {Image {{Artefact Propagation}} in {{Motion Estimation}} and {{Reconstruction}} in {{Interventional Cardiac C}}-Arm {{CT}}},
  volume = {59},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/59/12/3121},
  abstract = {The acquisition of data for cardiac imaging using a C-arm CT system requires several seconds and multiple heartbeats. Hence, incorporation of motion correction in the reconstruction step may improve the resulting image quality. Cardiac motion can be estimated by deformable 3-D/3-D registration performed on initial 3-D images of different heart phases. This motion information can be used for a motion-compensated reconstruction allowing the use of all acquired data for image reconstruction. However, the result of the registration procedure and hence the estimated deformations are influenced by the quality of the initial 3-D images. In this paper, the sensitivity of the 3-D/3-D registration step to the image quality of the initial images is studied. Different reconstruction algorithms are evaluated for a recently proposed cardiac C-arm CT acquisition protocol. The initial 3-D images are all based on retrospective electrocardiogram (ECG)-gated data. ECG-gating of data from a single C-arm rotation provides only a few projections per heart phase for image reconstruction. This view sparsity leads to prominent streak artefacts and a poor signal to noise ratio. Five different initial image reconstructions are evaluated: (1) cone beam filtered-backprojection (FDK), (2) cone beam filtered-backprojection and an additional bilateral filter (FFDK), (3) removal of the shadow of dense objects (catheter, pacing electrode, etc.) before reconstruction with a cone beam filtered-backprojection (cathFDK), (4) removal of the shadow of dense objects before reconstruction with a cone beam filtered-backprojection and a bilateral filter (cathFFDK). The last method (5) is an iterative few-view reconstruction (FV), the prior image constrained compressed sensing (PICCS) combined with the improved total variation (iTV) algorithm. All reconstructions are investigated with respect to the final motion-compensated reconstruction quality. The algorithms were tested on a mathematical phantom data set with and without a catheter and on two porcine models using qualitative and quantitative measures. The quantitative results of the phantom experiments show that if no dense object is present within the scan field of view, the quality of the FDK initial images is sufficient for motion estimation via 3-D/3-D registration. When a catheter or pacing electrode is present, the shadow of these objects needs to be removed before the initial image reconstruction. An additional bilateral filter shows no major improvement with respect to the final motion-compensated reconstruction quality. The results with respect to image quality of the cathFDK, cathFFDK and FV images are comparable. As conclusion, in terms of computational complexity, the algorithm of choice is the cathFDK algorithm.},
  number = {12},
  journal = {Physics in medicine and biology},
  author = {M{\"u}ller, K and Maier, A K and Schwemmer, C and Lauritsch, G and De Buck, S and Wielandts, J-Y and Hornegger, J and Fahrig, R},
  month = jun,
  year = {2014},
  pages = {3121--3138},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/BTWD8EH7/MÃ¼ller et al. - 2014 - Image Artefact Propagation in Motion Estimation an.pdf},
  pmid = {24840084},
  pmcid = {PMC4110354}
}

@article{roessl_cramer-rao_2009,
  title = {Cram{\'e}r-{{Rao}} Lower Bound of Basis Image Noise in Multiple-Energy x-Ray Imaging},
  volume = {54},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/54/5/014},
  abstract = {We present an analytical method to compute the basis image noise in the context of multi-energy x-ray imaging based on the Cram{\'e}r\textendash{}Rao lower bound (CRLB). The proposed formalism extends the original idea of Alvarez and Macovski ( 1976 Phys. Med. Biol. [/0031-9155/21/5/002] 21 733 ) to estimate the noise in the photo-effect and Compton-effect basis images in the case of dual-energy imaging. It includes an arbitrary number of independent, spectrally distinct attenuation measurements and also goes beyond the two-dimensional decomposition of the attenuation, including, e.g., a contrast agent as a third basis material. To illustrate our method, we consider three simple applications. The first application is to study the influence of the exact values for the energy thresholds on the basis image noise for a binned photon-counting system. The second application relates to the same detector system as the first and is an investigation of the dependence of the basis image noise on the energy resolution of the detector system. Finally, the third application provides an example for the case of an energy-integrating detector: the aim is to optimize the front-scintillator layer thickness of a dual-crystal detector for dual-energy imaging. The CRLB is used to minimize the noise of a photo-effect/Compton-effect basis material decomposition.},
  language = {en},
  number = {5},
  journal = {Physics in Medicine and Biology},
  author = {Roessl, E. and Herrmann, C.},
  year = {2009},
  pages = {1307},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/W835HJXE/Roessl and Herrmann - 2009 - CramÃ©r-Rao lower bound of basis image noise in mul.pdf}
}

@incollection{navab_estimate_2015,
  address = {Cham},
  title = {Estimate, {{Compensate}}, {{Iterate}}: {{Joint Motion Estimation}} and {{Compensation}} in 4-{{D Cardiac C}}-Arm {{Computed Tomography}}},
  volume = {9350},
  isbn = {978-3-319-24570-6 978-3-319-24571-3},
  shorttitle = {Estimate, {{Compensate}}, {{Iterate}}},
  booktitle = {Medical {{Image Computing}} and {{Computer}}-{{Assisted Intervention}} -- {{MICCAI}} 2015},
  publisher = {{Springer International Publishing}},
  author = {Taubmann, Oliver and Lauritsch, G{\"u}nter and Maier, Andreas and Fahrig, Rebecca and Hornegger, Joachim},
  editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
  year = {2015},
  pages = {579--586},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/3SG3SDPF/Taubmann et al. - 2015 - Estimate, Compensate, Iterate Joint Motion Estima.pdf}
}

@article{mory_cardiac_2014,
  title = {Cardiac {{C}}-Arm Computed Tomography Using a {{3D}} + Time {{ROI}} Reconstruction Method with Spatial and Temporal Regularization},
  volume = {41},
  issn = {0094-2405},
  doi = {10.1118/1.4860215},
  abstract = {PURPOSE: Reconstruction of the beating heart in 3D + time in the catheter laboratory using only the available C-arm system would improve diagnosis, guidance, device sizing, and outcome control for intracardiac interventions, e.g., electrophysiology, valvular disease treatment, structural or congenital heart disease. To obtain such a reconstruction, the patient's electrocardiogram (ECG) must be recorded during the acquisition and used in the reconstruction. In this paper, the authors present a 4D reconstruction method aiming to reconstruct the heart from a single sweep 10 s acquisition.
METHODS: The authors introduce the 4D RecOnstructiOn using Spatial and TEmporal Regularization (short 4D ROOSTER) method, which reconstructs all cardiac phases at once, as a 3D + time volume. The algorithm alternates between a reconstruction step based on conjugate gradient and four regularization steps: enforcing positivity, averaging along time outside a motion mask that contains the heart and vessels, 3D spatial total variation minimization, and 1D temporal total variation minimization.
RESULTS: 4D ROOSTER recovers the different temporal representations of a moving Shepp and Logan phantom, and outperforms both ECG-gated simultaneous algebraic reconstruction technique and prior image constrained compressed sensing on a clinical case. It generates 3D + time reconstructions with sharp edges which can be used, for example, to estimate the patient's left ventricular ejection fraction.
CONCLUSIONS: 4D ROOSTER can be applied for human cardiac C-arm CT, and potentially in other dynamic tomography areas. It can easily be adapted to other problems as regularization is decoupled from projection and back projection.},
  language = {eng},
  number = {2},
  journal = {Medical physics},
  author = {Mory, Cyril and Auvray, Vincent and Zhang, Bo and Grass, Michael and Sch{\"a}fer, Dirk and Chen, S James and Carroll, John D and Rit, Simon and Peyrin, Fran{\c c}oise and Douek, Philippe and Boussel, Lo{\"\i}c},
  month = feb,
  year = {2014},
  pages = {021903},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/ER4EE2FT/Mory et al. - Cardiac C-arm Computed Tomography using a 3D + tim.pdf},
  pmid = {24506624}
}

@article{persliden_scatter_1997,
  title = {Scatter Rejection by Air Gaps in Diagnostic Radiology. {{Calculations}} Using a {{Monte Carlo}} Collision Density Method and Consideration of Molecular Interference in Coherent Scattering},
  volume = {42},
  number = {1},
  journal = {Physics in medicine and biology},
  author = {Persliden, Jan and Carlsson, Gudrun Alm},
  year = {1997},
  pages = {155},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/ZI9CFJXD/pdf.pdf}
}

@article{holmes_acc_1998,
  title = {{{ACC}} Expert Consensus Document on Coronary Artery {{stentsDocument}} of the {{American College}} of {{Cardiology1}}},
  volume = {32},
  issn = {0735-1097},
  doi = {10.1016/S0735-1097(98)00427-6},
  number = {5},
  journal = {Journal of the American College of Cardiology},
  author = {Holmes, DavidR, Jr. and Hirshfeld, John, Jr. and Faxon, David and Vlietstra, RonaldE and Jacobs, Alice and King, SpencerB, III and Bashore, ThomasM and Bridges, NancyD and Higgins, CharlesB and Hiratzka, LorenF and Little, WilliamC and Magorien, RaymondD and Nocero, MichaelA, Jr. and Oesterle, Stephen and Vogel, RobertA and Forrester, JamesS and Douglas, PamelaS and Faxon, DavidP and Fisher, JohnD and Gibbons, RaymondJ and Halperin, JonathanL and Hutter, AdolphM, Jr. and Hochman, JudithS and Kaul, Sanjiv and Weintraub, WilliamS and Winters, WilliamL, Jr. and Wolk, MichaelJ},
  month = nov,
  year = {1998},
  keywords = {angioplasty; transluminal; percutaneous coronary,restenosis,stent; device},
  pages = {1471--1482}
}

@inproceedings{figueiredo_fast_2009,
  title = {Fast Frame-Based Image Deconvolution Using Variable Splitting and Constrained Optimization},
  booktitle = {Statistical {{Signal Processing}}, 2009. {{SSP}}'09. {{IEEE}}/{{SP}} 15th {{Workshop}} On},
  author = {Figueiredo, M. and Bioucas-Dias, J.M. and Afonso, M.V.},
  year = {2009},
  pages = {109--112},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/X8PHCC9H/Figueiredo et al. - 2009 - Fast frame-based image deconvolution using variabl.pdf}
}

@phdthesis{wils_tomographie_2011,
  type = {{THESE}},
  title = {{Tomographie par rayons X : correction des artefacts li{\'e}s {\`a} la cha{\^\i}ne d'acquisition}},
  shorttitle = {{Tomographie par rayons X}},
  abstract = {L'imagerie cone-beam computed tomography (CBCT) est une m{\'e}thodologie de contr{\^o}le non destructif permettant l'obtention d'images volumiques d'un objet. Le syst{\`e}me d'acquisition se compose d'un tube {\`a} rayons X et d'un d{\'e}tecteur plan num{\'e}rique. La recherche d{\'e}velopp{\'e}e dans ce manuscrit se d{\'e}roule dans le contexte industriel. L'objet est plac{\'e} sur une platine de rotation et une s{\'e}quence d'images 2D est acquise. Un algorithme de reconstruction procure des donn{\'e}es volumiques de l'att{\'e}nuation de l'objet. Ces informations permettent de r{\'e}aliser une {\'e}tude m{\'e}trologique et de valider ou non la conformit{\'e} de la pi{\`e}ce imag{\'e}e. La qualit{\'e} de l'image 3D est d{\'e}grad{\'e}e par diff{\'e}rents artefacts inh{\'e}rents {\`a} la plateforme d'acquisition. L'objectif de cette th{\`e}se est de mettre au point une m{\'e}thode de correction adapt{\'e}e {\`a} une plateforme de micro-tomographie par rayons X d'objets manufactur{\'e}s poly-mat{\'e}riaux. Le premier chapitre d{\'e}crit les bases de la physique et de l'algorithmie propres {\`a} la technique d'imagerie CBCT par rayons X ainsi que les diff{\'e}rents artefacts nuisant {\`a} la qualit{\'e} de l'image finale. Le travail pr{\'e}sent{\'e} ici se concentre sur deux types d'artefacts en particulier: les rayonnements secondaires issus de l'objet et du d{\'e}tecteur et le durcissement de faisceau. Le second chapitre pr{\'e}sente un {\'e}tat de l'art des m{\'e}thodes visant {\`a} corriger le rayonnement secondaire. Afin de quantifier le rayonnement secondaire, un outil de simulation bas{\'e} sur des techniques de Monte Carlo hybride est d{\'e}velopp{\'e}. Il permet de caract{\'e}riser le syst{\`e}me d'acquisition install{\'e} au laboratoire de fa{\c c}on r{\'e}aliste. Le troisi{\`e}me chapitre d{\'e}taille la mise en place et la validation de cet outil. Les calculs Monte Carlo {\'e}tant particuli{\`e}rement prohibitifs en terme de temps de calcul, des techniques d'optimisation et d'acc{\'e}l{\'e}ration sont d{\'e}crites. Le comportement du d{\'e}tecteur est {\'e}tudi{\'e} avec attention et il s'av{\`e}re qu'une repr{\'e}sentation 2D suffit pour mod{\'e}liser le rayonnement secondaire. Le mod{\`e}le de simulation permet une reproduction fid{\`e}le des projections acquises avec le syst{\`e}me r{\'e}el. Enfin, le dernier chapitre pr{\'e}sente la m{\'e}thodologie de correction que nous proposons. Une premi{\`e}re reconstruction bruit{\'e}e de l'objet imag{\'e} est segment{\'e}e afin d'obtenir un mod{\`e}le vox{\'e}lis{\'e} en densit{\'e}s et en mat{\'e}riaux. L'environnement de simulation fournit alors les projections associ{\'e}es {\`a} ce volume. Le volume est corrig{\'e} de fa{\c c}on it{\'e}rative. Des r{\'e}sultats de correction d'images tomographiques exp{\'e}rimentales sont pr{\'e}sent{\'e}s dans le cas d'un objet mono-mat{\'e}riaux et d'un objet poly-mat{\'e}riaux. Notre routine de correction r{\'e}duit les artefacts de cupping et am{\'e}liore la description du volume reconstruit.},
  language = {Fran{\c c}ais},
  school = {INSA de Lyon},
  author = {Wils, Patricia},
  month = nov,
  year = {2011},
  keywords = {CBCT - Cone-beam computed tomography,Imagerie 3D,Imagerie quantitative,Micro-tomographie X,Tomographie par rayons X,Tomographie quantitative},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/S8PMVBJP/Wils - 2011 - Tomographie par rayons X  correction des artefact.pdf}
}

@article{gottlieb_direct_2000,
  title = {On the Direct {{Fourier}} Method for Computer Tomography},
  volume = {19},
  number = {3},
  journal = {IEEE Trans. Med. Imaging},
  author = {Gottlieb, David and Gustafsson, Bertil and Forssen, Patrik},
  year = {2000},
  pages = {223--232},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/IIGI28V8/OnTheDirectFourierMethodForComputerTomography.pdf}
}

@misc{_image_,
  title = {Image {{Denoising}} with {{Wavelets}}},
  howpublished = {http://www.numerical-tours.com/matlab/denoisingwav\_2\_wavelet\_2d/},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/M9CDDQNX/denoisingwav_2_wavelet_2d.html}
}

@article{afonso_augmented_2011,
  title = {An {{Augmented Lagrangian Approach}} to the {{Constrained Optimization Formulation}} of {{Imaging Inverse Problems}}},
  volume = {20},
  issn = {1057-7149},
  doi = {10.1109/TIP.2010.2076294},
  abstract = {We propose a new fast algorithm for solving one of the standard approaches to ill-posed linear inverse problems (IPLIP), where a (possibly nonsmooth) regularizer is minimized under the constraint that the solution explains the observations sufficiently well. Although the regularizer and constraint are usually convex, several particular features of these problems (huge dimensionality, nonsmoothness) preclude the use of off-the-shelf optimization tools and have stimulated a considerable amount of research. In this paper, we propose a new efficient algorithm to handle one class of constrained problems (often known as basis pursuit denoising) tailored to image recovery applications. The proposed algorithm, which belongs to the family of augmented Lagrangian methods, can be used to deal with a variety of imaging IPLIP, including deconvolution and reconstruction from compressive observations (such as MRI), using either total-variation or wavelet-based (or, more generally, frame-based) regularization. The proposed algorithm is an instance of the so-called alternating direction method of multipliers, for which convergence sufficient conditions are known; we show that these conditions are satisfied by the proposed algorithm. Experiments on a set of image restoration and reconstruction benchmark problems show that the proposed algorithm is a strong contender for the state-of-the-art.},
  number = {3},
  journal = {IEEE Transactions on Image Processing},
  author = {Afonso, M.V. and Bioucas-Dias, J.M. and Figueiredo, M.A.T.},
  month = mar,
  year = {2011},
  keywords = {IPLIP;augmented,Lagrangian,approach;constrained,deconvolution;imaging,inverse,linear,optimization;ill-posed,problems;image,problems;optimisation;,reconstruction;total,recovery;image,restoration;imaging,restoration;inverse,variation;deconvolution;image},
  pages = {681 --695},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/T7BFF7F2/Augmented Lagrangian Frame based Inverse Problem.Figueiredo.2010.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/KT64CDUW/login.html}
}

@misc{_deconvolution_,
  title = {Deconvolution and Blind Deconvolution in Astronomy},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/P7Z7W6KU/Deconvolution and blind deconvolution in astronomy.pdf}
}

@article{afonso_fast_2010,
  title = {Fast {{Image Recovery Using Variable Splitting}} and {{Constrained Optimization}}},
  volume = {19},
  issn = {1057-7149},
  doi = {10.1109/TIP.2010.2047910},
  abstract = {We propose a new fast algorithm for solving one of the standard formulations of image restoration and reconstruction which consists of an unconstrained optimization problem where the objective includes an \emph{l}\textsubscript{2} data-fidelity term and a nonsmooth regularizer. This formulation allows both wavelet-based (with orthogonal or frame-based representations) regularization or total-variation regularization. Our approach is based on a variable splitting to obtain an equivalent constrained optimization formulation, which is then addressed with an augmented Lagrangian method. The proposed algorithm is an instance of the so-called alternating direction method of multipliers, for which convergence has been proved. Experiments on a set of image restoration and reconstruction benchmark problems show that the proposed algorithm is faster than the current state of the art methods.},
  number = {9},
  journal = {IEEE Transactions on Image Processing},
  author = {Afonso, M.V. and Bioucas-Dias, J.M. and Figueiredo, M.A.T.},
  month = sep,
  year = {2010},
  keywords = {Lagrangian,alternating,constrained,data-fidelity,direction,formulation;fast,image,method;augmented,method;convergence;equivalent,multiplier,optimization,problem;variable,reconstruction;image,recovery;image,regularization;image,regularization;unconstrained,regularizer;total-variation,restoration;l<sub>2</sub>,restoration;optimisation;wavelet,splitting;wavelet-based,term;nonsmooth,transforms;},
  pages = {2345 --2356},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/7T937MU9/Afonso et al. - 2010 - Fast Image Recovery Using Variable Splitting and C.pdf}
}

@article{schirra_improvement_2009,
  title = {Improvement of Cardiac {{CT}} Reconstruction Using Local Motion Vector Fields},
  volume = {33},
  issn = {0895-6111},
  doi = {16/j.compmedimag.2008.10.012},
  abstract = {The motion of the heart is a major challenge for cardiac imaging using CT. A novel approach to decrease motion blur and to improve the signal to noise ratio is motion compensated reconstruction which takes motion vector fields into account in order to correct motion. The presented work deals with the determination of local motion vector fields from high contrast objects and their utilization within motion compensated filtered back projection reconstruction. Image registration is applied during the quiescent cardiac phases. Temporal interpolation in parameter space is used in order to estimate motion during strong motion phases. The resulting motion vector fields are during image reconstruction. The method is assessed using a software phantom and several clinical cases for calcium scoring. As a criterion for reconstruction quality, calcium volume scores were derived from both, gated cardiac reconstruction and motion compensated reconstruction throughout the cardiac phases using low pitch helical cone beam CT acquisitions. The presented technique is a robust method to determine and utilize local motion vector fields. Motion compensated reconstruction using the derived motion vector fields leads to superior image quality compared to gated reconstruction. As a result, the gating window can be enlarged significantly, resulting in increased SNR, while reliable Hounsfield units are achieved due to the reduced level of motion artefacts. The enlargement of the gating window can be translated into reduced dose requirements.},
  number = {2},
  journal = {Computerized Medical Imaging and Graphics},
  author = {Schirra, Carsten Oliver and Bontus, Claas and {van Stevendaal}, Udo and D{\"o}ssel, Olaf and Grass, Michael},
  month = mar,
  year = {2009},
  keywords = {Calcium scoring,Cardiac CT,Motion compensated reconstruction,Motion model},
  pages = {122--130},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/S4NEJM6H/Schirra et al. - 2009 - Improvement of cardiac CT reconstruction using loc.pdf}
}

@article{movassaghi_quantitative_2004,
  title = {A {{Quantitative Analysis}} of 3-{{D Coronary Modeling From Two}} or {{More Projection Images}}},
  volume = {23},
  issn = {0278-0062},
  doi = {10.1109/TMI.2004.837340},
  number = {12},
  journal = {IEEE Transactions on Medical Imaging},
  author = {Movassaghi, B. and Rasche, V. and Grass, M. and Viergever, M.A. and Niessen, W.J.},
  month = dec,
  year = {2004},
  pages = {1517--1531},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/IJ5QCDVB/Movassaghi et al. - 2004 - A Quantitative Analysis of 3-D Coronary Modeling F.pdf}
}

@article{dong_x-ray_2013,
  title = {X-{{Ray CT Image Reconstruction}} via {{Wavelet Frame Based Regularization}} and {{Radon Domain Inpainting}}},
  volume = {54},
  issn = {0885-7474, 1573-7691},
  doi = {10.1007/s10915-012-9579-6},
  abstract = {X-ray computed tomography (CT) has been playing an important role in diagnostic of cancer and radiotherapy. However, high imaging dose added to healthy organs during CT scans is a serious clinical concern. Imaging dose in CT scans can be reduced by reducing the number of X-ray projections. In this paper, we consider 2D CT reconstructions using very small number of projections. Some regularization based reconstruction methods have already been proposed in the literature for such task, like the total variation (TV) based reconstruction (Sidky and Pan in Phys. Med. Biol. 53:4777, 2008; Sidky et al. in J. X-Ray Sci. Technol. 14(2):119\textendash{}139, 2006; Jia et al. in Med. Phys. 37:1757, 2010; Choi et al. in Med. Phys. 37:5113, 2010) and balanced approach with wavelet frame based regularization (Jia et al. in Phys. Med. Biol. 56:3787\textendash{}3807, 2011). For most of the existing methods, at least 40 projections is usually needed to get a satisfactory reconstruction. In order to keep radiation dose as minimal as possible, while increase the quality of the reconstructed images, one needs to enhance the resolution of the projected image in the Radon domain without increasing the total number of projections. The goal of this paper is to propose a CT reconstruction model with wavelet frame based regularization and Radon domain inpainting. The proposed model simultaneously reconstructs a high quality image and its corresponding high resolution measurements in Radon domain. In addition, we discovered that using the isotropic wavelet frame regularization proposed in Cai et al. (Image restorations: total variation, wavelet frames and beyond, 2011, preprint) is superior than using its anisotropic counterpart. Our proposed model, as well as other models presented in this paper, is solved rather efficiently by split Bregman algorithm (Goldstein and Osher in SIAM J. Imaging Sci. 2(2):323\textendash{}343, 2009; Cai et al. in Multiscale Model. Simul. 8(2):337\textendash{}369, 2009). Numerical simulations and comparisons will be presented at the end.},
  language = {en},
  number = {2-3},
  journal = {Journal of Scientific Computing},
  author = {Dong, Bin and Li, Jia and Shen, Zuowei},
  month = feb,
  year = {2013},
  keywords = {Algorithms,Appl.Mathematics/Computational Methods of Engineering,Computational Mathematics and Numerical Analysis,Computed tomography,Radon domain inpainting,Split Bregman algorithm,Theoretical; Mathematical and Computational Physics,Total variation,Wavelet frame},
  pages = {333--349},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/ITZ8R5D4/Dong et al. - 2013 - X-Ray CT Image Reconstruction via Wavelet Frame Ba.pdf}
}

@article{rigie_joint_2015,
  title = {Joint Reconstruction of Multi-Channel, Spectral {{CT}} Data via Constrained Total Nuclear Variation Minimization},
  volume = {60},
  issn = {0031-9155, 1361-6560},
  doi = {10.1088/0031-9155/60/5/1741},
  number = {5},
  journal = {Physics in Medicine and Biology},
  author = {Rigie, David S and La Rivi{\`e}re, Patrick J},
  month = feb,
  year = {2015},
  pages = {1741--1762},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/K9BJEK84/Rigie and La RiviÃ¨re - 2015 - Joint reconstruction of multi-channel, spectral CT.pdf}
}

@article{daubechies_iterative_2004,
  title = {An Iterative Thresholding Algorithm for Linear Inverse Problems with a Sparsity Constraint},
  volume = {57},
  copyright = {Copyright \textcopyright{} 2004 Wiley Periodicals, Inc.},
  issn = {1097-0312},
  doi = {10.1002/cpa.20042},
  abstract = {We consider linear inverse problems where the solution is assumed to have a sparse expansion on an arbitrary preassigned orthonormal basis. We prove that replacing the usual quadratic regularizing penalties by weighted $\dottedsquare\dottedsquare$p-penalties on the coefficients of such expansions, with 1 $\leq$ p $\leq$ 2, still regularizes the problem. Use of such $\dottedsquare\dottedsquare$p-penalized problems with p $<$ 2 is often advocated when one expects the underlying ideal noiseless solution to have a sparse expansion with respect to the basis under consideration. To compute the corresponding regularized solutions, we analyze an iterative algorithm that amounts to a Landweber iteration with thresholding (or nonlinear shrinkage) applied at each iteration step. We prove that this algorithm converges in norm. \textcopyright{} 2004 Wiley Periodicals, Inc.},
  language = {en},
  number = {11},
  journal = {Communications on Pure and Applied Mathematics},
  author = {Daubechies, I. and Defrise, M. and De Mol, C.},
  year = {2004},
  pages = {1413--1457},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/VA4JGVU5/Daubechies et al. - 2004 - An iterative thresholding algorithm for linear inv.pdf}
}

@article{ehrhardt_joint_2015,
  title = {Joint Reconstruction of {{PET}}-{{MRI}} by Exploiting Structural Similarity},
  volume = {31},
  issn = {0266-5611, 1361-6420},
  doi = {10.1088/0266-5611/31/1/015001},
  number = {1},
  journal = {Inverse Problems},
  author = {Ehrhardt, Matthias J and Thielemans, Kris and Pizarro, Luis and Atkinson, David and Ourselin, S{\'e}bastien and Hutton, Brian F and Arridge, Simon R},
  month = jan,
  year = {2015},
  pages = {015001},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/34WMGHB9/Ehrhardt et al. - 2015 - Joint reconstruction of PET-MRI by exploiting stru.pdf}
}

@book{moeslund_introduction_2012,
  address = {London},
  series = {Undergraduate Topics in Computer Science},
  title = {Introduction to {{Video}} and {{Image Processing}}},
  isbn = {978-1-4471-2502-0 978-1-4471-2503-7},
  publisher = {{Springer London}},
  author = {Moeslund, Thomas B.},
  year = {2012},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/QA9RW9WE/Moeslund - 2012 - Introduction to Video and Image Processing.pdf}
}

@article{numburi_optimization_2012,
  title = {Optimization of Acquisition and Contrast Injection Protocol for {{C}}-Arm {{CT}} Imaging in Transcatheter Aortic Valve Implantation: Initial Experience in a Swine Model},
  volume = {29},
  issn = {1569-5794, 1573-0743},
  shorttitle = {Optimization of Acquisition and Contrast Injection Protocol for {{C}}-Arm {{CT}} Imaging in Transcatheter Aortic Valve Implantation},
  doi = {10.1007/s10554-012-0075-8},
  number = {2},
  journal = {The International Journal of Cardiovascular Imaging},
  author = {Numburi, Uma D. and Kapadia, Samir R. and Schoenhagen, Paul and Tuzcu, E. Murat and Roden, Martin and Halliburton, Sandra S.},
  month = jun,
  year = {2012},
  pages = {405--415},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/BH473H6R/Numburi et al. - 2012 - Optimization of acquisition and contrast injection.pdf}
}

@article{bernchou_prediction_2015,
  title = {Prediction of Lung Density Changes after Radiotherapy by Cone Beam Computed Tomography Response Markers and Pre-Treatment Factors for Non-Small Cell Lung Cancer Patients},
  issn = {1879-0887},
  doi = {10.1016/j.radonc.2015.07.021},
  abstract = {BACKGROUND AND PURPOSE: This study investigates the ability of pre-treatment factors and response markers extracted from standard cone-beam computed tomography (CBCT) images to predict the lung density changes induced by radiotherapy for non-small cell lung cancer (NSCLC) patients.
METHODS AND MATERIALS: Density changes in follow-up computed tomography scans were evaluated for 135 NSCLC patients treated with radiotherapy. Early response markers were obtained by analysing changes in lung density in CBCT images acquired during the treatment course. The ability of pre-treatment factors and CBCT markers to predict lung density changes induced by radiotherapy was investigated.
RESULTS: Age and CBCT markers extracted at 10th, 20th, and 30th treatment fraction significantly predicted lung density changes in a multivariable analysis, and a set of response models based on these parameters were established. The correlation coefficient for the models was 0.35, 0.35, and 0.39, when based on the markers obtained at the 10th, 20th, and 30th fraction, respectively.
CONCLUSIONS: The study indicates that younger patients without lung tissue reactions early into their treatment course may have minimal radiation induced lung density increase at follow-up. Further investigations are needed to examine the ability of the models to identify patients with low risk of symptomatic toxicity.},
  language = {ENG},
  journal = {Radiotherapy and Oncology: Journal of the European Society for Therapeutic Radiology and Oncology},
  author = {Bernchou, Uffe and Hansen, Olfred and Schytte, Tine and Bertelsen, Anders and Hope, Andrew and Moseley, Douglas and Brink, Carsten},
  month = aug,
  year = {2015},
  pmid = {26255762}
}

@article{jandt_automatic_2009-1,
  title = {Automatic Generation of Time Resolved Motion Vector Fields of Coronary Arteries and {{4D}} Surface Extraction Using Rotational X-Ray Angiography},
  volume = {54},
  journal = {Physics in Medicine and Biology},
  author = {Jandt, U. and Sch{\"a}fer, D. and Grass, M. and Rasche, V.},
  year = {2009},
  pages = {45},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/I4MQPRD6/Jandt et al. - 2009 - Automatic generation of time resolved motion vecto.pdf}
}

@phdthesis{delmon_recalage_2013,
  title = {Recalage D{\'e}formable de Projections de Scanner {{X}} {\`a} Faisceau Conique},
  author = {Delmon, Vivien},
  year = {2013},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/3US6HVIJ/Delmon - 2013 - Recalage dÃ©formable de projections de scanner X Ã  .pdf}
}

@article{jia_gpu-based_2011,
  title = {{{GPU}}-Based Iterative Cone-Beam {{CT}} Reconstruction Using Tight Frame Regularization},
  volume = {56},
  number = {13},
  journal = {Physics in medicine and biology},
  author = {Jia, Xun and Dong, Bin and Lou, Yifei and Jiang, Steve B.},
  year = {2011},
  pages = {3787},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/562A96BE/1008.2042.pdf}
}

@article{sisniega_high-fidelity_2015,
  title = {High-Fidelity Artifact Correction for Cone-Beam {{CT}} Imaging of the Brain},
  volume = {60},
  issn = {0031-9155, 1361-6560},
  doi = {10.1088/0031-9155/60/4/1415},
  number = {4},
  journal = {Physics in Medicine and Biology},
  author = {Sisniega, A and Zbijewski, W and Xu, J and Dang, H and Stayman, J W and Yorkston, J and Aygun, N and Koliatsos, V and Siewerdsen, J H},
  month = feb,
  year = {2015},
  pages = {1415--1439},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/JACJDZJA/sisniega2015.pdf}
}

@inproceedings{xu_image_2011,
  title = {Image Smoothing via {{L}} 0 Gradient Minimization},
  volume = {30},
  booktitle = {{{ACM Transactions}} on {{Graphics}} ({{TOG}})},
  publisher = {{ACM}},
  author = {Xu, Li and Lu, Cewu and Xu, Yi and Jia, Jiaya},
  year = {2011},
  pages = {174},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/BN2BK2EA/Xu2011.pdf}
}

@article{blondel_3d_2004,
  title = {{{3D}} Tomographic Reconstruction of Coronary Arteries Using a Precomputed {{4D}} Motion Field},
  volume = {49},
  issn = {0031-9155},
  abstract = {In this paper, we present a new method to perform 3D tomographic reconstruction of coronary arteries from cone-beam rotational x-ray angiography acquisitions. We take advantage of the precomputation of the coronary artery motion, modelled as a parametric 4D motion field. Contrary to data gating or data triggering approaches, we homogeneously use all available frames, independently of the cardiac phase. In addition, we artificially subtract angiograms from their background structures. Our method significantly improves the reconstruction, by removing both motion and background artefacts. We have successfully tested it on the datasets from a synthetic phantom and 10 patients.},
  number = {11},
  journal = {Physics in Medicine and Biology},
  author = {Blondel, Christophe and Vaillant, R{\'e}gis and Malandain, Gr{\'e}goire and Ayache, Nicholas},
  month = jun,
  year = {2004},
  keywords = {Algorithms,Computer Simulation,Coronary Angiography,Humans,Image Interpretation; Computer-Assisted,Imaging; Three-Dimensional,Information Storage and Retrieval,Models; Biological,Movement,Numerical Analysis; Computer-Assisted,Phantoms; Imaging,Radiographic Image Enhancement,Reproducibility of Results,Sensitivity and Specificity,Signal Processing; Computer-Assisted,Subtraction Technique},
  pages = {2197--2208},
  pmid = {15248572}
}

@inproceedings{muller_4-d_2012,
  title = {4-{{D Motion Field Estimation}} by {{Combined Multiple Heart Phase Registration}} ({{CMHPR}}) for {{Cardiac C}}-Arm {{Data}}},
  author = {M{\"u}ller, K. and Rohkohl, C. and Lauritsch, G. and Schwemmer, C. and Heidb{\"u}chel, H. and De Buck, S. and Nuyens, D. and Kiriakou, Y. and K{\"o}hler, C. and Hornegger, J.},
  year = {2012},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/ACBX3EXX/MÃ¼ller et al. - 4-D Motion Field Estimation by Combined Multiple H.pdf}
}

@article{zhang_analysis_2008,
  title = {Analysis of Heart Rate and Heart Rate Variation during Cardiac {{CT}} Examinations},
  volume = {15},
  issn = {1076-6332},
  doi = {10.1016/j.acra.2007.07.023},
  abstract = {RATIONALE AND OBJECTIVES: We sought to examine heart rate and heart rate variability during cardiac computed tomography (CT).
MATERIALS AND METHODS: Ninety patients (59.0 +/- 13.5 years) underwent coronary CT angiography (CTA), with 52 patients also undergoing coronary artery calcium scanning (CAC). Forty-two patients with heart rate greater than 70 bpm were pretreated with oral beta-blockers (in five patients, use of beta-blocker was not known). Sixty-four patients were given sublingual nitroglycerin. Mean heart rate and percentage of beats outside a +/-5 bpm region about the mean were compared between baseline (free breathing), prescan hyperventilation, and scan acquisition (breath-hold).
RESULTS: Mean scan acquisition time was 13.1 +/- 1.5 seconds for CAC scanning and 14.2 +/- 2.9 seconds for coronary CTA. Mean heart rate during scan acquisition was significantly lower than at baseline (CAC 58.2 +/- 8.5 bpm; CTA 59.2 +/- 8.8 bpm; baseline 62.8 +/- 8.9 bpm; P $<$ .001). The percentage of beats outside a +/-5 bpm about the mean were not different between baseline and CTA scanning (3.5\% versus 3.3\%, P = .87). The injection of contrast had no significant effect on heart rate (58.2 bpm versus 59.2 bpm, P = .24) or percentage of beats outside a +/-5 bpm about the mean (3.0\% versus 3.3\%, P = .64). No significant difference was found between gender and age groups (P $>$ .05).
CONCLUSIONS: Breath-holding during cardiac CT scan acquisition significantly lowers the mean heart rate by approximately 4 bpm, but heart rate variability is the same or less compared with normal breathing.},
  language = {eng},
  number = {1},
  journal = {Academic radiology},
  author = {Zhang, Jie and Fletcher, Joel G and Scott Harmsen, W and Araoz, Philip A and Williamson, Eric E and Primak, Andrew N and McCollough, Cynthia H},
  month = jan,
  year = {2008},
  keywords = {Adult,Aged,Aged; 80 and over,Coronary Angiography,Electrocardiography,Female,Heart Diseases,Heart Rate,Humans,Male,Middle Aged,Tomography; X-Ray Computed},
  pages = {40--48},
  pmid = {18078905}
}

@article{kuntz_fully_2010,
  title = {Fully Automated Intrinsic Respiratory and Cardiac Gating for Small Animal {{CT}}},
  volume = {55},
  issn = {0031-9155, 1361-6560},
  doi = {10.1088/0031-9155/55/7/018},
  number = {7},
  journal = {Physics in Medicine and Biology},
  author = {Kuntz, J and Dinkel, J and Zwick, S and B{\"a}uerle, T and Grasruck, M and Kiessling, F and Gupta, R and Semmler, W and Bartling, S H},
  month = apr,
  year = {2010},
  pages = {2069--2085},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/B2N38DR6/Kuntz et al. - 2010 - Fully automated intrinsic respiratory and cardiac .pdf}
}

@article{naparstek_short-scan_1980,
  title = {Short-{{Scan Fan}}-{{Beam Algorithms}} for {{Cr}}},
  volume = {27},
  issn = {0018-9499},
  doi = {10.1109/TNS.1980.4330983},
  abstract = {Several short-scan reconstruction algorithms of the convolution type for fan-beam projections are presented and discussed. Their derivation fran new, exact integral representation formulas is outlined, and the performance of same of these algorithms is demonstrated with the aid of simulation results.},
  number = {3},
  journal = {IEEE Transactions on Nuclear Science},
  author = {Naparstek, Abraham},
  month = jun,
  year = {1980},
  keywords = {Chromium,Convolution,Equations,Interpolation,Silicon compounds,Terminology,attenuation},
  pages = {1112 --1120},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/FG78UNTE/login.html}
}

@article{verhoeven_limited-data_1993,
  title = {{{LIMITED}}-{{DATA COMPUTED}}-{{TOMOGRAPHY ALGORITHMS FOR THE PHYSICAL SCIENCES}}},
  volume = {32},
  issn = {0003-6935},
  abstract = {Five limited-data computed tomography algorithms are compared. The   algorithms used are adapted versions of the algebraic reconstruction   technique, the multiplicative algebraic reconstruction technique, the   Gerchberg-Papoulis algorithm, a spectral extrapolation algorithm   descended from that of Harris [J. Opt. Soc. Am. 54, 931-936 (1964)], and   an algorithm based on the singular value decomposition technique. These   algorithms were used to reconstruct phantom data with realistic levels   of noise from a number of different imaging geometries. The phantoms,   the imaging geometries, and the noise were chosen to simulate the   conditions encountered in typical computed tomography applications in   the physical sciences, and the implementations of the algorithms were   optimized for these applications. The multiplicative algebraic   reconstruction technique algorithm gave the best results overall; the   algebraic reconstruction technique gave the best results for very smooth   objects or very noisy (20-dB signal-to-noise ratio) data. My   implementations of both of these algorithms incorporate a priori   knowledge of the sign of the object, its extent, and its smoothness. The   smoothness of the reconstruction is enforced through the use of an   appropriate object model (by use of cubic B-spline basis functions and a   number of object coefficients appropriate to the object being   reconstructed). The average reconstruction error was 1.7\% of the maximum   phantom value with the multiplicative algebraic reconstruction technique   of a phantom with moderate-to-steep gradients by use of data from five   viewing angles with a 30-dB signal-to-noise ratio.},
  language = {English},
  number = {20},
  journal = {Applied Optics},
  author = {VERHOEVEN, D},
  month = jul,
  year = {1993},
  keywords = {constraint,emission tomography,image-reconstruction,iterative convolution,likelihood,maximum-entropy,optical microscope tomography,projection data,restoration,transform},
  pages = {3736--3754},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/929VEXTE/full_record.html},
  note = {WOS:A1993LM07800014}
}

@inproceedings{mory_real-time_2012,
  title = {Real-{{Time}} 3d Image Segmentation by User-Constrained Template Deformation},
  booktitle = {Medical {{Image Computing}} and {{Computer}}-{{Assisted Intervention}}\textendash{}{{MICCAI}} 2012},
  publisher = {{Springer}},
  author = {Mory, Beno{\^\i}t and Somphone, Oudom and Prevost, Raphael and Ardon, Roberto},
  year = {2012},
  pages = {561--568},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/9GJ86X9T/MICCAI2012_LiveMorph.pdf}
}

@article{rasche_automatic_2006,
  title = {Automatic Selection of the Optimal Cardiac Phase for Gated Three-Dimensional Coronary {{X}}-Ray Angiography},
  volume = {13},
  number = {5},
  journal = {Academic radiology},
  author = {Rasche, V. and Movassaghi, B. and Grass, M. and Sch{\"a}fer, D. and Buecker, A.},
  year = {2006},
  pages = {630--640},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/VIIRI2UE/Rasche et al. - 2006 - Automatic selection of the optimal cardiac phase f.pdf}
}

@article{nagy_restoring_1998,
  title = {Restoring {{Images Degraded}} by {{Spatially Variant Blur}}},
  volume = {19},
  issn = {1064-8275},
  doi = {http://dx.doi.org/10.1137/S106482759528507X},
  abstract = {Restoration of images that have been blurred by the effects of a Gaussian blurring function is an ill-posed but well-studied problem. Any blur that is spatially invariant can be expressed as a convolution kernel in an integral equation. Fast and effective algorithms then exist for determining the original image by preconditioned iterative methods. If the blurring function is spatially variant, however, then the problem is more difficult. In this work we develop fast algorithms for forming the convolution and for recovering the original image when the convolution functions are spatially variant but have a small domain of support. This assumption leads to a discrete problem involving a banded matrix. We devise an effective preconditioner and prove that the preconditioned matrix differs from the identity by a matrix of small rank plus a matrix of small norm. Numerical examples are given, related to the Hubble Space Telescope (HST) Wide-Field/Planetary Camera. The algorithms that we develop are applicable to other ill-posed integral equations as well.},
  number = {4},
  journal = {SIAM J. Sci. Comput.},
  author = {Nagy, James G and O'Leary, Dianne P},
  month = jul,
  year = {1998},
  keywords = {Algorithms,algorithm design and analysis,computations on matrices},
  pages = {1063--1082},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/QMCT9XM5/Nagy et O'Leary - 1998 - Restoring Images Degraded by Spatially Variant Blu.pdf},
  note = {ACM ID: 294170}
}

@article{schafer_motion-compensated_2006,
  title = {Motion-Compensated and Gated Cone Beam Filtered Back-Projection for 3-{{D}} Rotational {{X}}-Ray Angiography},
  volume = {25},
  issn = {0278-0062},
  doi = {10.1109/TMI.2006.876147},
  number = {7},
  journal = {IEEE Transactions on Medical Imaging},
  author = {Sch{\"a}fer, D. and Borgert, J. and Rasche, V. and Grass, M.},
  month = jul,
  year = {2006},
  pages = {898--906},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/ZU6R58P3/SchÃ¤fer et al. - 2006 - Motion-compensated and gated cone beam filtered ba.pdf}
}

@article{chen_temporal_2009,
  title = {Temporal Resolution Improvement Using {{PICCS}} in {{MDCT}} Cardiac Imaging},
  volume = {36},
  issn = {0094-2405},
  abstract = {The current paradigm for temporal resolution improvement is to add more source-detector units and/or increase the gantry rotation speed. The purpose of this article is to present an innovative alternative method to potentially improve temporal resolution by approximately a factor of 2 for all MDCT scanners without requiring hardware modification. The central enabling technology is a most recently developed image reconstruction method: Prior image constrained compressed sensing (PICCS). Using the method, cardiac CT images can be accurately reconstructed using the projection data acquired in an angular range of about 120 degrees, which is roughly 50\% of the standard short-scan angular range (approximately 240 degrees for an MDCT scanner). As a result, the temporal resolution of MDCT cardiac imaging can be universally improved by approximately a factor of 2. In order to validate the proposed method, two in vivo animal experiments were conducted using a state-of-the-art 64-slice CT scanner (GE Healthcare, Waukesha, WI) at different gantry rotation times and different heart rates. One animal was scanned at heart rate of 83 beats per minute (bpm) using 400 ms gantry rotation time and the second animal was scanned at 94 bpm using 350 ms gantry rotation time, respectively. Cardiac coronary CT imaging can be successfully performed at high heart rates using a single-source MDCT scanner and projection data from a single heart beat with gantry rotation times of 400 and 350 ms. Using the proposed PICCS method, the temporal resolution of cardiac CT imaging can be effectively improved by approximately a factor of 2 without modifying any scanner hardware. This potentially provides a new method for single-source MDCT scanners to achieve reliable coronary CT imaging for patients at higher heart rates than the current heart rate limit of 70 bpm without using the well-known multisegment FBP reconstruction algorithm. This method also enables dual-source MDCT scanner to achieve higher temporal resolution without further hardware modifications.},
  number = {6},
  journal = {Medical physics},
  author = {Chen, Guang-Hong and Tang, Jie and Hsieh, Jiang},
  month = jun,
  year = {2009},
  keywords = {Algorithms,Heart,Radiographic Image Enhancement,Radiographic Image Interpretation; Computer-Assisted,Reproducibility of Results,Sensitivity and Specificity,Tomography; X-Ray Computed},
  pages = {2130--2135},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/9VHKBTBV/Chen et al. - 2009 - Temporal resolution improvement using PICCS in MDC.pdf},
  pmid = {19610302}
}

@article{jandt_automatic_2009,
  title = {Automatic Generation of {{3D}} Coronary Artery Centerlines Using Rotational X-Ray Angiography},
  volume = {13},
  number = {6},
  journal = {Medical Image Analysis},
  author = {Jandt, U. and Sch{\"a}fer, D. and Grass, M. and Rasche, V.},
  year = {2009},
  pages = {846--858},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/RICCASGB/Jandt et al. - 2009 - Automatic generation of 3D coronary artery centerl.pdf}
}

@article{nielsen_cardiac_2005,
  title = {Cardiac Cone-Beam {{CT}} Volume Reconstruction Using {{ART}}},
  volume = {32},
  issn = {0094-2405},
  abstract = {Modern computed tomography systems allow volume imaging of the heart. Up to now, approximately two-dimensional (2D) and 3D algorithms based on filtered backprojection are used for the reconstruction. These algorithms become more sensitive to artifacts when the cone angle of the x-ray beam increases as it is the current trend of computed tomography (CT) technology. In this paper, we investigate the potential of iterative reconstruction based on the algebraic reconstruction technique (ART) for helical cardiac cone-beam CT. Iterative reconstruction has the advantages that it takes the cone angle into account exactly and that it can be combined with retrospective cardiac gating fairly easily. We introduce a modified ART algorithm for cardiac CT reconstruction. We apply it to clinical cardiac data from a 16-slice CT scanner and compare the images to those obtained with a current analytical reconstruction method. In a second part, we investigate the potential of iterative reconstruction for a large area detector with 256 slices. For the clinical cases, iterative reconstruction produces excellent images of diagnostic quality. For the large area detector, iterative reconstruction produces images superior to analytical reconstruction in terms of cone-beam artifacts.},
  number = {4},
  journal = {Medical Physics},
  author = {Nielsen, T and Manzke, R and Proksa, R and Grass, M},
  month = apr,
  year = {2005},
  keywords = {Algorithms,Cardiac Volume,Humans,Image Processing; Computer-Assisted,Imaging; Three-Dimensional,Models; Statistical,Models; Theoretical,Myocardium,Phantoms; Imaging,Radiographic Image Enhancement,Radiographic Image Interpretation; Computer-Assisted,Software,Time Factors,Tomography; X-Ray Computed},
  pages = {851--860},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/PTIEB9RR/2005_MedPhys_Nielsen.pdf},
  pmid = {15895567}
}

@article{schlomka_experimental_2008,
  title = {Experimental Feasibility of Multi-Energy Photon-Counting {{K}}-Edge Imaging in Pre-Clinical Computed Tomography},
  volume = {53},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/53/15/002},
  abstract = {Theoretical considerations predicted the feasibility of K-edge x-ray computed tomography (CT) imaging using energy discriminating detectors with more than two energy bins. This technique enables material-specific imaging in CT, which in combination with high- Z element based contrast agents, opens up possibilities for new medical applications. In this paper, we present a CT system with energy detection capabilities, which was used to demonstrate the feasibility of quantitative K-edge CT imaging experimentally. A phantom was imaged containing PMMA, calcium-hydroxyapatite, water and two contrast agents based on iodine and gadolinium, respectively. Separate images of the attenuation by photoelectric absorption and Compton scattering were reconstructed from energy-resolved projection data using maximum-likelihood basis-component decomposition. The data analysis further enabled the display of images of the individual contrast agents and their concentrations, separated from the anatomical background. Measured concentrations of iodine and gadolinium were in good agreement with the actual concentrations. Prior to the tomographic measurements, the detector response functions for monochromatic illumination using synchrotron radiation were determined in the energy range 25 keV\textendash{}60 keV. These data were used to calibrate the detector and derive a phenomenological model for the detector response and the energy bin sensitivities.},
  language = {en},
  number = {15},
  journal = {Physics in Medicine and Biology},
  author = {Schlomka, J. P. and Roessl, E. and Dorscheid, R. and Dill, S. and Martens, G. and Istel, T. and B{\"a}umer, C. and Herrmann, C. and Steadman, R. and {G Zeitler} and Livne, A. and Proksa, R.},
  year = {2008},
  pages = {4031},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/KCT7DHWB/Schlomka et al. - 2008 - Experimental feasibility of multi-energy photon-co.pdf}
}

@article{watt_consistent_1989,
  title = {{{CONSISTENT ITERATIVE CONVOLUTION}} - {{A COUPLED APPROACH TO TOMOGRAPHIC}}   {{RECONSTRUCTION}}},
  volume = {6},
  issn = {0740-3232},
  doi = {10.1364/JOSAA.6.000044},
  language = {English},
  number = {1},
  journal = {Journal of the Optical Society of America a-Optics Image Science and Vision},
  author = {WATT, DW and VEST, CM},
  month = jan,
  year = {1989},
  pages = {44--51},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/T25XE6BW/full_record.html},
  note = {WOS:A1989R645900006}
}

@article{armitage_least-error_2012,
  title = {Least-Error Projection Sorting to Optimize Retrospectively Gated Cardiac Micro-{{CT}} of Free-Breathing Mice},
  volume = {39},
  issn = {0094-2405},
  doi = {10.1118/1.3681949},
  abstract = {PURPOSE: To develop and characterize a technique for optimizing image quality by eliminating streaking artifacts in retrospectively gated microcomputed tomography (micro-CT) images of mice caused by insufficient and irregular angular sampling.
METHODS: A least-error sorting technique was developed to minimize streak artifacts in retrospectively gated cardiac micro-CT images. To ensure complete filling of projection space, for each angular position, the projection acquired closest to the desired cardiac phase is used to reconstruct a volumetric image. An acrylic slanted-edge phantom undergoing cyclic motion was used to characterize the system's resolution. The phantom was scanned using a volumetric micro-CT scanner equipped with a flat-panel detector mounted on a slip-ring gantry. Projection images of the moving phantom were collected over a period of 60 s using a variety of acquisition protocols with the rotation period of the gantry ranging from 1 to 5 s. The modulation transfer function (MTF) of the reconstructed images was measured for many combinations of acquisition and reconstruction parameters. The use of the least-error technique was also demonstrated in vivo.
RESULTS: The motion blurring introduced into the images at physiologically significant velocities of 6 cm$\slash$s agreed well with predicted values; limiting resolution (frequency at 10\% MTF) degraded from 2.5 to 1.0 mm(-1) for a velocity of 6 cm$\slash$s and 5 s$\slash$rotation gantry speed. Faster gantry rotation speeds led to improved temporal resolution but the scanner's data storage and transfer rates and field of view limitations made scanning at gantry speeds faster than 2 s$\slash$rotation impractical.
CONCLUSIONS: The least-error technique effectively eliminates streaking artifact caused by missing views and allows for optimization of image quality in retrospectively gated micro-CT.},
  language = {eng},
  number = {3},
  journal = {Medical physics},
  author = {Armitage, Stephen E J and Pollmann, Steven I and Detombe, Sarah A and Drangova, Maria},
  month = mar,
  year = {2012},
  keywords = {Animals,Cardiac-Gated Imaging Techniques,Heart,Image Processing; Computer-Assisted,Mice,Radiation,Respiration,Retrospective Studies,Time Factors,X-Ray Microtomography},
  pages = {1452--1461},
  pmid = {22380378}
}

@article{leng_streaking_2008,
  title = {Streaking Artifacts Reduction in Four-Dimensional Cone-Beam Computed Tomography},
  volume = {35},
  issn = {0094-2405},
  abstract = {Cone-beam computed tomography (CBCT) using an "on-board" x-ray imaging device integrated into a radiation therapy system has recently been made available for patient positioning, target localization, and adaptive treatment planning. One of the challenges for gantry mounted image-guided radiation therapy (IGRT) systems is the slow acquisition of projections for cone-beam CT (CBCT), which makes them sensitive to any patient motion during the scans. Aiming at motion artifact reduction, four-dimensional CBCT (4D CBCT) techniques have been introduced, where a surrogate for the target's motion profile is utilized to sort the cone-beam data by respiratory phase. However, due to the limited gantry rotation speed and limited readout speed of the on-board imager, fewer than 100 projections are available for the image reconstruction at each respiratory phase. Thus, severe undersampling streaking artifacts plague 4D CBCT images. In this paper, the authors propose a simple scheme to significantly reduce the streaking artifacts. In this method, a prior image is first reconstructed using all available projections without gating, in which static structures are well reconstructed while moving objects are blurred. The undersampling streaking artifacts from static structures are estimated from this prior image volume and then can be removed from the phase images using gated reconstruction. The proposed method was validated using numerical simulations, experimental phantom data, and patient data. The fidelity of stationary and moving objects is maintained, while large gains in streak artifact reduction are observed. Using this technique one can reconstruct 4D CBCT datasets using no more projections than are acquired in a 60 s scan. At the same time, a temporal gating window as narrow as 100 ms was utilized. Compared to the conventional 4D CBCT reconstruction, streaking artifacts were reduced by 60\% to 70\%.},
  number = {10},
  journal = {Medical physics},
  author = {Leng, Shuai and Zambelli, Joseph and Tolakanahalli, Ranjini and Nett, Brian and Munro, Peter and Star-Lack, Joshua and Paliwal, Bhudatt and Chen, Guang-Hong},
  month = oct,
  year = {2008},
  keywords = {Algorithms,Artifacts,Cone-Beam Computed Tomography,Humans,Imaging; Three-Dimensional,Motion,Phantoms; Imaging,Radiographic Image Enhancement,Radiographic Image Interpretation; Computer-Assisted,Reproducibility of Results,Respiratory Mechanics,Sensitivity and Specificity},
  pages = {4649--4659},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/KUZI83ZZ/Leng et al. - 2008 - Streaking artifacts reduction in four-dimensional .pdf},
  pmid = {18975711}
}

@article{dang_pilot_2015,
  title = {A {{Pilot Evaluation}} of a 4-{{Dimensional Cone}}-{{Beam Computed Tomographic Scheme Based}} on {{Simultaneous Motion Estimation}} and {{Image Reconstruction}}},
  volume = {91},
  issn = {0360-3016},
  doi = {10.1016/j.ijrobp.2014.10.029},
  abstract = {Purpose
To evaluate the performance of a 4-dimensional (4-D) cone-beam computed tomographic (CBCT) reconstruction scheme based on simultaneous motion estimation and image reconstruction (SMEIR) through patient studies.
Methods and Materials
The SMEIR algorithm contains 2 alternating steps: (1) motion-compensated CBCT reconstruction using projections from all phases to reconstruct a reference phase 4D-CBCT by explicitly considering the motion models between each different phase and (2) estimation of motion models directly from projections by matching the measured projections to the forward projection of the deformed reference phase 4D-CBCT. Four lung cancer patients were scanned for 4 to 6~minutes to obtain approximately 2000 projections for each patient. To evaluate the performance of the SMEIR algorithm on a conventional 1-minute CBCT scan, the number of projections at each phase was reduced by a factor of 5, 8, or 10 for each patient. Then, 4D-CBCTs were reconstructed from the down-sampled projections using Feldkamp-Davis-Kress, total variation (TV) minimization, prior image constrained compressive sensing (PICCS), and SMEIR. Using the 4D-CBCT reconstructed from the fully sampled projections as a reference, the relative error (RE) of reconstructed images, root mean square error (RMSE), and maximum error (MaxE) of estimated tumor positions were analyzed to quantify the performance of the SMEIR algorithm.
Results
The SMEIR algorithm can achieve results consistent with the reference 4D-CBCT reconstructed with many more projections per phase. With an average of 30 to 40 projections per phase, the MaxE in tumor position detection is less than 1~mm in SMEIR for all 4 patients.
Conclusion
The results from a limited number of patients show that SMEIR is a promising tool for high-quality 4D-CBCT reconstruction and tumor motion modeling.},
  number = {2},
  journal = {International Journal of Radiation Oncology*Biology*Physics},
  author = {Dang, Jun and Gu, Xuejun and Pan, Tinsu and Wang, Jing},
  month = feb,
  year = {2015},
  pages = {410--418},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/3WACQ8GN/Dang et al. - 2015 - A Pilot Evaluation of a 4-Dimensional Cone-Beam Co.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/MPHTGGSP/S0360301614042990.html}
}

@article{lauzier_prior_2012,
  title = {Prior Image Constrained Compressed Sensing: {{Implementation}} and Performance Evaluation},
  volume = {39},
  issn = {00942405},
  shorttitle = {Prior Image Constrained Compressed Sensing},
  doi = {10.1118/1.3666946},
  number = {1},
  journal = {Medical Physics},
  author = {Lauzier, Pascal Th{\'e}riault and Tang, Jie and Chen, Guang-Hong},
  year = {2012},
  pages = {66},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/E3TXKMGJ/Lauzier et al. - 2012 - Prior image constrained compressed sensing Implem.pdf}
}

@article{jaffray_flat-panel_2002,
  title = {Flat-Panel Cone-Beam Computed Tomography for Image-Guided Radiation Therapy},
  volume = {53},
  number = {5},
  journal = {International Journal of Radiation Oncology* Biology* Physics},
  author = {Jaffray, David A. and Siewerdsen, Jeffrey H. and Wong, John W. and Martinez, Alvaro A.},
  year = {2002},
  pages = {1337--1349},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/XMKKAH6M/jaffray2002.pdf}
}

@article{cai_cine_2012,
  title = {Cine Cone Beam {{CT}} Reconstruction Using Low-Rank Matrix Factorization: Algorithm and a Proof-of-Principle Study},
  shorttitle = {Cine Cone Beam {{CT}} Reconstruction Using Low-Rank Matrix Factorization},
  author = {Cai, J. and Jia, Xun and Gao, Hao and Jiang, S. and Shen, Zuowei and Zhao, Hongkai},
  year = {2012},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/XAI45TID/Cai et al. - 2012 - Cine cone beam CT reconstruction using low-rank ma.pdf}
}

@article{kudo_sinogram_1991,
  title = {{{SINOGRAM RECOVERY WITH THE METHOD OF CONVEX PROJECTIONS FOR LIMITED}}-{{DATA}}   {{RECONSTRUCTION IN COMPUTED}}-{{TOMOGRAPHY}}},
  volume = {8},
  issn = {0740-3232},
  doi = {10.1364/JOSAA.8.001148},
  abstract = {Image reconstruction from incomplete projection data is strongly   required in widespread applications of computed tomography.  This   problem can be formulated as a sinogram-recovery problem.  The   sinogram-recovery problem is to find a complete sinogram that is   compatible with the Helgason-Ludwig consistency condition, the measured   incomplete sinogram, and other a priori knowledge about the sinogram in   question.  The direct use of the Helgason-Ludwig consistency condition   considerably reduces computational requirements and the accumulation of   digital-prcessing errors over the conventional iterative   reconstruction-reprojection method.  Most research for solving the   sinogram-recovery problem is based on directly inverting systems of   linear equations associated with the Helgason-Ludwig consistency   condition.  However, these noniterative techniques cannot be applied to   various different types of limited-data situations in a unified way.    Moreover, nonlinear a priori constraints such as the nonnegativity and   the amplitude limit are not easily incorporated.  We solve the   sinogram-recovery problem by using an iterative signal-recovery   technique known as the method of projection onto convex sets.  Once an   estimation of the complete sinogram is obtained, the conventional   convolution-backprojection method can be utilized to reconstruct an   image.  The performance of the proposed method is investigated both with   numerical phantoms and with actual x-ray data.},
  language = {English},
  number = {7},
  journal = {Journal of the Optical Society of America a-Optics Image Science and Vision},
  author = {KUDO, H and SAITO, T},
  month = jul,
  year = {1991},
  keywords = {Algorithms,angle tomography,image-reconstruction,incomplete data problems,reprojection,restoration,transform},
  pages = {1148--1160},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/SU3ZRHHS/full_record.html},
  note = {WOS:A1991FU03100021}
}

@inproceedings{rohkohl_ecg-gated_2010,
  title = {{{ECG}}-Gated Interventional Cardiac Reconstruction for Non-Periodic Motion},
  volume = {13},
  abstract = {The 3-D reconstruction of cardiac vasculature using C-arm CT is an active and challenging field of research. In interventional environments patients often do have arrhythmic heart signals or cannot hold breath during the complete data acquisition. This important group of patients cannot be reconstructed with current approaches that do strongly depend on a high degree of cardiac motion periodicity for working properly. In a last year's MICCAI contribution a first algorithm was presented that is able to estimate non-periodic 4-D motion patterns. However, to some degree that algorithm still depends on periodicity, as it requires a prior image which is obtained using a simple ECG-gated reconstruction. In this work we aim to provide a solution to this problem by developing a motion compensated ECG-gating algorithm. It is built upon a 4-D time-continuous affine motion model which is capable of compactly describing highly non-periodic motion patterns. A stochastic optimization scheme is derived which minimizes the error between the measured projection data and the forward projection of the motion compensated reconstruction. For evaluation, the algorithm is applied to 5 datasets of the left coronary arteries of patients that have ignored the breath hold command and/or had arrhythmic heart signals during the data acquisition. By applying the developed algorithm the average visibility of the vessel segments could be increased by 27\%. The results show that the proposed algorithm provides excellent reconstruction quality in cases where classical approaches fail. The algorithm is highly parallelizable and a clinically feasible runtime of under 4 minutes is achieved using modern graphics card hardware.},
  booktitle = {Medical {{Image Computing}} and {{Computer}}-{{Assisted Intervention}}: {{MICCAI}} ... {{International Conference}} on {{Medical Image Computing}} and {{Computer}}-{{Assisted Intervention}}},
  author = {Rohkohl, Christopher and Lauritsch, G{\"u}nter and Biller, Lisa and Hornegger, Joachim},
  year = {2010},
  keywords = {Algorithms,Artifacts,Cardiac-Gated Imaging Techniques,Coronary Angiography,Motion,Periodicity,Radiographic Image Enhancement,Radiographic Image Interpretation; Computer-Assisted,Radiography; Interventional,Reproducibility of Results,Sensitivity and Specificity,Tomography; X-Ray Computed},
  pages = {151--158},
  pmid = {20879226}
}

@article{li_radiation_2005,
  title = {Radiation Dose Reduction in Four-Dimensional Computed Tomography},
  volume = {32},
  issn = {0094-2405},
  abstract = {Four-dimensional (4D) CT is useful in many clinical situations, where detailed abdominal and thoracic imaging is needed over the course of the respiratory cycle. However, it usually delivers a larger radiation dose than the standard three-dimensional (3D) CT, since multiple scans at each couch position are required in order to provide the temporal information. Our purpose in this work is to develop a method to perform 4D CT scans at relatively low current, hence reducing the radiation exposure of the patients. To deal with the increased statistical noise caused by the low current, we proposed a novel 4D penalized weighted least square (4D-PWLS) smoothing method, which can incorporate both spatial and phase information. The 4D images at different phases were registered to the same phase via a deformable model, thereby, a regularization term combining temporal and spatial neighbors can be designed for the 4D-PWLS objective function. The proposed method was tested with phantom experiments and a patient study, and superior noise suppression and resolution preservation were observed. A quantitative evaluation of the benefit of the proposed method to 4D radiotherapy and 4D PET/CT imaging are under investigation.},
  number = {12},
  journal = {Medical physics},
  author = {Li, T and Schreibmann, E and Thorndyke, B and Tillman, G and Boyer, A and Koong, A and Goodman, K and Xing, L},
  month = dec,
  year = {2005},
  keywords = {Biophysical Phenomena,Biophysics,Humans,Movement,Neoplasms,Phantoms; Imaging,Radiation Dosage,Radiographic Image Interpretation; Computer-Assisted,Radiotherapy Planning; Computer-Assisted,Respiration,Thoracic Neoplasms,Tomography; X-Ray Computed},
  pages = {3650--3660},
  pmid = {16475764}
}

@article{clackdoyle_tomographic_2012,
  title = {Tomographic {{Reconstruction}} in the 21st {{Century}} [{{Region}}-of-Interest Reconstruction from Incomplete Data]},
  volume = {33},
  abstract = {Universit{\'e} Jean Monnet - Saint-Etienne},
  number = {1},
  journal = {Progress in Biomedical Engineering},
  author = {Clackdoyle, Rolf and Defrise, Michel},
  year = {2012},
  pages = {23--42},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/7RZTTFNG/Defrise Clackdoyle.pdf}
}

@article{zhong_4d_2016,
  title = {{{4D}} Cone-Beam {{CT}} Reconstruction Using Multi-Organ Meshes for Sliding Motion Modeling},
  volume = {61},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/61/3/996},
  abstract = {A simultaneous motion estimation and image reconstruction (SMEIR) strategy was proposed for 4D cone-beam CT (4D-CBCT) reconstruction and showed excellent results in both phantom and lung cancer patient studies. In the original SMEIR algorithm, the deformation vector field (DVF) was defined on voxel grid and estimated by enforcing a global smoothness regularization term on the motion fields. The objective of this work is to improve the computation efficiency and motion estimation accuracy of SMEIR for 4D-CBCT through developing a multi-organ meshing model. Feature-based adaptive meshes were generated to reduce the number of unknowns in the DVF estimation and accurately capture the organ shapes and motion. Additionally, the discontinuity in the motion fields between different organs during respiration was explicitly considered in the multi-organ mesh model. This will help with the accurate visualization and motion estimation of the tumor on the organ boundaries in 4D-CBCT. To further improve the computational efficiency, a GPU-based parallel implementation was designed. The performance of the proposed algorithm was evaluated on a synthetic sliding motion phantom, a 4D NCAT phantom, and four lung cancer patients. The proposed multi-organ mesh based strategy outperformed the conventional Feldkamp\textendash{}Davis\textendash{}Kress, iterative total variation minimization, original SMEIR and single meshing method based on both qualitative and quantitative evaluations.},
  language = {en},
  number = {3},
  journal = {Physics in Medicine and Biology},
  author = {Zhong, Zichun and Gu, Xuejun and Mao, Weihua and Wang, Jing},
  year = {2016},
  pages = {996}
}

@phdthesis{turbell_h._cone-beam_2001,
  title = {Cone-{{Beam Reconstruction Using Filtered Backprojection}}},
  school = {Link{\"o}ping University},
  author = {Turbell, H.},
  year = {2001},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/HUEV4G6A/Turbell_Thesis_FBP_2001.pdf}
}

@book{smith_scientist_1997,
  address = {San Diego, CA, USA},
  title = {The Scientist and Engineer's Guide to Digital Signal Processing},
  isbn = {0-9660176-3-3},
  publisher = {{California Technical Publishing}},
  author = {Smith, Steven W.},
  year = {1997}
}

@article{simon_non-invasive_2000,
  title = {Non-Invasive Imaging of Regional Lung Function Using x-Ray Computed Tomography},
  volume = {16},
  number = {5-6},
  journal = {Journal of clinical monitoring and computing},
  author = {Simon, Brett A.},
  year = {2000},
  pages = {433--442},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/9VCIADZG/simon2000.pdf}
}

@article{schirra_statistical_2013,
  title = {Statistical {{Reconstruction}} of {{Material Decomposed Data}} in {{Spectral CT}}},
  volume = {32},
  issn = {0278-0062, 1558-254X},
  doi = {10.1109/TMI.2013.2250991},
  number = {7},
  journal = {IEEE Transactions on Medical Imaging},
  author = {Schirra, C. O. and Roessl, E. and Koehler, T. and Brendel, B. and Thran, A. and Pan, D. and Anastasio, M. A. and Proksa, R.},
  month = jul,
  year = {2013},
  pages = {1249--1257},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/JTGEMS53/schirra2013.pdf}
}

@article{gullberg_reconstruction_1979,
  title = {The Reconstruction of Fan-Beam Data by Filtering the Back-Projection},
  volume = {10},
  issn = {0146-664X},
  doi = {10.1016/0146-664X(79)90033-9},
  abstract = {Expressions for the fan-beam projection and back-projection operators are given along with an evaluation of the impulse response for the composite of the projection and back-projection operations. The fan-beam geometries for both curved and flat detectors have back-projection operators such that the impulse response for the composite of the projection and back-projection operations is equal to 1/|r - r0| if the data are taken over 360$^\circ$, and thus two-dimensional Fourier filter techniques can be used to reconstruct transverse sections from fan-beam projection data. It is shown that the impulse response for the composite of the projection and back-projection operators for fan-beam geometry is not spatially invariant if the data are taken over 180$^\circ$ as is true for parallel-beam projection data. The convolution algorithm is approximately four times faster and requires 40\% less memory than the filter of the back-projection algorithm. However, the filter of the back-projection algorithm has the advantage of easily implementing different frequency space filters which lends itself to easily changing the noise propagation vs resolution properties of the convolution kernel.},
  number = {1},
  journal = {Computer Graphics and Image Processing},
  author = {Gullberg, Grant T},
  month = may,
  year = {1979},
  pages = {30--47},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/UQ4Z6GTT/0146664X79900339.html}
}

@article{zhu_automatic_2012,
  title = {Automatic {{Patient Table Removal}} in {{CT Images}}},
  volume = {25},
  issn = {0897-1889, 1618-727X},
  doi = {10.1007/s10278-012-9454-x},
  language = {en},
  number = {4},
  journal = {Journal of Digital Imaging},
  author = {Zhu, Yang-Ming and Cochoff, Steven M. and Sukalac, Ronald},
  month = aug,
  year = {2012},
  pages = {480--485},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/UMXTGKHA/Zhu et al. - 2012 - Automatic Patient Table Removal in CT Images.pdf}
}

@article{kachelries_empirical_2006,
  title = {Empirical Cupping Correction: {{A}} First-Order Raw Data Precorrection for Cone-Beam Computed Tomography},
  volume = {33},
  issn = {0094-2405},
  shorttitle = {Empirical Cupping Correction},
  doi = {10.1118/1.2188076},
  abstract = {We propose an empirical cupping correction (ECC) algorithm to correct for CT cupping artifacts that are induced by nonlinearities in the projection data. The method is raw data based, empirical, and requires neither knowledge of the x-ray spectrum nor of the attenuation coefficients. It aims at linearizing the attenuation data using a precorrection function of polynomial form. The coefficients of the polynomial are determined once using a calibration scan of a homogeneous phantom. Computing the coefficients is done in image domain by fitting a series of basis images to a template image. The template image is obtained directly from the uncorrected phantom image and no assumptions on the phantom size or of its positioning are made. Raw data are precorrected by passing them through the once-determined polynomial. As an example we demonstrate how ECC can be used to perform water precorrection for an in vivo micro-CT scanner (TomoScope 30 s, VAMP GmbH, Erlangen, Germany). For this particular case, practical considerations regarding the definition of the template image are given. ECC strives to remove the cupping artifacts and to obtain well-calibrated CT values. Although ECC is a first-order correction and cannot compete with iterative higher-order beam hardening or scatter correction algorithms, our in vivo mouse images show a significant reduction of bone-induced artifacts as well. A combination of ECC with analytical techniques yielding a hybrid cupping correction method is possible and allows for channel-dependent correction functions.},
  number = {5},
  journal = {Medical Physics},
  author = {Kachelrie{\ss}, Marc and Sourbelle, Katia and Kalender, Willi A.},
  month = may,
  year = {2006},
  keywords = {Calibration,Computed tomography,Image reconstruction,Image scanners,Medical imaging},
  pages = {1269--1274},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/5AF5CVDI/KachelrieÃŸ et al. - 2006 - Empirical cupping correction A first-order raw da.pdf}
}

@article{chen_prior_2008,
  title = {Prior Image Constrained Compressed Sensing ({{PICCS}}): {{A}} Method to Accurately Reconstruct Dynamic {{CT}} Images from Highly Undersampled Projection Data Sets},
  volume = {35},
  issn = {00942405},
  shorttitle = {Prior Image Constrained Compressed Sensing ({{PICCS}})},
  doi = {10.1118/1.2836423},
  number = {2},
  journal = {Medical Physics},
  author = {Chen, Guang-Hong and Tang, Jie and Leng, Shuai},
  year = {2008},
  pages = {660},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/5MTNGTWS/Chen et al. - 2008 - Prior image constrained compressed sensing (PICCS).pdf}
}

@inproceedings{gac_multi-gpu_2016,
  title = {Multi-{{GPU}} Parallelization of {{3D X}}-{{Ray Reconstruction}}},
  booktitle = {Meeting on {{Tomography}} and {{Applications Discrete Tomography}} and {{Image Reconstruction}}},
  author = {Gac, Nicolas},
  year = {2016},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/5WDAKEM5/hal-01403793.html}
}

@article{sawall_low-dose_2011,
  title = {Low-Dose Cardio-Respiratory Phase-Correlated Cone-Beam Micro-{{CT}} of Small Animals},
  volume = {38},
  issn = {0094-2405},
  abstract = {PURPOSE: Micro-CT imaging of animal hearts typically requires a double gating procedure because scans during a breath-hold are not possible due to the long scan times and the high respiratory rates, Simultaneous respiratory and cardiac gating can either be done prospectively or retrospectively. True five-dimensional information can be either retrieved with retrospective gating or with prospective gating if several prospective gates are acquired. In any case, the amount of information available to reconstruct one volume for a given respiratory and cardiac phase is orders of magnitud lower than the total amount of information acquired. For example, the reconstruction of a volume from a 10\% wide respiratory and a 20\% wide cardiac window uses only 2\% of the data acquired. Achieving a similar image quality as a nongated scan would therefore require to increase the amount of data and thereby the dose to the animal by up to a factor of 50.
METHODS: To achieve the goal of low-dose phase-correlated (LDPC) imaging, the authors propose to use a highly efficient combination of slightly modified existing algorithms. In particular, the authors developed a variant of the McKinnon-Bates image reconstruction algorithm and combined it with bilateral filtering in up to five dimensions to significantly reduce image noise without impairing spatial or temporal resolution.
RESULTS: The preliminary results indicate that the proposed LDPC reconstruction method typically reduces image noise by a factor of up to 6 (e.g., from 170 to 30 HU), while the dose values lie in a range from 60 to 500 mGy. Compared to other publications that apply 250-1800 mGy for the same task [C. T. Badea et al., "4D micro-CT of the mouse heart," Mol. Imaging 4(2), 110-116 (2005); M. Drangova et al., "Fast retrospectively gated quantitative four-dimensional (4D) cardiac micro computed tomography imaging of free-breathing mice," Invest. Radiol. 42(2), 85-94 (2007); S. H. Bartling et al., "Retrospective motion gating in small animal CT of mice and rats," Invest. Radiol. 42(10), 704-714 (2007)], the authors' LDPC approach therefore achieves a more than tenfold dose usage improvement.
CONCLUSIONS: The LDPC reconstruction method improves phase-correlated imaging from highly undersampled data. Artifacts caused by sparse angular sampling are removed and the image noise is decreased, while spatial and temporal resolution are preserved. Thus, the administered dose per animal can be decreased allowing for long-term studies with reduced metabolic inference.},
  language = {eng},
  number = {3},
  journal = {Medical physics},
  author = {Sawall, Stefan and Bergner, Frank and Lapp, Robert and Mronz, Markus and Karolczak, Marek and Hess, Andreas and Kachelriess, Marc},
  month = mar,
  year = {2011},
  keywords = {Animals,Cone-Beam Computed Tomography,Heart,Image Processing; Computer-Assisted,Lung,Mice,Movement,Radiation Dosage,Respiration,X-Ray Microtomography},
  pages = {1416--1424},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/H82UQA2A/Sawall et al. - 2011 - Low-dose cardio-respiratory phase-correlated cone-.pdf},
  pmid = {21520853}
}

@misc{ben_hadj_restoration_2011,
  title = {Restoration {{Method}} for {{Spatially Variant Blurred Images}}},
  howpublished = {http://hal.inria.fr/inria-00602650\_v1/},
  author = {Ben Hadj, Saima and Blanc F{\'e}raud, Laure},
  month = jun,
  year = {2011},
  keywords = {Deconvolution; energy minimization; spatially-variant PSF; total variation.},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/V64ZCH5X/Ben Hadj et Blanc FÃ©raud - 2011 - Restoration Method for Spatially Variant Blurred I.pdf},
  note = {Most of existing image restoration techniques suppose that the blur is spatially-invariant. However, various physical phenomena related to the optical instrument properties make that degradations may change in the image domain. Taking into account space-variance of optical aberrations in the restoration process is an important task that should enhance the accuracy of the estimated object. This latter issue has received little attention by researchers in these last years. In this work, we derive a restoration method for spatially-variant blurred images. In our approach, we consider a blur modeled by a space-varying linear combination of spatially invariant blurs. We develop the example of a piecewise-constant PSF model with regular transitions between areas in order to alleviate blur alteration effect. Furthermore, we develop for this model, an appropriate deconvolution method based on minimization of a criterion with total variation regularization. For this purpose, we fit a domain decomposition-based minimization approach that was recently developed by Fornassier et al., 2009 to the deconvolution problem with a spatially varying PSF model. We thus obtain a fast restoration algorithm where the true image estimation is performed in a parallel way on different sub-regions of the image. We also study the convergence of the proposed method especially for the considered spatially varying PSF model.}
}

@article{girard_contrast-enhanced_2011,
  title = {Contrast-Enhanced {{C}}-Arm {{CT}} Evaluation of Radiofrequency Ablation Lesions in the Left Ventricle},
  volume = {4},
  issn = {1876-7591},
  doi = {10.1016/j.jcmg.2010.11.019},
  abstract = {OBJECTIVES The purpose of this study was to evaluate use of cardiac C-arm computed tomography (CT) in the assessment of the dimensions and temporal characteristics of radiofrequency ablation (RFA) lesions. This imaging modality uses a standard C-arm fluoroscopy system rotating around the patient, providing CT-like images during the RFA procedure. BACKGROUND Both cardiac magnetic resonance (CMR) and CT can be used to assess myocardial necrotic tissue. Several studies have reported visualizing cardiac RFA lesions with CMR; however, obtaining CMR images during interventional procedures is not common practice. Direct visualization of RFA lesions using C-arm CT during the procedure may improve outcomes and circumvent complications associated with cardiac ablation procedures. METHODS RFA lesions were created on the endocardial surface of the left ventricle of 9 swine using a 7-F RFA catheter. An electrocardiographically gated C-arm CT imaging protocol was used to acquire projection images during iodine contrast injection and after the injection every 5 min for up to 30 min, with no additional contrast. Reconstructed images were analyzed offline. The mean and SD of the signal intensity of the lesion and normal myocardium were measured in all images in each time series. Lesion dimensions and area were measured and compared in pathologic specimens and C-arm CT images. RESULTS All ablation lesions (n = 29) were visualized and lesion dimensions, as measured on C-arm CT, correlated well with postmortem tissue measurements (linear dimensions: concordance correlation = 0.87; area: concordance correlation = 0.90. Lesions were visualized as a perfusion defect on first-pass C-arm CT images with a signal intensity of 95 HU lower than that of normal myocardium (95\% confidence interval: -111 HU to -79 HU). Images acquired at 1 and 5 min exhibited an enhancing ring surrounding the perfusion defect in 24 lesions (83\%). CONCLUSIONS RFA lesion size, including transmurality, can be assessed using electrocardiographically gated cardiac C-arm CT in the interventional suite. Visualization of RFA lesions using cardiac C-arm CT may facilitate the assessment of adequate lesion delivery and provide valuable feedback during cardiac ablation procedures.},
  number = {3},
  journal = {JACC. Cardiovascular Imaging},
  author = {Girard, Erin E and Al-Ahmad, Amin and Rosenberg, Jarrett and Luong, Richard and Moore, Teri and Lauritsch, G{\"u}nter and Boese, Jan and Fahrig, Rebecca},
  month = mar,
  year = {2011},
  pages = {259--268},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/26FM46P7/Girard et al. - 2011 - Contrast-enhanced C-arm CT evaluation of radiofreq.pdf},
  pmid = {21414574}
}

@article{holland_diaphragmatic_1998,
  title = {Diaphragmatic and Cardiac Motion during Suspended Breathing: Preliminary Experience and Implications for Breath-Hold {{MR}} Imaging},
  volume = {209},
  issn = {0033-8419},
  shorttitle = {Diaphragmatic and Cardiac Motion during Suspended Breathing},
  abstract = {PURPOSE: To investigate and quantify motion of the diaphragm and heart during suspended breathing at end inspiration and end expiration.
MATERIALS AND METHODS: In 10 healthy adult volunteers, line scanning was performed to monitor the position of the diaphragm during a breath hold at end inspiration and end expiration, with a spatial and temporal resolution of 0.25 mm and 200 msec, respectively. Electrocardiographically gated, turbo fast low-angle shot (FLASH) magnetic resonance (MR) imaging was performed to monitor movement of the diaphragm and heart.
RESULTS: During a breath hold, the diaphragm moved upward. At end expiration, the velocity of the diaphragm during suspended breathing was constant (mean, 0.15 mm/sec). At end inspiration, motion of the diaphragm during suspended breathing was more complex (range, 0.1-7.9 mm/sec). During a 20-second breath hold, mean displacement of the diaphragm was 25\% of that during normal breathing. FLASH MR imaging revealed variations in the position of the heart during a breath hold. During suspended respiration, the heart did not return to the same position on consecutive heartbeats and, consequently, the margins of the heart typically moved inward.
CONCLUSION: Breath holding does not eliminate motion of the diaphragm. Changes in the motion of the diaphragm and transthoracic pressure during a breath hold result in complex movement of the heart and may cause blurring during breath-hold MR imaging.},
  language = {eng},
  number = {2},
  journal = {Radiology},
  author = {Holland, A E and Goldfarb, J W and Edelman, R R},
  month = nov,
  year = {1998},
  keywords = {Adult,Artifacts,Diaphragm,Electrocardiography,Female,Heart,Humans,Image Processing; Computer-Assisted,Magnetic Resonance Imaging,Male,Myocardial Contraction,Respiration,Signal Processing; Computer-Assisted},
  pages = {483--489},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/XPIUBEJD/Holland et al. - 1998 - Diaphragmatic and cardiac motion during suspended .pdf},
  pmid = {9807578}
}

@article{sonke_respiratory_2005,
  title = {Respiratory Correlated Cone Beam {{CT}}},
  volume = {32},
  issn = {00942405},
  doi = {10.1118/1.1869074},
  language = {en},
  number = {4},
  journal = {Medical Physics},
  author = {Sonke, Jan-Jakob and Zijp, Lambert and Remeijer, Peter and {van Herk}, Marcel},
  year = {2005},
  pages = {1176},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/33W6H4A8/Sonke et al. - 2005 - Respiratory correlated cone beam CT.pdf}
}

@inproceedings{dhawan_wiener_1984,
  title = {Wiener {{Filtering For Deconvolution Of Geometric Artifacts In Limited}}-{{View Image Reconstruction}}},
  doi = {10.1117/12.964749},
  author = {Dhawan, Atam P. and Rangayyan, Rangaraj M. and Gordon, R.},
  editor = {Duerinckx, Andre J. and Loew, Murray H. and Prewitt, Judith M. S.},
  month = aug,
  year = {1984},
  pages = {168--172},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/BF3TGXSQ/x648.html}
}

@article{liu_5d_2015,
  title = {{{5D}} Respiratory Motion Model Based Image Reconstruction Algorithm for {{4D}} Cone-Beam Computed Tomography},
  volume = {31},
  issn = {0266-5611, 1361-6420},
  doi = {10.1088/0266-5611/31/11/115007},
  number = {11},
  journal = {Inverse Problems},
  author = {Liu, Jiulong and Zhang, Xue and Zhang, Xiaoqun and Zhao, Hongkai and Gao, Yu and Thomas, David and Low, Daniel A and Gao, Hao},
  month = nov,
  year = {2015},
  pages = {115007},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/MP98KKFI/Liu et al. - 2015 - 5D respiratory motion model based image reconstruc.pdf}
}

@article{cormack_representation_1964,
  title = {Representation of a {{Function}} by {{Its Line Integrals}}, with {{Some Radiological Applications}}. {{II}}},
  volume = {35},
  issn = {00218979},
  doi = {doi:10.1063/1.1713127},
  abstract = {A method is described for determining a variable gamma-ray absorption coefficient in a sample from measurements made outside the sample, and the applicability of the method to other radiological problems (e.g., positron scanning) is pointed out. An experimental test of the method is described, and it is concluded that the method works well.},
  number = {10},
  journal = {Journal of Applied Physics},
  author = {Cormack, A. M.},
  month = oct,
  year = {1964},
  pages = {2908--2913},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/A65ERIN5/p2908_s1.html}
}

@article{xu_efficiency_2010,
  title = {On the Efficiency of Iterative Ordered Subset Reconstruction Algorithms for Acceleration on {{GPUs}}},
  volume = {98},
  issn = {1872-7565},
  doi = {10.1016/j.cmpb.2009.09.003},
  abstract = {Expectation Maximization (EM) and the Simultaneous Iterative Reconstruction Technique (SIRT) are two iterative computed tomography reconstruction algorithms often used when the data contain a high amount of statistical noise, have been acquired from a limited angular range, or have a limited number of views. A popular mechanism to increase the rate of convergence of these types of algorithms has been to perform the correctional updates within subsets of the projection data. This has given rise to the method of Ordered Subsets EM (OS-EM) and the Simultaneous Algebraic Reconstruction Technique (SART). Commodity graphics hardware (GPUs) has shown great promise to combat the high computational demands incurred by iterative reconstruction algorithms. However, we find that the special architecture and programming model of GPUs add extra constraints on the real-time performance of ordered subsets algorithms, counteracting the speedup benefits of smaller subsets observed on CPUs. This gives rise to new relationships governing the optimal number of subsets as well as relaxation factor settings for obtaining the smallest wall-clock time for reconstruction-a factor that is likely application-dependent. In this paper we study the generalization of SIRT into Ordered Subsets SIRT and show that this allows one to optimize the computational performance of GPU-accelerated iterative algebraic reconstruction methods.},
  language = {eng},
  number = {3},
  journal = {Computer methods and programs in biomedicine},
  author = {Xu, Fang and Xu, Wei and Jones, Mel and Keszthelyi, Bettina and Sedat, John and Agard, David and Mueller, Klaus},
  month = jun,
  year = {2010},
  keywords = {Algorithms,Computer Graphics,Humans,Image Processing; Computer-Assisted,Software},
  pages = {261--270},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/55544TMA/Xu et al. - 2010 - On the efficiency of iterative ordered subset reco.pdf},
  pmid = {19850372},
  pmcid = {PMC2871068}
}

@article{sidky_convex_2012,
  title = {Convex Optimization Problem Prototyping for Image Reconstruction in Computed Tomography with the {{Chambolle}}\textendash{}{{Pock}} Algorithm},
  volume = {57},
  issn = {0031-9155, 1361-6560},
  doi = {10.1088/0031-9155/57/10/3065},
  number = {10},
  journal = {Physics in Medicine and Biology},
  author = {Sidky, Emil Y and J{\o}rgensen, Jakob H and Pan, Xiaochuan},
  month = may,
  year = {2012},
  pages = {3065--3091},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/PPVIN8JD/sidky2012.pdf}
}

@article{zhang_high_2017,
  title = {High Quality Four Dimensional Cone-Beam {{CT}} Reconstruction Using Motion-Compensated Total Variation Regularization},
  issn = {0031-9155, 1361-6560},
  doi = {10.1088/1361-6560/aa6128},
  journal = {Physics in Medicine and Biology},
  author = {Zhang, Hua and Ma, Jianhua and Bian, Zhaoying and Zeng, Dong and Feng, Qianjin and Chen, Wufan},
  month = feb,
  year = {2017},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/NHUENK55/Zhang et al. - 2017 - High quality four dimensional cone-beam CT reconst.pdf}
}

@article{abascal_novel_2016,
  title = {A {{Novel Prior}}- and {{Motion}}-{{Based Compressed Sensing Method}} for {{Small}}-{{Animal Respiratory Gated CT}}},
  volume = {11},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0149841},
  language = {en},
  number = {3},
  journal = {PLOS ONE},
  author = {Abascal, Juan F. P. J. and Abella, Monica and Marinetto, Eugenio and Pascau, Javier and Desco, Manuel},
  editor = {Zhang, Qinghui},
  month = mar,
  year = {2016},
  pages = {e0149841},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/QT3JBVKH/abascal2016.pdf}
}

@article{ducros_regularization_2017,
  title = {Regularization of {{Nonlinear Decomposition}} of {{Spectral X}}-Ray {{Projection Images}}},
  journal = {Medical Physics},
  author = {Ducros, Nicolas and Abascal, Juan F. P. J. and Sixou, Bruno and Rit, Simon and Peyrin, Fran{\c c}oise},
  year = {2017},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/XJHIZA9M/Ducros et al. - 2017 - Regularization of Nonlinear Decomposition of Spect.pdf}
}

@inproceedings{petschke_analytical_2015,
  title = {An Analytical Formula for the Covariance Matrix of Basis Material Projection Estimates in Spectral X-Ray Computed Tomography},
  booktitle = {Nuclear {{Science Symposium}} and {{Medical Imaging Conference}} ({{NSS}}/{{MIC}}), 2015 {{IEEE}}},
  publisher = {{IEEE}},
  author = {Petschke, Adam and Zou, Yu},
  year = {2015},
  pages = {1--2},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/DRUVCP3V/Petschke and Zou - 2015 - An analytical formula for the covariance matrix of.pdf}
}

@article{thing_accuracy_2017,
  title = {Accuracy of Dose Calculation Based on Artefact Corrected {{Cone Beam CT}} Images of Lung Cancer Patients},
  volume = {1},
  issn = {2405-6316},
  doi = {10.1016/j.phro.2016.11.001},
  abstract = {Background and purpose
Accurate Cone Beam CT (CBCT) based dose calculations are hindered by limited CBCT image quality. Using retrospective artefact corrections, this paper investigated the accuracy of dose calculations performed directly on CBCT images of lung cancer patients.
Materials and methods
Dose calculations were made directly on clinical and artefact corrected CBCT images of 21 lung cancer patients with a re-simulation CT (rCT) image acquired during radiotherapy. The original treatment plan was copied to the rCT and CBCT images and dose was recalculated. Dose comparisons were made using gamma analysis and dose statistics. Gamma comparisons were made using 2\%/2 mm and 1\%/1 mm criteria, and pass rates of the clinical and improved CBCT images were calculated using the rCT based dose as reference.
Results
Dose distributions calculated on the artefact corrected CBCT images had a median 2\%/2 mm gamma pass rate of 99.4\% when compared to the reference rCT. Doses calculated on the clinical CBCT images had a median 2\%/2 mm gamma pass rate of 93.1\%. Wilcoxon signed rank test showed the pass rates in the entire CBCT field of view different at p \&lt; 0.001 . Clinical CBCT image based dose calculations overestimated the dose, while the improved CBCT doses were in closer agreement with the rCT doses.
Conclusions
Comprehensive artefact correction of CBCT images allowed highly accurate dose calculations to be performed directly on CBCT images of lung cancer patients, following the standard CT-based workflow in a treatment planning system.},
  journal = {Physics and Imaging in Radiation Oncology},
  author = {Thing, Rune Slot and Bernchou, Uffe and Hansen, Olfred and Brink, Carsten},
  month = jan,
  year = {2017},
  keywords = {Image guided radiotherapy,Adaptive radiotherapy,Dose of the day,Cone Beam CT},
  pages = {6--11},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/K4AFACQS/Thing et al. - 2017 - Accuracy of dose calculation based on artefact cor.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/8T7XAMAG/S2405631616300045.html}
}

@article{rohkohl_technical_2009,
  title = {Technical {{Note}}: {{RabbitCT}}-an Open Platform for Benchmarking {{3D}} Cone-Beam Reconstruction Algorithmsa): {{RabbitCT}}-Platform for Benchmarking {{3D}} Reconstruction Algorithms},
  volume = {36},
  issn = {00942405},
  shorttitle = {Technical {{Note}}},
  doi = {10.1118/1.3180956},
  language = {en},
  number = {9},
  journal = {Medical Physics},
  author = {Rohkohl, C. and Keck, B. and Hofmann, H. G. and Hornegger, J.},
  month = aug,
  year = {2009},
  pages = {3940--3944},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/KMK8SD5D/Rohkohl09-TNR.pdf}
}

@article{cai_full-spectral_2013,
  title = {A Full-Spectral {{Bayesian}} Reconstruction Approach Based on the Material Decomposition Model Applied in Dual-Energy Computed Tomography},
  volume = {40},
  number = {11},
  journal = {Medical Physics},
  author = {Cai, Caifang and Rodet, Thomas and Legoupil, Samuel and Mohammad-Djafari, Ali},
  month = nov,
  year = {2013},
  pages = {111916--111931},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/5PP6QP8V/Cai et al. - 2013 - A full-spectral Bayesian reconstruction approach b.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/F7G2UHRV/hal-00911736.html}
}

@inproceedings{fessler_maximum-likelihood_2002,
  title = {Maximum-Likelihood Dual-Energy Tomographic Image Reconstruction},
  booktitle = {Medical {{Imaging}} 2002},
  publisher = {{International Society for Optics and Photonics}},
  author = {Fessler, Jeffrey A. and Elbakri, Idris A. and Sukovic, Predrag and Clinthorne, Neal H.},
  year = {2002},
  pages = {38--49},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/WJNKCA79/Fessler et al. - 2002 - Maximum-likelihood dual-energy tomographic image r.pdf}
}

@article{chung_numerical_2010-1,
  title = {Numerical {{Algorithms}} for {{Polyenergetic Digital Breast Tomosynthesis Reconstruction}}},
  volume = {3},
  issn = {1936-4954},
  doi = {10.1137/090749633},
  language = {en},
  number = {1},
  journal = {SIAM Journal on Imaging Sciences},
  author = {Chung, Julianne and Nagy, James G. and Sechopoulos, Ioannis},
  month = jan,
  year = {2010},
  pages = {133--152}
}

@article{nakada_joint_2015,
  title = {Joint Estimation of Tissue Types and Linear Attenuation Coefficients for Photon Counting {{CT}}},
  volume = {42},
  issn = {2473-4209},
  doi = {10.1118/1.4927261},
  abstract = {Purpose:

Newly developed spectral computed tomography (CT) such as photon counting detector CT enables more accurate tissue-type identification through material decomposition technique. Many iterative reconstruction methods, including those developed for spectral CT, however, employ a regularization term whose penalty transition is designed using pixel value of CT image itself. Similarly, the tissue-type identification methods are then applied after reconstruction; thus, it is impossible to take into account probability distribution obtained from projection likelihood. The purpose of this work is to develop comprehensive image reconstruction and tissue-type identification algorithm which improves quality of both reconstructed image and tissue-type map.

Methods:

The authors propose a new framework to jointly perform image reconstruction, material decomposition, and tissue-type identification for photon counting detector CT by applying maximum a posteriori estimation with voxel-based latent variables for the tissue types. The latent variables are treated using a voxel-based coupled Markov random field to describe the continuity and discontinuity of human organs and a set of Gaussian distributions to incorporate the statistical relation between the tissue types and their attenuation characteristics. The performance of the proposed method is quantitatively compared to that of filtered backprojection and a quadratic penalized likelihood method by 100 noise realization.

Results:

Results showed a superior trade-off between image noise and resolution to current reconstruction methods. The standard deviation (SD) and bias of reconstructed image were improved from quadratic penalized likelihood method: bias, -0.9 vs -0.1 Hounsfield unit (HU); SD, 46.8 vs 27.4 HU. The accuracy of tissue-type identification was also improved from quadratic penalized likelihood method: 80.1\% vs 86.9\%.

Conclusions:

The proposed method makes it possible not only to identify tissue types more accurately but also to reconstruct CT images with decreased noise and enhanced sharpness owing to the information about the tissue types.},
  language = {en},
  number = {9},
  journal = {Medical Physics},
  author = {Nakada, Kento and Taguchi, Katsuyuki and Fung, George S. K. and Amaya, Kenji},
  month = sep,
  year = {2015},
  keywords = {Computed tomography,Image reconstruction,Image resolution,Iterative reconstruction,Medical image noise,Medical image reconstruction,Reconstruction,computerised tomography,image denoising,medical image processing,noise,probability,photon counting,biological tissues,Gaussian distribution,Markov processes,Spatial resolution,Fluctuation phenomena; random processes; noise; and Brownian motion,Computerised tomographs,Biological material; e.g. blood; urine,Haemocytometers,Digital computing or data processing equipment or methods; specially adapted for specific applications,Image data processing or generation; in general,Image enhancement or restoration; e.g. from bit-mapped to bit-mapped creating a similar image,maximum a posteriori estimation,coupled Markov random field,Tissue characterization,Statistical properties,Probability theory,Image sensors},
  pages = {5329--5341},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/HV76MITM/Nakada et al. - 2015 - Joint estimation of tissue types and linear attenu.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/HJANVEG2/abstract.html}
}

@article{schmidt_spectral_2017,
  title = {A {{Spectral CT}} Method to Directly Estimate Basis Material Maps from Experimental Photon-Counting Data},
  issn = {0278-0062, 1558-254X},
  doi = {10.1109/TMI.2017.2696338},
  journal = {IEEE Transactions on Medical Imaging},
  author = {Schmidt, Taly and Barber, Rina and Sidky, Emil},
  year = {2017},
  pages = {1--1},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/9TU97M45/Schmidt et al. - 2017 - A Spectral CT method to directly estimate basis ma.pdf}
}

@article{chung_numerical_2010,
  title = {Numerical {{Algorithms}} for {{Polyenergetic Digital Breast Tomosynthesis Reconstruction}}},
  volume = {3},
  issn = {1936-4954},
  doi = {10.1137/090749633},
  language = {en},
  number = {1},
  journal = {SIAM Journal on Imaging Sciences},
  author = {Chung, Julianne and Nagy, James G. and Sechopoulos, Ioannis},
  month = jan,
  year = {2010},
  pages = {133--152},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/4KCUSRGD/CNS_10.pdf}
}

@article{zhao_extended_2015,
  title = {An {{Extended Algebraic Reconstruction Technique}} ({{E}}-{{ART}}) for {{Dual Spectral CT}}},
  volume = {34},
  issn = {0278-0062, 1558-254X},
  doi = {10.1109/TMI.2014.2373396},
  number = {3},
  journal = {IEEE Transactions on Medical Imaging},
  author = {Zhao, Yunsong and Zhao, Xing and Zhang, Peng},
  month = mar,
  year = {2015},
  pages = {761--768},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/S4NTNAQI/Zhao et al. - 2015 - An Extended Algebraic Reconstruction Technique (E-.pdf}
}

@article{ruoqiao_zhang_model-based_2014,
  title = {Model-{{Based Iterative Reconstruction}} for {{Dual}}-{{Energy X}}-{{Ray CT Using}} a {{Joint Quadratic Likelihood Model}}},
  volume = {33},
  issn = {0278-0062, 1558-254X},
  doi = {10.1109/TMI.2013.2282370},
  number = {1},
  journal = {IEEE Transactions on Medical Imaging},
  author = {{Ruoqiao Zhang} and Thibault, Jean-Baptiste and Bouman, Charles A. and Sauer, Ken D. and {Jiang Hsieh}},
  month = jan,
  year = {2014},
  pages = {117--134},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/7V54ZIQS/zhang2014.pdf}
}

@article{mcclelland_generalized_2017,
  title = {A Generalized Framework Unifying Image Registration and Respiratory Motion Models and Incorporating Image Reconstruction, for Partial Image Data or Full Images},
  volume = {62},
  issn = {0031-9155, 1361-6560},
  doi = {10.1088/1361-6560/aa6070},
  number = {11},
  journal = {Physics in Medicine and Biology},
  author = {McClelland, Jamie R and Modat, Marc and Arridge, Simon and Grimes, Helen and D?Souza, Derek and Thomas, David and Connell, Dylan O? and Low, Daniel A and Kaza, Evangelia and Collins, David J and Leach, Martin O and Hawkes, David J},
  month = jun,
  year = {2017},
  pages = {4273--4292},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/C5XW2X7B/McClelland et al. - 2017 - A generalized framework unifying image registratio.pdf}
}

@article{arcadu_crucial_2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1612.05515},
  primaryClass = {cs},
  title = {On the Crucial Impact of the Coupling Projector-Backprojector in Iterative Tomographic Reconstruction},
  abstract = {The performance of an iterative reconstruction algorithm for X-ray tomography is strongly determined by the features of the used forward and backprojector. For this reason, a large number of studies has focused on the to design of projectors with increasingly higher accuracy and speed. To what extent the accuracy of an iterative algorithm is affected by the mathematical affinity and the similarity between the actual implementation of the forward and backprojection, referred here as "coupling projector-backprojector", has been an overlooked aspect so far. The experimental study presented here shows that the reconstruction quality and the convergence of an iterative algorithm greatly rely on a good matching between the implementation of the tomographic operators. In comparison, other aspects like the accuracy of the standalone operators, the usage of physical constraints or the choice of stopping criteria may even play a less relevant role.},
  journal = {arXiv:1612.05515 [cs]},
  author = {Arcadu, Filippo and Stampanoni, Marco and Marone, Federica},
  month = dec,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/W9SPANWR/Arcadu et al. - 2016 - On the crucial impact of the coupling projector-ba.pdf}
}

@book{james_statistical_2006,
  edition = {2nd},
  title = {Statistical {{Methods}} in {{Experimental Physics}}},
  isbn = {978-981-256-795-6},
  abstract = {The first edition of this classic book has become the authoritative reference for physicists desiring to master the finer points of statistical data analysis. This second edition contains all the important material of the first, much of it unavailable from any other sources. In addition, many chapters have been updated with considerable new material, especially in areas concerning the theory and practice of confidence intervals, including the important Feldman-Cousins method. Both frequentist and Bayesian methodologies are presented, with a strong emphasis on techniques useful to physicists and other scientists in the interpretation of experimental data and comparison with scientific theories. This is a valuable textbook for advanced graduate students in the physical sciences as well as a reference for active researchers. Sample Chapter(s) Chapter 1: Introduction (122 KB) Contents: Basic Concepts in Probability Convergence and the Law of Large Numbers Probability Distributions Information Decision Theory Theory of Estimators Point Estimation in Practice Interval Estimation Test of Hypotheses Goodness-of-Fit Tests Readership: Advanced students, lecturers and research scientists in physics, astronomy and related sciences. Improved presentation compared to the first editionUpdated chapters in addition to new ones on the theory and practice of confidence intervals --$>$Keywords: Statistics; Confidence Intervals; Parameter Estimation; Hypothesis; Data Analysis; Goodness of Fit; Probability, Frequentist; Bayesian; Curve Fitting; Error Analysis; --$>$ Updated pub date on 1/6/2006 Updated grade on 3/7/2006 Added s/c on 23/10/2006 Updated contents, pp \& pub date on 5/12/2006 Added ebook on 30/08/2007 Added review on 17/1/2008 Updated price on 18/06/2008 Hui Chee --$>$},
  publisher = {{World Scientific Publishing}},
  author = {James, Frederik},
  year = {2006},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/39IAH5NB/6096.html}
}

@inproceedings{brown_impact_2015,
  title = {Impact of {{Spectral Separation}} in {{Dual}}-{{Energy CT}} with {{Anti}}-{{Correlated Statistical Reconstruction}}},
  abstract = {We  compare  the  performance  of  a  realistic  dual-layer  spectral  CT  system  with  an  ideal  detector  having perfect energy  separation.  When  combined  with  an  appropriate  reconstruction method, we show that the performance of the realistic detector  nearly  matches  the  ideal  detector,  for  a  given  material classification  task.},
  author = {Brown, Kevin and Zabic, Stanislav and Shechter, Gilad},
  year = {2015},
  pages = {491--494},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/VHN89NAG/brown2015.pdf}
}

@article{mechlem_joint_2017,
  title = {Joint Statistical Iterative Material Image Reconstruction for Spectral Computed Tomography Using a Semi-Empirical Forward Model},
  issn = {0278-0062, 1558-254X},
  doi = {10.1109/TMI.2017.2726687},
  journal = {IEEE Transactions on Medical Imaging},
  author = {Mechlem, Korbinian and Ehn, Sebastian and Sellerer, Thorsten and Braig, Eva and Munzel, Daniela and Pfeiffer, Franz and Noel, Peter B.},
  year = {2017},
  pages = {1--1},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/6W43VTJE/Mechlem et al. - 2017 - Joint statistical iterative material image reconst.pdf}
}

@article{erdogan_monotonic_1999,
  title = {Monotonic Algorithms for Transmission Tomography},
  volume = {18},
  number = {9},
  journal = {IEEE transactions on medical imaging},
  author = {Erdogan, Hakan and Fessler, Jeffrey A.},
  year = {1999},
  pages = {801--814},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/GKCQUJUI/Erdogan and Fessler - 1999 - Monotonic algorithms for transmission tomography.pdf}
}

@article{tilley_model-based_2016,
  title = {Model-Based {{Iterative Reconstruction}} for {{Flat}}-{{Panel Cone}}-{{Beam CT}} with {{Focal Spot Blur}}, {{Detector Blur}}, and {{Correlated Noise}}},
  volume = {61},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/61/1/296},
  abstract = {While model-based reconstruction methods have been successfully applied to flat-panel cone-beam CT (FP-CBCT) systems, typical implementations ignore both spatial correlations in the projection data as well as system blurs due to the detector and focal spot in the x-ray source. In this work, we develop a forward model for flat-panel-based systems that includes blur and noise correlation associated with finite focal spot size and an indirect detector (e.g., scintillator). This forward model is used to develop a staged reconstruction framework where projection data are deconvolved and log-transformed, followed by a generalized least-squares reconstruction that utilizes a non-diagonal statistical weighting to account for the correlation that arises from the acquisition and data processing chain. We investigate the performance of this novel reconstruction approach in both simulated data and in CBCT test-bench data. In comparison to traditional filtered backprojection and model-based methods that ignore noise correlation, the proposed approach yields a superior noise-resolution tradeoff. For example, for a system with 0.34 mm FWHM scintillator blur and 0.70 FWHM focal spot blur, using the a correlated noise model instead of an uncorrelated noise model increased resolution by 42\% (with variance matched at 6.9 \texttimes{} 10-8 mm-2). While this advantage holds across a wide range of systems with differing blur characteristics, the improvements are greatest for systems where source blur is larger than detector blur.},
  number = {1},
  journal = {Physics in medicine and biology},
  author = {Tilley, Steven and Siewerdsen, Jeffrey H. and Stayman, J. Webster},
  month = jan,
  year = {2016},
  pages = {296--319},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/IHAKKPSZ/Tilley et al. - 2016 - Model-based Iterative Reconstruction for Flat-Pane.pdf},
  pmid = {26649783},
  pmcid = {PMC5011041}
}

@inproceedings{hoeschen_impact_2015,
  title = {Impact of Covariance Modeling in Dual-Energy Spectral {{CT}} Image Reconstruction},
  doi = {10.1117/12.2082000},
  author = {Liu, Yan and Yu, Zhou and Zou, Yu},
  editor = {Hoeschen, Christoph and Kontos, Despina and Flohr, Thomas G.},
  month = mar,
  year = {2015},
  pages = {94123Q},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/TRJWIW2E/Liu et al. - 2015 - Impact of covariance modeling in dual-energy spect.pdf}
}

@article{weidinger_polychromatic_2016,
  title = {Polychromatic {{Iterative Statistical Material Image Reconstruction}} for {{Photon}}-{{Counting Computed Tomography}}},
  volume = {2016},
  issn = {1687-4188, 1687-4196},
  doi = {10.1155/2016/5871604},
  language = {en},
  journal = {International Journal of Biomedical Imaging},
  author = {Weidinger, Thomas and Buzug, Thorsten M. and Flohr, Thomas and Kappler, Steffen and Stierstorfer, Karl},
  year = {2016},
  pages = {1--15},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/2HIFVC2D/Weidinger et al. - 2016 - Polychromatic Iterative Statistical Material Image.pdf}
}

@article{de_pierro_modified_1995,
  title = {A Modified Expectation Maximization Algorithm for Penalized Likelihood Estimation in Emission Tomography},
  volume = {14},
  number = {1},
  journal = {IEEE transactions on medical imaging},
  author = {De Pierro, Alvaro R.},
  year = {1995},
  pages = {132--137},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/EZPAM9QR/De Pierro - 1995 - A modified expectation maximization algorithm for .pdf}
}

@article{hansen_air_2012,
  series = {Inverse Problems: Computation and Applications},
  title = {{{AIR Tools}} \textemdash{} {{A MATLAB}} Package of Algebraic Iterative Reconstruction Methods},
  volume = {236},
  issn = {0377-0427},
  doi = {10.1016/j.cam.2011.09.039},
  abstract = {We present a MATLAB package with implementations of several algebraic iterative reconstruction methods for discretizations of inverse problems. These so-called row action methods rely on semi-convergence for achieving the necessary regularization of the problem. Two classes of methods are implemented: Algebraic Reconstruction Techniques (ART) and Simultaneous Iterative Reconstruction Techniques (SIRT). In addition we provide a few simplified test problems from medical and seismic tomography. For each iterative method, a number of strategies are available for choosing the relaxation parameter and the stopping rule. The relaxation parameter can be fixed, or chosen adaptively in each iteration; in the former case we provide a new ``training'' algorithm that finds the optimal parameter for a given test problem. The stopping rules provided are the discrepancy principle, the monotone error rule, and the NCP criterion; for the first two methods ``training'' can be used to find the optimal discrepancy parameter.},
  number = {8},
  journal = {Journal of Computational and Applied Mathematics},
  author = {Hansen, Per Christian and Saxild-Hansen, Maria},
  month = feb,
  year = {2012},
  keywords = {ART methods,SIRT methods,Semi-convergence,Relaxation parameters,Stopping rules,Tomographic imaging},
  pages = {2167--2178},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/5VTPIT5E/Hansen and Saxild-Hansen - 2012 - AIR Tools â€” A MATLAB package of algebraic iterativ.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/3VDEAHZ4/S0377042711005188.html}
}

@article{green_bayesian_1990,
  title = {Bayesian Reconstructions from Emission Tomography Data Using a Modified {{EM}} Algorithm},
  volume = {9},
  number = {1},
  journal = {IEEE transactions on medical imaging},
  author = {Green, Peter J.},
  year = {1990},
  pages = {84--93},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/7Q3UPQRL/Green - 1990 - Bayesian reconstructions from emission tomography .pdf}
}

@article{erdogan_ordered_1999,
  title = {Ordered Subsets Algorithms for Transmission Tomography},
  volume = {44},
  issn = {0031-9155},
  doi = {10.1088/0031-9155/44/11/311},
  abstract = {The ordered subsets EM (OSEM) algorithm has enjoyed considerable interest for emission image reconstruction due to its acceleration of the original EM algorithm and ease of programming. The transmission EM reconstruction algorithm converges very slowly and is not used in practice. In this paper, we introduce a simultaneous update algorithm called separable paraboloidal surrogates (SPS) that converges much faster than the transmission EM algorithm. Furthermore, unlike the `convex algorithm' for transmission tomography, the proposed algorithm is monotonic even with nonzero background counts. We demonstrate that the ordered subsets principle can also be applied to the new SPS algorithm for transmission tomography to accelerate `convergence', albeit with similar sacrifice of global convergence properties as for OSEM. We implemented and evaluated this ordered subsets transmission (OSTR) algorithm. The results indicate that the OSTR algorithm speeds up the increase in the objective function by roughly the number of subsets in the early iterates when compared to the ordinary SPS algorithm. We compute mean square errors and segmentation errors for different methods and show that OSTR is superior to OSEM applied to the logarithm of the transmission data. However, penalized-likelihood reconstructions yield the best quality images among all other methods tested.},
  language = {en},
  number = {11},
  journal = {Physics in Medicine \& Biology},
  author = {Erdogan, H. and Fessler, J. A.},
  year = {1999},
  pages = {2835},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/47KW4ZNQ/Ordered subsets algorithms for transmission tomogr.pdf}
}

@article{pearson_generalized_2016,
  title = {Generalized {{Hampel Filters}}},
  volume = {2016},
  issn = {1687-6180},
  doi = {10.1186/s13634-016-0383-6},
  abstract = {The standard median filter based on a symmetric moving window has only one tuning parameter: the window width. Despite this limitation, this filter has proven extremely useful and has motivated a number of extensions: weighted median filters, recursive median filters, and various cascade structures. The Hampel filter is a member of the class of decsion filters that replaces the central value in the data window with the median if it lies far enough from the median to be deemed an outlier. This filter depends on both the window width and an additional tuning parameter t, reducing to the median filter when t=0, so it may be regarded as another median filter extension. This paper adopts this view, defining and exploring the class of generalized Hampel filters obtained by applying the median filter extensions listed above: weighted Hampel filters, recursive Hampel filters, and their cascades. An important concept introduced here is that of an implosion sequence, a signal for which generalized Hampel filter performance is independent of the threshold parameter t. These sequences are important because the added flexibility of the generalized Hampel filters offers no practical advantage for implosion sequences. Partial characterization results are presented for these sequences, as are useful relationships between root sequences for generalized Hampel filters and their median-based counterparts. To illustrate the performance of this filter class, two examples are considered: one is simulation-based, providing a basis for quantitative evaluation of signal recovery performance as a function of t, while the other is a sequence of monthly Italian industrial production index values that exhibits glaring outliers.},
  language = {en},
  number = {1},
  journal = {EURASIP Journal on Advances in Signal Processing},
  author = {Pearson, Ronald K. and Neuvo, Yrj{\"o} and Astola, Jaakko and Gabbouj, Moncef},
  month = dec,
  year = {2016},
  pages = {87},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/M2WNKTX6/Pearson et al. - 2016 - Generalized Hampel Filters.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/PEPID9CW/Pearson et al. - 2016 - Generalized Hampel Filters.html}
}

@article{chen_image_2017,
  title = {Image Reconstruction and Scan Configurations Enabled by Optimization-Based Algorithms in Multispectral {{CT}}},
  volume = {62},
  issn = {0031-9155},
  doi = {10.1088/1361-6560/aa8a4b},
  abstract = {Optimization-based algorithms for image reconstruction in multispectral (or photon-counting) computed tomography (MCT) remains a topic of active research. The challenge of optimization-based image reconstruction in MCT stems from the inherently non-linear data model that can lead to a non-convex optimization program for which no mathematically exact solver seems to exist for achieving globally optimal solutions. In this work, based upon a non-linear data model, we design a non-convex optimization program, derive its first-order-optimality conditions, and propose an algorithm to solve the program for image reconstruction in MCT. In addition to consideration of image reconstruction for the standard scan configuration, the emphasis is on investigating the algorithm's potential for enabling non-standard scan configurations with no or minimum hardware modification to existing CT systems, which has potential practical implications for lowered hardware cost, enhanced scanning flexibility, and reduced imaging dose/time in MCT. Numerical studies are carried out for verification of the algorithm and its implementation, and for a preliminary demonstration and characterization of the algorithm in reconstructing images and in enabling non-standard configurations with varying scanning angular range and/or x-ray illumination coverage in MCT.},
  language = {en},
  number = {22},
  journal = {Physics in Medicine \& Biology},
  author = {Chen, Buxin and Zhang, Zheng and Sidky, Emil Y. and Xia, Dan and Pan, Xiaochuan},
  year = {2017},
  pages = {8763},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/RJSVU8HW/Chen et al. - 2017 - Image reconstruction and scan configurations enabl.pdf}
}

@article{alvarez_energy-selective_1976,
  title = {Energy-Selective Reconstructions in {{X}}-Ray Computerized Tomography},
  volume = {21},
  issn = {0031-9155},
  abstract = {All X-ray computerized tomography systems that are available or proposed base their reconstructions on measurements that integrate over energy. X-ray tubes produce a broad spectrum of photon energies and a great deal of information can be derived by measuring changes in the transmitted spectrum. We show that for any material, complete energy spectral information may be summarized by a few constants which are independent of energy. A technique is presented which uses simple, low-resolution, energy spectrum measurements and conventional computerized tomography techniques to calculate these constants at every point within a cross-section of an object. For comparable accuracy, patient dose is shown to be approximately the same as that produced by conventional systems. Possible uses of energy spectral information for diagnosis are presented.},
  language = {eng},
  number = {5},
  journal = {Physics in Medicine and Biology},
  author = {Alvarez, R. E. and Macovski, A.},
  month = sep,
  year = {1976},
  keywords = {Humans,Radiation Dosage,Technology; Radiologic,Tomography; X-Ray Computed,Energy Transfer},
  pages = {733--744},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/ZT5G39D8/alvarez1976.pdf},
  pmid = {967922}
}

@article{maass_image-based_2009,
  title = {Image-Based Dual Energy {{CT}} Using Optimized Precorrection Functions: A Practical New Approach of Material Decomposition in Image Domain},
  volume = {36},
  issn = {0094-2405},
  shorttitle = {Image-Based Dual Energy {{CT}} Using Optimized Precorrection Functions},
  doi = {10.1118/1.3157235},
  abstract = {Dual energy CT (DECT) measures the object of interest using two different x-ray spectra in order to provide energy-selective CT images or in order to get the material decomposition of the object. Today, two decomposition techniques are known. Image-based DECT uses linear combinations of reconstructed images to get an image that contains material-selective DECT information. Rawdata-based DECT correctly treats the available information by passing the rawdata through a decomposition function that uses information from both rawdata sets to create DECT specific (e.g., material-selective) rawdata. Then the image reconstruction yields material-selective images. Rawdata-based image decomposition generally obtains better image quality; however, it needs matched rawdata sets. This means that physically the same lines need to be measured for each spectrum. In today's CT scanners, this is not the case. The authors propose a new image-based method to combine mismatched rawdata sets for DECT information. The method allows for implementation in a scanner's rawdata precorrection pipeline or may be used in image domain. They compare the ability of the three methods (image-based standard method, proposed method, and rawdata-based standard method) to perform material decomposition and to provide monochromatic images. Thereby they use typical clinical and preclinical scanner arrangements including circular cone-beam CT and spiral CT. The proposed method is found to perform better than the image-based standard method and is inferior to the rawdata-based method. However, the proposed method can be used with the frequent case of mismatched data sets that exclude rawdata-based methods.},
  language = {eng},
  number = {8},
  journal = {Medical Physics},
  author = {Maass, Clemens and Baer, Matthias and Kachelriess, Marc},
  month = aug,
  year = {2009},
  keywords = {Algorithms,Animals,Calibration,Image Processing; Computer-Assisted,Mice,Radiography; Thoracic,Tomography; X-Ray Computed,X-Ray Microtomography},
  pages = {3818--3829},
  pmid = {19746815}
}

@article{niu_iterative_2014,
  title = {Iterative Image-Domain Decomposition for Dual-Energy {{CT}}},
  volume = {41},
  issn = {2473-4209},
  doi = {10.1118/1.4866386},
  abstract = {Purpose:

Dual energy CT (DECT) imaging plays an important role in advanced imaging applications due to its capability of material decomposition. Direct decomposition via matrix inversion suffers from significant degradation of image signal-to-noise ratios, which reduces clinical values of DECT. Existing denoising algorithms achieve suboptimal performance since they suppress image noise either before or after the decomposition and do not fully explore the noise statistical properties of the decomposition process. In this work, the authors propose an iterative image-domain decomposition method for noise suppression in DECT, using the full variance-covariance matrix of the decomposed images.

Methods:

The proposed algorithm is formulated in the form of least-square estimation with smoothness regularization. Based on the design principles of a best linear unbiased estimator, the authors include the inverse of the estimated variance-covariance matrix of the decomposed images as the penalty weight in the least-square term. The regularization term enforces the image smoothness by calculating the square sum of neighboring pixel value differences. To retain the boundary sharpness of the decomposed images, the authors detect the edges in the CT images before decomposition. These edge pixels have small weights in the calculation of the regularization term. Distinct from the existing denoising algorithms applied on the images before or after decomposition, the method has an iterative process for noise suppression, with decomposition performed in each iteration. The authors implement the proposed algorithm using a standard conjugate gradient algorithm. The method performance is evaluated using an evaluation phantom (Catphan\textcopyright{}600) and an anthropomorphic head phantom. The results are compared with those generated using direct matrix inversion with no noise suppression, a denoising method applied on the decomposed images, and an existing algorithm with similar formulation as the proposed method but with an edge-preserving regularization term.

Results:

On the Catphan phantom, the method maintains the same spatial resolution on the decomposed images as that of the CT images before decomposition (8 pairs/cm) while significantly reducing their noise standard deviation. Compared to that obtained by the direct matrix inversion, the noise standard deviation in the images decomposed by the proposed algorithm is reduced by over 98\%. Without considering the noise correlation properties in the formulation, the denoising scheme degrades the spatial resolution to 6 pairs/cm for the same level of noise suppression. Compared to the edge-preserving algorithm, the method achieves better low-contrast detectability. A quantitative study is performed on the contrast-rod slice of Catphan phantom. The proposed method achieves lower electron density measurement error as compared to that by the direct matrix inversion, and significantly reduces the error variation by over 97\%. On the head phantom, the method reduces the noise standard deviation of decomposed images by over 97\% without blurring the sinus structures.

Conclusions:

The authors propose an iterative image-domain decomposition method for DECT. The method combines noise suppression and material decomposition into an iterative process and achieves both goals simultaneously. By exploring the full variance-covariance properties of the decomposed images and utilizing the edge predetection, the proposed algorithm shows superior performance on noise suppression with high image spatial resolution and low-contrast detectability.},
  language = {en},
  number = {4},
  journal = {Medical Physics},
  author = {Niu, Tianye and Dong, Xue and Petrongolo, Michael and Zhu, Lei},
  month = apr,
  year = {2014},
  keywords = {Computed tomography,Medical image noise,Medical image reconstruction,Medical imaging,computerised tomography,conjugate gradient methods,edge detection,image denoising,iterative methods,least squares approximations,medical image processing,phantoms,Spatial resolution,Computerised tomographs,Biological material; e.g. blood; urine,Haemocytometers,Digital computing or data processing equipment or methods; specially adapted for specific applications,Image data processing or generation; in general,Image enhancement or restoration; e.g. from bit-mapped to bit-mapped creating a similar image,best linear unbiased estimation,covariance matrices,Density measurement,dual-energy CT,edge predetection,General statistical methods,image resolution,image-domain decomposition,matrix inversion,Matrix inversion,Matrix theory,Medical image smoothing,Medical image spatial resolution,Medical X-ray imaging,Noise,noise suppression,Numerical linear algebra},
  pages = {n/a--n/a},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/IEDQG6C9/Niu et al. - 2014 - Iterative image-domain decomposition for dual-ener.pdf}
}

@patent{fessler_method_2003,
  title = {Method for Statistically Reconstructing Images from a Plurality of Transmission Measurements Having Energy Diversity and Image Reconstructor Apparatus Utilizing the Method},
  abstract = {A method for statistically reconstructing images from a plurality of transmission measurements having energy diversity and image reconstructor apparatus utilizing the method are provided. A statistical (maximum-likelihood) method for dual-energy X-ray CT accommodates a wide variety of potential system configurations and measurement noise models. Regularized methods (such as penalized-likelihood or Bayesian estimations) are straightforward extensions. One version of the algorithm monotonically decreases the negative log-likelihood cost function each iteration. An ordered-subsets variation of the algorithm provides a fast and practical version. The method and apparatus provide material characterization and quantitatively accurate CT values in a variety of applications. The method and apparatus provide improved noise/dose properties.},
  assignee = {The Regents Of The University Of Michigan},
  number = {WO2003071483 A2},
  author = {Fessler, Jeffrey A.},
  month = aug,
  year = {2003},
  note = {International Classification G06T5/00; Cooperative Classification G06T2207/10081, G06T5/002, G06T11/006, A61B6/482, A61B6/4241, A61B6/405, A61B6/032, A61B6/4035, G06T2211/408, G06T2211/424; European Classification A61B6/48D, G06T11/00T3, A61B6/03B, A61B6/40H, A61B6/40F, A61B6/42B8, G06T5/00D}
}

@article{steadman_chromaix2_2017,
  title = {{{ChromAIX2}}: {{A}} Large Area, High Count-Rate Energy-Resolving Photon Counting {{ASIC}} for a {{Spectral CT Prototype}}},
  volume = {862},
  shorttitle = {{{ChromAIX2}}},
  doi = {10.1016/j.nima.2017.05.010},
  abstract = {Spectral CT based on energy-resolving photon counting detectors is expected to deliver additional diagnostic value at a lower dose than current state-of-the-art CT [1]. The capability of simultaneously providing a number of spectrally distinct measurements not only allows distinguishing between photo-electric and Compton interactions but also discriminating contrast agents that exhibit a K-edge discontinuity in the absorption spectrum, referred to as K-edge Imaging [2]. Such detectors are based on direct converting sensors (e.g. CdTe or CdZnTe) and high-rate photon counting electronics.
To support the development of Spectral CT and show the feasibility of obtaining rates exceeding 10 Mcps/pixel (Poissonian observed count-rate), the ChromAIX ASIC has been previously reported showing 13.5 Mcps/pixel (150 Mcps/mm$^2$ incident) [3]. The ChromAIX has been improved to offer the possibility of a large area coverage detector, and increased overall performance. The new ASIC is called ChromAIX2, and delivers count-rates exceeding 15 Mcps/pixel with an rms-noise performance of approximately 260 e-. It has an isotropic pixel pitch of 500 $\mathrm{\mu}$m in an array of 22\texttimes{}32 pixels and is tile-able on three of its sides. The pixel topology consists of a two stage amplifier (CSA and Shaper) and a number of test features allowing to thoroughly characterize the ASIC without a sensor. A total of 5 independent thresholds are also available within each pixel, allowing to acquire 5 spectrally distinct measurements simultaneously. The ASIC also incorporates a baseline restorer to eliminate excess currents induced by the sensor (e.g. dark current and low frequency drifts) which would otherwise cause an energy estimation error.
In this paper we report on the inherent electrical performance of the ChromAXI2 as well as measurements obtained with CZT (CdZnTe)/CdTe sensors and X-rays and radioactive sources.},
  journal = {Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment},
  author = {Steadman, Roger and Herrmann, Christoph and Livne, Amir},
  month = may,
  year = {2017}
}

@article{roessl_fourier_2016,
  title = {A {{Fourier}} Approach to Pulse Pile-up in Photon-Counting x-Ray Detectors},
  volume = {43},
  issn = {2473-4209},
  doi = {10.1118/1.4941743},
  abstract = {PURPOSE: An analytic Fourier approach to predict the expected number of counts registered in a photon-counting detector subject to pulse pile-up for arbitrary photon flux, detector response function, and pulse-shape is presented. The analysis provides a complete forward model for energy-sensitive, photon-counting x-ray detectors for spectral computed tomography.
METHODS: The formalism of the stochastic theory of the expected frequency of level crossings of shot noise processes is applied to the pulse pile-up effect and build on a recently published analytic Fourier representation of the level crossing frequency of shot noise processes with piece-wise continuous kernels with jumps.
RESULTS: The general analytic result is validated by a Monte Carlo simulation for pulses of the form g(t) = e(-t/$\tau$)\,(t $>$ 0) and a Gaussian detector response function. The Monte Carlo simulations are in excellent agreement with the analytic predictions of photon counts within the numerical accuracy of the calculations.
CONCLUSIONS: The phenomenon of pulse pile-up is identified with the level-crossing problem of shot noise processes and an exact, analytic formula for the expected number of counts in energy-sensitive, photon-counting x-ray detectors for arbitrary photon flux, response function, and pulse-shapes is derived. The framework serves as a theoretical foundation for future works on pulse pile-up.},
  language = {eng},
  number = {3},
  journal = {Medical Physics},
  author = {Roessl, Ewald and Daerr, Heiner and Proksa, Roland},
  month = mar,
  year = {2016},
  keywords = {Photons,Radiometry,X-Rays,Fourier Analysis,Monte Carlo Method},
  pages = {1295--1298},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/HJHCPLTI/Roessl et al. - 2016 - A Fourier approach to pulse pile-up in photon-coun.pdf},
  pmid = {26936714}
}

@incollection{fessler_optimization_,
  title = {Optimization {{Transfer Methods}}},
  booktitle = {Image {{Reconstruction}}: {{Algorithms}} and {{Analysis}}},
  author = {Fessler, Jeffrey A.},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/8CXACMK5/Fessler - Optimization Transfer Methods.pdf}
}

@article{bergounioux_variational_2010,
  title = {A Variational Method Using Fractional Order {{Hilbert}} Spaces for Tomographic Reconstruction of Blurred and Noised Binary Images},
  volume = {259},
  issn = {00221236},
  doi = {10.1016/j.jfa.2010.05.016},
  language = {en},
  number = {9},
  journal = {Journal of Functional Analysis},
  author = {Bergounioux, M. and Tr{\'e}lat, E.},
  month = nov,
  year = {2010},
  pages = {2296--2332},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/R68A8UTT/Bergounioux and TrÃ©lat - 2010 - A variational method using fractional order Hilber.pdf}
}

@article{keerthi_convergence_2002,
  title = {Convergence of a {{Generalized SMO Algorithm}} for {{SVM Classifier Design}}},
  volume = {46},
  issn = {0885-6125, 1573-0565},
  doi = {10.1023/A:1012431217818},
  abstract = {Convergence of a generalized version of the modified SMO algorithms given by Keerthi et al. for SVM classifier design is proved. The convergence results are also extended to modified SMO algorithms for solving $\nu$-SVM classifier problems.},
  language = {en},
  number = {1-3},
  journal = {Machine Learning},
  author = {Keerthi, S. S. and Gilbert, E. G.},
  month = jan,
  year = {2002},
  pages = {351--360},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/2J5CZZ2Q/Keerthi and Gilbert - 2002 - Convergence of a Generalized SMO Algorithm for SVM.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/IWSEEANL/A1012431217818.html}
}

@article{clackdoyle_data_2016,
  title = {Data {{Consistency Conditions}} for {{Cone}}-{{Beam Projections}} on a {{Circular Trajectory}}},
  volume = {23},
  number = {12},
  journal = {IEEE signal processing letters},
  author = {Clackdoyle, Rolf and Desbat, Laurent and Lesaint, J{\'e}r{\^o}me and Rit, Simon},
  year = {2016},
  pages = {1746--1750},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/A9QZ239A/7586046.html}
}

@article{lesaint_calibration_2017,
  title = {Calibration for {{Circular Cone}}-{{Beam CT Based}} on {{Consistency Conditions}}},
  volume = {1},
  number = {6},
  journal = {IEEE Transactions on Radiation and Plasma Medical Sciences},
  author = {Lesaint, J{\'e}r{\^o}me and Rit, Simon and Clackdoyle, Rolf and Desbat, Laurent},
  year = {2017},
  pages = {517--526},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/RHKMPSVF/Lesaint et al. - 2017 - Calibration for Circular Cone-Beam CT Based on Con.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/9LWHBYZV/7999272.html}
}

@article{sidky_three_2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1801.06263},
  primaryClass = {physics},
  title = {Three Material Decomposition for Spectral Computed Tomography Enabled by Block-Diagonal Step-Preconditioning},
  abstract = {A potential application for spectral computed tomography (CT) with multi-energy-window photon-counting detectors is quantitative medical imaging with K-edge contrast agents. Image reconstruction for spectral CT with such contrast agents necessitates expression of the X-ray linear attenuation map in at least three expansion functions, for example, bone/water/K-edge-material or photo-electric- process/Compton-process/K-edge-material. The use of three expansion functions can result in slow convergence for iterative image reconstruction (IIR) algorithms applied to spectral CT. We propose a block-diagonal step-preconditioner for use with a primal-dual iterative image reconstruction framework that we have been developing for spectral CT. We demonstrate the advantage of the new step-preconditioner on a sensitive spectral CT simulation where the test object has low concentration of Gadolinium (Gd) contrast agent and the X-ray attenuation map is represented by three materials - PMMA, a soft-tissue equivalent, Aluminum, a bone equivalent, and Gd.},
  journal = {arXiv:1801.06263 [physics]},
  author = {Sidky, Emil Y. and Barber, Rina Foygel and Gilat-Schmidt, Taly and Pan, Xiaochuan},
  month = jan,
  year = {2018},
  keywords = {Physics - Medical Physics},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/BLYSW763/Sidky et al. - 2018 - Three material decomposition for spectral computed.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/4ZYBNHTI/1801.html}
}

@article{pan_computed_2010,
  title = {Computed {{Tomography}} in {{Color}}: {{NanoK}}-{{Enhanced Spectral CT Molecular Imaging}}},
  volume = {122},
  issn = {1521-3757},
  shorttitle = {Computed {{Tomography}} in {{Color}}},
  doi = {10.1002/ange.201005657},
  language = {en},
  number = {50},
  journal = {Angewandte Chemie},
  author = {Pan, Dipanjan and Roessl, Ewald and Schlomka, Jens-Peter and Caruthers, Shelton D. and Senpan, Angana and Scott, Mike J. and Allen, John S. and Zhang, Huiying and Hu, Grace and Gaffney, Patrick J. and Choi, Eric T. and Rasche, Volker and Wickline, Samuel A. and Proksa, Roland and Lanza, Gregory M.},
  month = dec,
  year = {2010},
  keywords = {Bismut,Computertomographie,Kontrastmittel,Molekulare Bildgebung,Nanokolloide},
  pages = {9829--9833},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/G9VXT3LV/Pan et al. - 2010 - Computed Tomography in Color NanoK-Enhanced Spect.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/KJIX62AA/abstract.html}
}

@inproceedings{brendel_penalized_2016,
  address = {Bamberg, Germany},
  title = {Penalized {{Likelihood Decomposition}} for {{Dual Layer Spectral CT}}},
  abstract = {Dual  layer  CT  systems  are  spectral  CT  systems,
which acquire for each scan spatially and temporally synchronous spectral  projection  data.  This  has  the  advantage  that  spectral evaluations  can  be  done  retrospectively  for  every  scan  (even  if the scan was initially not intended to deliver spectral results), and that  material  decomposition  can  be  done  directly  in  projection domain. The material decomposition in projection domain avoids inherently  beam  hardening  artifacts,  which  is  not  the  case  for decomposition  approaches  in  image  domain.  Since  material decomposition  is  an  ill-posed  nonlinear  problem  that  amplifies noise of the native projection data and may generate bias, noise reduction  in  the  projection  domain  is  helpful  to  reduce  noise streaks as well as bias in the reconstructed material images. An algorithm that combines material decomposition and noise reduction is introduced in this abstract. The algorithm, which is called Penalized Likelihood Decomposition (PL-Decomp), is derived indetail,  and its performance  and  properties  are  illustrated.},
  booktitle = {Proceeding of the Fourth {{International Meeting}} in {{X}}-Ray {{Computed Tomography}}},
  author = {Brendel, Bernhard and Bergner, Frank and Brown, Kevin and Koehler, Thomas},
  month = jul,
  year = {2016},
  pages = {41--44},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/AQ3BDMJZ/Brendel et al. - 2016 - Penalized Likelihood Decomposition for Dual Layer .pdf}
}

@book{beutel_handbook_2000,
  title = {Handbook of {{Medical Imaging}}: {{Medical}} Image Processing and Analysis},
  isbn = {978-0-8194-3622-1},
  shorttitle = {Handbook of {{Medical Imaging}}},
  abstract = {A little more than 100 years after the discovery of x-rays, this three-volume set is intended to provide a comprehensive overview of the theory and current practice of Medical Imaging as we enter the 21st century. As evidenced by the variety of research described in these volumes, medical imaging is still undergoing very rapid change. In more than 50 chapters, well-known experts provide the most current information available for students, researchers and practitioners working in this exciting field. These useful volumes can be ordered as a set or individually.},
  language = {en},
  publisher = {{SPIE Press}},
  author = {Beutel, Jacob},
  year = {2000},
  keywords = {Medical / Diagnosis}
}

@article{maass_exact_2011,
  title = {Exact Dual Energy Material Decomposition from Inconsistent Rays ({{MDIR}})},
  volume = {38},
  issn = {0094-2405},
  doi = {10.1118/1.3533686},
  abstract = {PURPOSE: Dual energy CT (DECT) allows calculating images that show the spatial distribution of the electron density and the atomic number or, more common, images of two basis material densities. In contrast, the Hounsfield unit that is shown in standard CT images is a measure of the x-ray attenuation, which is a function of the atomic number and electron density. To acquire additional information, DECT measures the object of interest using two different detected x-ray spectra. Most clinical CT scanners realize dual energy CT by fast tube voltage switching or by dual source dual detector arrangements and therefore do not allow measuring geometrically identical lines with each spectrum. Then, it is not possible to preprocess the raw data and calculate dual energy-specific raw data sets. The combination of the information of both spectra rather needs to be carried out in image domain after image reconstruction. Compared to the ideal raw data-based dual energy approaches, those image-based DECT methods are inferior because they are not able to correctly deal with the polychromatic nature of the x-rays. This article proposes a dedicated dual energy reconstruction algorithm for inconsistent rays that correctly accounts for all spectral effects.
METHODS: Material decomposition from inconsistent rays (MDIR) is an iterative method to indirectly perform raw data-based DECT even though different lines were measured for both spectra. Its iterative nature allows treating the x-ray polychromaticity correctly. The iterative process is initialized by density images that were obtained from an image-based material decomposition. Those images suffer from errors that originate from the polychromatic nature of the spectra. These errors are calculated by polychromatic forward projection of each measured line. After correction of the initial material density images, the polychromatic forward projection is repeated with more accurate material density images, yielding a more accurate error calculation. To demonstrate the proposed method, simulations and measurements were performed using clinical and preclinical dual source dual energy CT scanners.
RESULTS: Two iterations of MDIR are sufficient to greatly improve the qualitative and quantitative information in material density images. It is shown that banding artifacts, cupping artifacts, and mean density errors can be completely eliminated. Simulations with high geometrical inconsistency between the rays of different spectra indicate that nearly exact material decomposition is possible with MDIR. Furthermore, simulations show that the method works well in the presence of materials with K-edges within the detected spectrum. Phantom measurements using a clinical dual source CT scanner show the elimination of artifacts, which cause up to 4\% mean density error.
CONCLUSIONS: At moderate computational burden, the proposed MDIR algorithm yields images of the same high quality as direct raw data-based DECT methods. In contrast to those, MDIR is applicable to the case of inconsistent rays, as it often occurs in clinical or preclinical CT. Compared to image-based methods MDIR reduces artifacts and improves mean density errors in material density images. All dual energy postprocessing methods that are in use today, such as bone removal, virtual noncontrast images, etc., can be applied to the images provided by MDIR.},
  language = {eng},
  number = {2},
  journal = {Medical Physics},
  author = {Maass, Clemens and Meyer, Esther and Kachelriess, Marc},
  month = feb,
  year = {2011},
  keywords = {Animals,Artifacts,Head,Humans,Image Processing; Computer-Assisted,Mice,Phantoms; Imaging,X-Ray Microtomography,X-Rays,Radiography; Dual-Energy Scanned Projection},
  pages = {691--700},
  pmid = {21452706}
}

@article{sukovic_penalized_2000,
  title = {Penalized Weighted Least-Squares Image Reconstruction for Dual Energy {{X}}-Ray Transmission Tomography},
  volume = {19},
  issn = {0278-0062},
  doi = {10.1109/42.896783},
  abstract = {Presents a dual-energy (DE) transmission computed tomography (CT) reconstruction method. It is statistically motivated and features nonnegativity constraints in the density domain. A penalized weighted least squares (PWLS) objective function has been chosen to handle the non-Poisson noise added by amorphous silicon (aSi:H) detectors. A Gauss-Seidel algorithm has been used to minimize the objective function. The behavior of the method in terms of bias/standard deviation tradeoff has been compared to that of a DE method that is based on filtered back projection (FBP). The advantages of the DE PWLS method are largest for high noise and/or low flux cases. Qualitative results suggest this as well. Also, the reconstructed images of an object with opaque regions are presented. Possible applications of the method are: attenuation correction for positron emission tomography (PET) images, various quantitative computed tomography (QCT) methods such as bone mineral densitometry (BMD), and the removal of metal streak artifacts.},
  number = {11},
  journal = {IEEE Transactions on Medical Imaging},
  author = {Sukovic, P. and Clinthorne, N. H.},
  month = nov,
  year = {2000},
  keywords = {Algorithms,Amorphous silicon,amorphous silicon detectors,Artifacts,Attenuation,bone mineral densitometry,Computed tomography,Computer Simulation,computerised tomography,Data Interpretation; Statistical,densitometry,density domain,Detectors,dual energy X-ray transmission tomography,Gauss-Seidel algorithm,Gaussian processes,high noise cases,image reconstruction,Image reconstruction,Information Storage and Retrieval,least mean squares methods,Least squares methods,Least-Squares Analysis,low flux cases,medical image processing,metal streak artifacts removal,Models; Biological,Models; Statistical,nonnegativity constraints,nonPoisson noise,Numerical Analysis; Computer-Assisted,objective function minimisation,opaque regions,penalized weighted least-squares image reconstruction,PET images,Phantoms; Imaging,positron emission tomography,Positron emission tomography,quantitative computed tomography methods,Radiation Dosage,Radiographic Image Enhancement,Radiographic Image Interpretation; Computer-Assisted,Reconstruction algorithms,Reproducibility of Results,Sensitivity and Specificity,Si:H,Signal Processing; Computer-Assisted,Tomography; X-Ray Computed,X-ray imaging},
  pages = {1075--1081},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/F5GDJNYS/896783.html}
}

@article{mendonca_flexible_2014,
  title = {A {{Flexible Method}} for {{Multi}}-{{Material Decomposition}} of {{Dual}}-{{Energy CT Images}}},
  volume = {33},
  issn = {1558-254X},
  doi = {10.1109/TMI.2013.2281719},
  abstract = {The ability of dual-energy computed-tomographic (CT) systems to determine the concentration of constituent materials in a mixture, known as material decomposition, is the basis for many of dual-energy CT's clinical applications. However, the complex composition of tissues and organs in the human body poses a challenge for many material decomposition methods, which assume the presence of only two, or at most three, materials in the mixture. We developed a flexible, model-based method that extends dual-energy CT's core material decomposition capability to handle more complex situations, in which it is necessary to disambiguate among and quantify the concentration of a larger number of materials. The proposed method, named multi-material decomposition (MMD), was used to develop two image analysis algorithms. The first was virtual unenhancement (VUE), which digitally removes the effect of contrast agents from contrast-enhanced dual-energy CT exams. VUE has the ability to reduce patient dose and improve clinical workflow, and can be used in a number of clinical applications such as CT urography and CT angiography. The second algorithm developed was liver-fat quantification (LFQ), which accurately quantifies the fat concentration in the liver from dual-energy CT exams. LFQ can form the basis of a clinical application targeting the diagnosis and treatment of fatty liver disease. Using image data collected from a cohort consisting of 50 patients and from phantoms, the application of MMD to VUE and LFQ yielded quantitatively accurate results when compared against gold standards. Furthermore, consistent results were obtained across all phases of imaging (contrast-free and contrast-enhanced). This is of particular importance since most clinical protocols for abdominal imaging with CT call for multi-phase imaging. We conclude that MMD can successfully form the basis of a number of dual-energy CT image analysis algorithms, and has the potential to improve the clinical utility of dual-energy CT in disease management.},
  language = {eng},
  number = {1},
  journal = {IEEE transactions on medical imaging},
  author = {Mendonca, Paulo R. S. and Lamb, Peter and Sahani, Dushyant V.},
  month = jan,
  year = {2014},
  keywords = {Algorithms,Humans,Image Enhancement,Image Interpretation; Computer-Assisted,Reproducibility of Results,Sensitivity and Specificity,Tomography; X-Ray Computed,Radiography; Dual-Energy Scanned Projection,Absorptiometry; Photon},
  pages = {99--116},
  pmid = {24058018}
}

@article{poludniowski_calculation_2007,
  title = {Calculation of X-Ray Spectra Emerging from an x-Ray Tube. {{Part II}}. {{X}}-Ray Production and Filtration in x-Ray Targets},
  volume = {34},
  issn = {0094-2405},
  doi = {10.1118/1.2734726},
  abstract = {A new approach to the calculation of the x-ray spectrum emerging from an x-ray tube is proposed. Theoretical results for the bremsstrahlung cross section appearing in the literature are summarized. Four different treatments of electron penetration, based on the work presented in Part I, are then used to generate bremsstrahlung spectra. These spectra are compared to experimental data at 50, 80 and 100 kVp tube potentials. The most sophisticated treatment of electron penetration was required to obtain good agreement. With this treatment both the National Institute of Standards and Technology bremsstrahlung cross sections, based on accurate partial wave calculations, and the Bethe-Heitler cross section [H. A. Bethe and W. Heitler, Proc R. Soc. London, Ser. A. 146, 83-112 (1934)] corrected by a modified Elwert factor [G. Elwert, Ann. Phys. (Leipzig) 426, 178-208 (1939)], provided good agreement to measured data. An approximate treatment of the characteristic spectrum is suggested. The dependencies of the bremsstrahlung and characteristic outputs of an x-ray tube on tube potential are compared to experimentally derived data for 70-140 kVp potentials. Agreement is to within a few percent of the total output over the entire range. The spectral predictions of the semiempirical models of Birch and Marshall [R. Birch and M. Marshall, Phys. Med. Biol. 24, 505-513 (1979)] (IPEM Report 78) and of Tucker et al. [D. M. Tucker, G. T. Barnes, and D. P. Chakraborty, Med. Phys. 18, 211-218 (1991).] are also assessed. The predictions of Tucker et al. are very close to the model developed here. The predictions of IPEM Report 78 are similar, but consistently harder for the range of tube potentials examined (50-100 kV). Unlike the semiempirical models, the model proposed here requires the introduction of no empirical and unphysical parameters in the differential bremsstrahlung cross section, bar an overall normalization factor which is close to unity.},
  language = {eng},
  number = {6},
  journal = {Medical Physics},
  author = {Poludniowski, Gavin G.},
  month = jun,
  year = {2007},
  keywords = {Computer-Aided Design,Electrons,Equipment Design,Equipment Failure Analysis,Filtration,Models; Theoretical,Radiation Dosage,Radiography,Radiometry,Scattering; Radiation,Spectrometry; X-Ray Emission,X-Rays},
  pages = {2175--2186},
  pmid = {17654920}
}

@article{poludniowski_spekcalc_2009,
  title = {{{SpekCalc}}: A Program to Calculate Photon Spectra from Tungsten Anode x-Ray Tubes},
  volume = {54},
  issn = {1361-6560},
  shorttitle = {{{SpekCalc}}},
  doi = {10.1088/0031-9155/54/19/N01},
  abstract = {A software program, SpekCalc, is presented for the calculation of x-ray spectra from tungsten anode x-ray tubes. SpekCalc was designed primarily for use in a medical physics context, for both research and education purposes, but may also be of interest to those working with x-ray tubes in industry. Noteworthy is the particularly wide range of tube potentials (40-300 kVp) and anode angles (recommended: 6-30 degrees) that can be modelled: the program is therefore potentially of use to those working in superficial/orthovoltage radiotherapy, as well as diagnostic radiology. The utility is free to download and is based on a deterministic model of x-ray spectrum generation (Poludniowski 2007 Med. Phys. 34 2175). Filtration can be applied for seven materials (air, water, Be, Al, Cu, Sn and W). In this note SpekCalc is described and illustrative examples are shown. Predictions are compared to those of a state-of-the-art Monte Carlo code (BEAMnrc) and, where possible, to an alternative, widely-used, spectrum calculation program (IPEM78).},
  language = {eng},
  number = {19},
  journal = {Physics in Medicine and Biology},
  author = {Poludniowski, G. and Landry, G. and DeBlois, F. and Evans, P. M. and Verhaegen, F.},
  month = oct,
  year = {2009},
  keywords = {Electrodes,Monte Carlo Method,Photons,Software,Spectrum Analysis,Tungsten,X-Rays},
  pages = {N433--438},
  pmid = {19724100}
}

@book{colton_inverse_2013,
  address = {New York},
  edition = {3},
  series = {Applied Mathematical Sciences},
  title = {Inverse {{Acoustic}} and {{Electromagnetic Scattering Theory}}},
  isbn = {978-1-4614-4941-6},
  abstract = {The inverse scattering problem is central to many areas of science and technology such as radar and sonar, medical imaging, geophysical exploration and nondestructive testing. This book is devoted to the mathematical and numerical analysis of the inverse scattering problem for acoustic and electromagnetic waves. In this third edition, new sections have been added on the linear sampling and factorization methods for solving the inverse scattering problem as well as expanded treatments of iteration methods and uniqueness theorems for the inverse obstacle problem. These additions have in turn required an expanded presentation of both transmission eigenvalues and boundary integral equations in Sobolev spaces. As in the previous editions, emphasis has been given to simplicity over generality thus providing the reader with an accessible introduction to the field of inverse scattering theory.Review of earlier editions: ``Colton and Kress have written a scholarly, state of the art account of their view of direct and inverse scattering. The book is a pleasure to read as a graduate text or to dip into at leisure. It suggests a number of open problems and will be a source of inspiration for many years to come.''SIAM Review, September 1994 ``This book should be on the desk of any researcher, any student, any teacher interested in scattering theory.''Mathematical Intelligencer, June 1994},
  language = {en},
  publisher = {{Springer-Verlag}},
  author = {Colton, David and Kress, Rainer},
  year = {2013},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/YWFVR83N/9781461449416.html}
}

@article{wirgin_inverse_2004,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {math-ph/0401050},
  title = {The Inverse Crime},
  abstract = {The inverse crime occurs when the same (or very nearly the same) theoretical ingredients are employed to synthesize as well as to invert data in an inverse problem. This act has been qualified as trivial and therefore to be avoided by Colton and Kress.},
  journal = {arXiv:math-ph/0401050},
  author = {Wirgin, Armand},
  month = jan,
  year = {2004},
  keywords = {Mathematical Physics},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/2P9VVPQI/Wirgin - 2004 - The inverse crime.pdf;/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/NMAX9YB5/0401050.html}
}

@article{hansen_fast_2018,
  title = {Fast {{4D}} Cone-Beam {{CT}} from 60 s Acquisitions},
  volume = {5},
  issn = {24056316},
  doi = {10.1016/j.phro.2018.02.004},
  language = {en},
  journal = {Physics and Imaging in Radiation Oncology},
  author = {Hansen, David C. and S{\o}rensen, Thomas Sangild},
  month = jan,
  year = {2018},
  pages = {69--75},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/74ZEETLB/Hansen and SÃ¸rensen - 2018 - Fast 4D cone-beam CT from 60 s acquisitions.pdf}
}

@article{kim_combining_2015,
  title = {Combining {{Ordered Subsets}} and {{Momentum}} for {{Accelerated X}}-{{Ray CT Image Reconstruction}}},
  volume = {34},
  issn = {0278-0062},
  doi = {10.1109/TMI.2014.2350962},
  abstract = {Statistical X-ray computed tomography (CT) reconstruction can improve image quality from reduced dose scans, but requires very long computation time. Ordered subsets (OS) methods have been widely used for research in X-ray CT statistical image reconstruction (and are used in clinical PET and SPECT reconstruction). In particular, OS methods based on separable quadratic surrogates (OS-SQS) are massively parallelizable and are well suited to modern computing architectures, but the number of iterations required for convergence should be reduced for better practical use. This paper introduces OS-SQS-momentum algorithms that combine Nesterov's momentum techniques with OS-SQS methods, greatly improving convergence speed in early iterations. If the number of subsets is too large, the OS-SQS-momentum methods can be unstable, so we propose diminishing step sizes that stabilize the method while preserving the very fast convergence behavior. Experiments with simulated and real 3D CT scan data illustrate the performance of the proposed algorithms.},
  number = {1},
  journal = {IEEE Transactions on Medical Imaging},
  author = {Kim, D. and Ramani, S. and Fessler, J. A.},
  month = jan,
  year = {2015},
  keywords = {Acceleration,Algorithm design and analysis,Algorithms,clinical PET reconstruction,clinical SPECT reconstruction,combine Nesterov momentum techniques,Computed tomography,Computed tomography (CT),computerised tomography,Convergence,convergence of numerical methods,Cost function,diagnostic radiography,fast convergence behavior,Humans,Image Processing; Computer-Assisted,image reconstruction,Image reconstruction,iterative methods,medical image processing,momentum,ordered subset method based separable quadratic surrogates,ordered subsets,OS-SQS-momentum algorithms,parallelizable iterative algorithm,parallelizable iterative algorithms,Phantoms; Imaging,Radiography; Thoracic,relaxation,separable quadratic surrogates,statistical analysis,statistical image reconstruction,stochastic gradient,Tomography; X-Ray Computed,X-ray CT statistical image reconstruction,X-ray imaging},
  pages = {167--178},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/JNTDD6QC/6882248.html}
}

@inproceedings{zeller_charge_2009,
  title = {Charge Sharing between Pixels in the Spectral {{Medipix2}} X-Ray Detector},
  doi = {10.1109/IVCNZ.2009.5378381},
  abstract = {This paper gives an overview of the Medipix2 x-ray detector and its use in medical imaging, with the MARS-CT scanner (MARS, Medipix All Resolution System) as an example. The Medipix2 chip is a photon counting pixel detector with the ability of energy discrimination. It was developed at CERN and is composed of a sensor layer bump bonded to electronics layer. It has 256{\~A}Â—256 pixels, each one covering an area of 55{\~A}Â—55{\^A}\textquestiondown{}m2. Furthermore, every pixel can be read out separately. The MARS-CT scanner uses these properties to scan biological objects obtaining multi-energy (spectral) x-ray images with high contrast between materials and high spatial resolution. Charge sharing is the phenomenon by which the electron-hole charge cloud, induced in the sensor layer by an absorbed photon, is detected by a cluster of neighbouring pixels. Each pixel in the cluster generates a signal corresponding to its fraction of the cloud, so the detector will record several photons each of lower energies. This effect has to be considered with the use of Medipix2, because of its small pixels and the hybrid architecture. The effect was measured and a simulation modelled with the aim to reconstruct the spectrum removing the distortion of the detection process.},
  booktitle = {2009 24th {{International Conference Image}} and {{Vision Computing New Zealand}}},
  author = {Zeller, H. and Dufreneix, S. and Clark, M. and Butler, P. H. and Butler, A. P. H. and Cook, N. and Tlustos, L.},
  month = nov,
  year = {2009},
  keywords = {biomedical imaging,Biomedical imaging,Biosensors,Bonding,charge sharing,Clouds,computerised tomography,electron-hole charge cloud,Energy resolution,Image resolution,Mars,MARS-CT scanner,medical imaging,Medipix2 chip,Optoelectronic and photonic sensors,photon counting pixel detector,spectral Medipix2 x-ray detector,X-ray detection,X-ray detectors,X-ray imaging},
  pages = {363--366},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/CDZDZZEP/5378381.html}
}

@article{taguchi_analytical_2010,
  title = {An Analytical Model of the Effects of Pulse Pileup on the Energy Spectrum Recorded by Energy Resolved Photon Counting X-Ray Detectors},
  volume = {37},
  issn = {0094-2405},
  doi = {10.1118/1.3429056},
  abstract = {Purpose: Recently, novel CdTe photon counting x-ray detectors (PCXDs) with energy discrimination capabilities have been developed. When such detectors are operated under a high x-ray flux, however, coincident pulses distort the recorded energy spectrum. These distortions are called pulse pileup effects. It is essential to compensate for these effects on the recorded energy spectrum in order to take full advantage of spectral information PCXDs provide. Such compensation can be achieved by incorporating a pileup model into the image reconstruction process for computed tomography, that is, as a part of the forward imaging process, and iteratively estimating either the imaged object or the line integrals using, e.g., a maximum likelihood approach. The aim of this study was to develop a new analytical pulse pileup model for both peak and tail pileup effects for nonparalyzable detectors., Methods: The model takes into account the following factors: The bipolar shape of the pulse, the distribution function of time intervals between random events, and the input probability density function of photon energies. The authors used Monte Carlo simulations to evaluate the model., Results: The recorded spectra estimated by the model were in an excellent agreement with those obtained by Monte Carlo simulations for various levels of pulse pileup effects. The coefficients of variation (i.e., the root mean square difference divided by the mean of measurements) were 5.3\%\textendash{}10.0\% for deadtime losses of 1\%\textendash{}50\% with a polychromatic incident x-ray spectrum., Conclusions: The proposed pulse pileup model can predict recorded spectrum with relatively good accuracy.},
  number = {8},
  journal = {Medical Physics},
  author = {Taguchi, Katsuyuki and Frey, Eric C. and Wang, Xiaolan and Iwanczyk, Jan S. and Barber, William C.},
  month = aug,
  year = {2010},
  pages = {3957--3969},
  file = {/home/mory/.mozilla/firefox/iqe4jjbz.default/zotero/storage/5SZWI7I3/Taguchi et al. - 2010 - An analytical model of the effects of pulse pileup.pdf},
  pmid = {20879558},
  pmcid = {PMC2917451}
}

@article{taguchi_spatio-energetic_2016,
  title = {Spatio-Energetic Cross Talk in Photon Counting Detectors: {{Detector}} Model and Correlated {{Poisson}} Data Generator},
  volume = {43},
  issn = {2473-4209},
  shorttitle = {Spatio-Energetic Cross Talk in Photon Counting Detectors},
  doi = {10.1118/1.4966699},
  abstract = {PURPOSE: An x-ray photon interacts with photon counting detectors (PCDs) and generates an electron charge cloud or multiple clouds. The clouds (thus, the photon energy) may be split between two adjacent PCD pixels when the interaction occurs near pixel boundaries, producing a count at both of the pixels. This is called double-counting with charge sharing. (A photoelectric effect with K-shell fluorescence x-ray emission would result in double-counting as well). As a result, PCD data are spatially and energetically correlated, although the output of individual PCD pixels is Poisson distributed. Major problems include the lack of a detector noise model for the spatio-energetic cross talk and lack of a computationally efficient simulation tool for generating correlated Poisson data. A Monte Carlo (MC) simulation can accurately simulate these phenomena and produce noisy data; however, it is not computationally efficient.
METHODS: In this study, the authors developed a new detector model and implemented it in an efficient software simulator that uses a Poisson random number generator to produce correlated noisy integer counts. The detector model takes the following effects into account: (1) detection efficiency; (2) incomplete charge collection and ballistic effect; (3) interaction with PCDs via photoelectric effect (with or without K-shell fluorescence x-ray emission, which may escape from the PCDs or be reabsorbed); and (4) electronic noise. The correlation was modeled by using these two simplifying assumptions: energy conservation and mutual exclusiveness. The mutual exclusiveness is that no more than two pixels measure energy from one photon. The effect of model parameters has been studied and results were compared with MC simulations. The agreement, with respect to the spectrum, was evaluated using the reduced $\chi$2statistics or a weighted sum of squared errors, $\chi$red2($\geq$1), where $\chi$red2=1 indicates a perfect fit.
RESULTS: The model produced spectra with flat field irradiation that qualitatively agree with previous studies. The spectra generated with different model and geometry parameters allowed for understanding the effect of the parameters on the spectrum and the correlation of data. The agreement between the model and MC data was very strong. The mean spectra with 90 keV and 140 kVp agreed exceptionally well: $\chi$red2values were 1.049 with 90 keV data and 1.007 with 140 kVp data. The degrees of cross talk (in terms of the relative increase from single pixel irradiation to flat field irradiation) were 22\% with 90 keV and 19\% with 140 kVp for MC simulations, while they were 21\% and 17\%, respectively, for the model. The covariance was in strong agreement qualitatively, although it was overestimated. The noisy data generation was very efficient, taking less than a CPU minute as opposed to CPU hours for MC simulators.
CONCLUSIONS: The authors have developed a novel, computationally efficient PCD model that takes into account double-counting and resulting spatio-energetic correlation between PCD pixels. The MC simulation validated the accuracy.},
  language = {eng},
  number = {12},
  journal = {Medical Physics},
  author = {Taguchi, Katsuyuki and Polster, Christoph and Lee, Okkyun and Stierstorfer, Karl and Kappler, Steffen},
  month = dec,
  year = {2016},
  keywords = {Models; Theoretical,Photons,Spectrum Analysis,Poisson Distribution},
  pages = {6386},
  pmid = {27908175}
}


